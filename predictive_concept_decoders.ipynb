{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PIP_CACHE_DIR\"] = \"/workspace/.cache/pip\"\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/.cache/huggingface\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aV-I6YQz3I3U",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.51.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.4.1+cu124)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting nnsight\n",
      "  Downloading nnsight-0.5.13-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (15 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0) (3.13.1)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.51.0)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.51.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.51.0)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting astor (from nnsight)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cloudpickle (from nnsight)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting python-socketio[client] (from nnsight)\n",
      "  Downloading python_socketio-5.16.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pydantic>=2.9.0 (from nnsight)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting toml (from nnsight)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from nnsight) (8.27.0)\n",
      "Collecting rich (from nnsight)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9.0->nnsight)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.51.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.51.0) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting bidict>=0.21.0 (from python-socketio[client]->nnsight)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.11.0 (from python-socketio[client]->nnsight)\n",
      "  Downloading python_engineio-4.13.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->nnsight)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->nnsight) (0.8.4)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->nnsight)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->nnsight) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->nnsight) (0.2.13)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.11.0->python-socketio[client]->nnsight)\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython->nnsight) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython->nnsight) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython->nnsight) (0.2.3)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight)\n",
      "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of wsproto to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading wsproto-1.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Downloading wsproto-1.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m227.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m130.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m169.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m312.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m254.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m482.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m201.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m287.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m220.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m198.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m164.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m206.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m202.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m230.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading nnsight-0.5.13-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (98 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m379.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m387.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m399.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m304.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m320.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m308.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m426.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m492.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading python_engineio-4.13.0-py3-none-any.whl (59 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading python_socketio-5.16.0-py3-none-any.whl (79 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, xxhash, wsproto, tzdata, typing-extensions, triton, tqdm, toml, sympy, safetensors, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, mdurl, hf-xet, frozenlist, dill, cloudpickle, bidict, astor, annotated-types, aiohappyeyeballs, yarl, typing-inspection, simple-websocket, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, huggingface_hub, aiosignal, tokenizers, rich, python-engineio, pydantic, nvidia-cusolver-cu12, aiohttp, transformers, torch, python-socketio, datasets, accelerate, peft, nnsight\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.99\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.99\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1+cu124\n",
      "    Uninstalling torch-2.4.1+cu124:\n",
      "      Successfully uninstalled torch-2.4.1+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 annotated-types-0.7.0 astor-0.8.1 bidict-0.23.1 cloudpickle-3.1.2 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 huggingface_hub-0.36.0 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.7.0 multiprocess-0.70.18 nnsight-0.5.13 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 peft-0.18.0 propcache-0.4.1 pyarrow-22.0.0 pydantic-2.12.5 pydantic-core-2.41.5 python-engineio-4.13.0 python-socketio-5.16.0 pytz-2025.2 regex-2025.11.3 rich-14.2.0 safetensors-0.7.0 simple-websocket-1.1.0 sympy-1.14.0 tokenizers-0.22.1 toml-0.10.2 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 triton-3.5.1 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 wsproto-1.2.0 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Found existing installation: torchvision 0.19.1+cu124\n",
      "Uninstalling torchvision-0.19.1+cu124:\n",
      "  Successfully uninstalled torchvision-0.19.1+cu124\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install --no-cache-dir -U \"transformers>=4.51.0\" accelerate datasets torch pandas tqdm nnsight huggingface_hub peft\n",
    "# !pip install --no-cache-dir typing-extensions --upgrade\n",
    "# !pip uninstall -y torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ptcRh6uv1z42"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "# from nnsight import LanguageModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # other option: Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel \u001b[38;5;28;01mas\u001b[39;00m DDP\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m local_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCAL_RANK\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     83\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:95\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mguard():\n\u001b[0;32m---> 95\u001b[0m         func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:1762\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1759\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m   1760\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[0;32m-> 1762\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py:267\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[1;32m    270\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py:252\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    250\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "dist.init_process_group(backend=\"nccl\")\n",
    "local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = torch.device(f\"cuda:{local_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "53931fd35e5e4b008f1ed365a68b71af",
      "10dfda429d3d4ed7b10d931b38bb672d",
      "287fad4781ac41ffa6540e2ee13752f7",
      "67bb0fae78884bb9bac6cbcbc4ca8871",
      "ecb150261cf24b4b9041a7446444bf32",
      "594cd08d0f904b8bb1796d04b5b14c71",
      "019057c06cdd4b44a337557fea6609bd",
      "661c0445090247db9940c61b5c3bf620",
      "a73b6a178a234d028935dbf9086955f4",
      "9bc801c2ef324241be7d1d359b533e9e",
      "3408a621e4214ce996f688dc3b178911",
      "f8c9c121f9ea4249a3e13a1df268ccba",
      "befcc2d602914a2ba87d278e584bdabd",
      "ba5c490caeee49d1983067926a007f38",
      "1607d4c338524071a733ce0d1e35476a",
      "7ef9ad12ef644c80badeead162a18e9d",
      "0099c4dbd08343d0b044d01969f8a7b1",
      "5efe6f553f144a2d99ab9d2e454519ed",
      "45e2a55b0cd243a08fb472a40df0c506",
      "5c6229ea95a64dff81f1c078a8cd4280"
     ]
    },
    "id": "aJHE57iZCSwb",
    "outputId": "1b2a42c1-ae72-45d2-c7ed-43a374f75f2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cce5eba4c3b4341a95d731afd099213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uE3mAo003-Kz"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name=\"meta-llama/Llama-3.2-1B-Instruct\", attn_implementation=\"sdpa\", mode=\"eval\", **kwargs):\n",
    "    \"\"\"Load model and tokenizer with standard setup.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, config_dict) where config_dict has num_layers, num_heads, head_dim\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=\"auto\",\n",
    "        device_map=None,\n",
    "        attn_implementation=attn_implementation,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    head_dim = model.config.hidden_size // num_heads\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    config = {\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"head_dim\": head_dim,\n",
    "    }\n",
    "\n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wcHtMKLW6zTf"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_name=\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True, first_k=int(1e5), buffer_frac=0.1, val_frac=0.05):\n",
    "    ds_stream = load_dataset(\n",
    "        dataset_name,\n",
    "        split=split,\n",
    "        streaming=streaming\n",
    "    )\n",
    "    ds_stream = ds_stream.shuffle(buffer_size=int(first_k * buffer_frac), seed=0)\n",
    "    ds_train = ds_stream.take(first_k)\n",
    "    ds_val = ds_stream.skip(first_k).take(int(first_k * val_frac))\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RaTH4YrM9y_4"
   },
   "outputs": [],
   "source": [
    "def get_cropped_text_ids(dataset, tokenizer, prefix_ids, cropped_len=48):\n",
    "    for item in dataset:\n",
    "        text = item[\"text\"]\n",
    "        text_ids = tokenizer(\n",
    "            text,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=False\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        if len(text_ids) >= cropped_len:\n",
    "            start = rng.randint(0, len(text_ids) - cropped_len)\n",
    "            selected_ids = text_ids[start:start + cropped_len]\n",
    "            yield prefix_ids + selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "0942b1988c1c4eeda667c5db81bc04a7",
      "d70ed9c8408e4a1abe588ffebc84e8b6",
      "d6b06a0b03dd4ebc93a9437b57346e7f",
      "a3ff263f2cc54c60b4cc9a280dc63ca8",
      "d850fdeafb04413f8115429934af88f4",
      "7ac81d50c30143b68423b882e9d37e3c",
      "d1051bf354364a4d9d7c8ed1d63a2da4",
      "4c3716da30e448ed939d6f5631d025a6",
      "04919139298942b99a660f5bc61cce62",
      "50d5aaf79a034437bf85251670ba8356",
      "42f667218c4741b5be88988e28f1aa3c",
      "fe2654afbf9d4f838a63bdb136b2186e",
      "7c1f3c1c72514833969ccf238d4210f1",
      "3e955f9746ac4d13b11ad0213c194e42",
      "aa0b0c34bbaa4b2090e5724c89af16aa",
      "e9866e8ab44a405e86da879a761930ec",
      "f69f01552d1743d59bb6738672f53cad",
      "393a0a1d140444c7a44b11711363f28d",
      "e68357c4f2494ef9821ed6eb1e3337ee",
      "ee0eeebd787f466294f7b3536025614f",
      "66a3fb70f0f14a4e820714a8f495c0e1",
      "58989bf8a5244d418647c7c8711dc2bc",
      "ddc00c46ac694e9bb70d4b0ee84b51c9",
      "c87ccd015f6440338d9a37091fb04604",
      "eee46f7a4b10433ba465e9b152d08f07",
      "7f2d3612698b4004bb79ae459d8f34ee",
      "c1fc1500dbc24741906b5b4a426bf1ee",
      "ba016ceaf58d487994c53d301ae4e1b9",
      "7d81e4f25df84191b5b7be21fc1bd804",
      "9209c383ba57422f9cef98f6ec96c763",
      "42f65d05518c4852886bdfed61424f2c",
      "729730f0b74340e1895c90ef8edb2bb3",
      "d31c8a0340c74411a63f842e5bf891d1",
      "2e4a247a1f8746338d4b4f660865d255",
      "4d7cfbef6d5d41a0a29361f45728a2ff",
      "efc57584738a41e8b4078e00a1944bea",
      "fbd022d35f2246e0888c8d557821aea9",
      "9c0c0839f3bf4b3db6c596bdf5e2dbdb",
      "735041947c904ec4bcebcc1fa7c0091a",
      "7605ccbae741402eaf25afefa8a75369",
      "245cc983d58642d19a8f698c50fa9242",
      "65c2942536eb4be18e8a4ceb1146e8af",
      "de29e1b58ca04fc3818a6696fe855577",
      "278e1700058449d4a22b20d8a7255074",
      "970a32d00f0f4e0abcc6cc4ec0189a02",
      "5143368f2e314e2daea484da3b588545",
      "c6e11c909920432f81199e4f9b5bfd26",
      "41f8d00cb6a9484b97c84cf93998625f",
      "a753e1197ddf4efa8e92c4e17fc90921",
      "f9ae5f8760654ea2b1b88207c6182402",
      "6dfd3107421a4f308bcb01e9bd2d09b1",
      "da04eac95e7b4ba3bc2a78305e49488a",
      "616ad8bb34b64c03ad85ab3a869eba28",
      "b63cbc6d2191473c95f94971fdf9c141",
      "e6abed246b9f4e81a5c15bb70a01a6cb",
      "38165237ab894c1186a772aaa8c45497",
      "b6f97e2f86f04f82a52490a36704c588",
      "d09561eacd5043cda61d27bd45d023cd",
      "53b863bafee84f73ae4d1e4f68b90c01",
      "896f5afbb52d40ec97e0e8114ccee39c",
      "321575caef5a4e38bf0708435f75044e",
      "81a571bfdb5448d28aae0cb46b57c4c9",
      "9f58ddedae0746d98c6274b78d4c850f",
      "333d03e2488249ef9be61ddb5f0a5aaa",
      "ed7709e0619042c598b121da1076a6a4",
      "4189510eadaf4cdf9bd5e2dafd1037ee"
     ]
    },
    "id": "8W9UOBC67ov9",
    "outputId": "c00066b2-3e97-4178-e85e-e818a1f2b3aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ec617ac7614498ad7a6b321d613bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec836c82f9db4413bd223d63b6962223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd7713509034da1877597220bd19283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d841b00355ba4c70922a5f7fe4d527c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dcf36cba7d4fe088e9e368752563df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0d830cd3be45dc843014b9d313aba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject, tokenizer, config = load_model_and_tokenizer()\n",
    "subject = subject.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hfeJo-3Uf5GC"
   },
   "outputs": [],
   "source": [
    "for p in subject.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "bc12929f25584d68a4d62d02d8d16e55",
      "58eb49612aa044de988496b007fdb1c6",
      "25cd624f0085440aa0d605340b8758a4",
      "408564fb9c914cbaa8c8bdb2e00650f1",
      "b21c8c39c1a44155917aba8d3a73f03d",
      "346f6fdd05d4402ebced88b629e6c0a5",
      "971fa4cc1cac45b392788af1dd59cd9f",
      "b94ef5bd123846fbb63c75e2bbdeaffb",
      "2a738098d47441c386b6b8c275f0e2ea",
      "ae63695bd476498ead3a8be7dcac4ef0",
      "6ff3ba00c1e04e64b65deb14e1a18949",
      "b112e57902f4415a898fe634aa8793e2",
      "61e50ba122fe4a348eb61acd52ae8b51",
      "14feea9eee1642f3baecaeda6cbc2c26",
      "f081569d4dfe49c19ffa54072affd38e",
      "617b35f0d5a843cf939e728c37ae63b3",
      "7b436aab3e8a4e0992deb52008ef5175",
      "6544221ed92040cb957cfd3a33dcaf1f",
      "d2b54a2eb6b64caaaada818c4f88b549",
      "c1c4aeb62cd74303804a6c867dd24ee0",
      "c41580dd8b114dd89677c87c12a6285f",
      "a5552e3376e3482d86bfc7743e1b95d0"
     ]
    },
    "id": "Kkq4Povq74GY",
    "outputId": "93659674-13c5-4782-b71c-e6e2003bf675"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a881917318a49b4adc70270091244b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387815ef31841ddbab582f06d4fbc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba5c8e67249414eb7bbdbd20f26f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_val = load_data(first_k=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Tyqh8dKjSZVn"
   },
   "outputs": [],
   "source": [
    "# nnsight_model = LanguageModel(subject, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IDMfp_wDK73k"
   },
   "outputs": [],
   "source": [
    "INSTRUCT_PREFIX = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "Cutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "prefix_ids = tokenizer(\n",
    "    INSTRUCT_PREFIX,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=False\n",
    ")[\"input_ids\"]\n",
    "len_prefix = len(prefix_ids)\n",
    "\n",
    "cropped_len = 48\n",
    "\n",
    "rng = random.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdllgwjD9zn7"
   },
   "outputs": [],
   "source": [
    "# text = INSTRUCT_PREFIX + next(iter(ds_val))[\"text\"][:200]\n",
    "# enc = tokenizer(\n",
    "#     text,\n",
    "#     return_tensors=None,\n",
    "#     add_special_tokens=False\n",
    "# )\n",
    "# input_ids = enc[\"input_ids\"]\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# for a, b in zip(tokens, input_ids):\n",
    "#     print(a, \"    \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnYxMC4kMw6B"
   },
   "outputs": [],
   "source": [
    "# text = INSTRUCT_PREFIX + next(iter(ds_val))[\"text\"][:20]\n",
    "# enc = tokenizer(\n",
    "#     text,\n",
    "#     return_tensors=None,\n",
    "#     add_special_tokens=False\n",
    "# )\n",
    "# input_ids = enc[\"input_ids\"]\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# for a, b in zip(tokens, input_ids):\n",
    "#     print(a, \"    \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KAFh6dxi9DlA"
   },
   "outputs": [],
   "source": [
    "# cropped_data_ids = get_cropped_text_ids(ds_val, tokenizer, prefix_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pu2O26HUPzCd",
    "outputId": "bdc91aba-f2e0-4d20-a291-10745c3e3294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 9125, 128007, 1432, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 1432, 128009, 128006, 882, 128007, 271, 527, 1274, 889, 1505, 5694, 19596, 14918, 4245, 311, 264, 26682, 12205, 315, 6677, 922, 2574, 12765, 304, 279, 3596, 8915, 1917, 13, 578, 22963, 1917, 11031, 779, 5043, 1606, 315, 279, 502, 2574, 12765, 1855, 323, 1475, 2046, 4028, 279, 1917, 627, 2181, 1587, 539, 1935, 25294]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# li = next(iter(cropped_data_ids))\n",
    "# print(li)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(li)\n",
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2ZVQIFq9wt1"
   },
   "outputs": [],
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "HKx4UYUBNs33",
    "outputId": "d5d65836-b94e-4c3b-d66c-0860776c199b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' are people who find themselves floating mainly due to a shallow wealth of knowledge about things happening in the ever dynamic world. The soccer world moves so fast because of the new things happening each and every week across the world.\\nIt does not take rocket'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(li[30:], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXkKXwg9OMY-",
    "outputId": "011d7d0b-e996-4ba9-f7fe-0c2fe355599c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc = tokenizer(\n",
    "#     \" X\" * 16,\n",
    "#     return_tensors=None,\n",
    "#     add_special_tokens=True\n",
    "# )[\"input_ids\"]\n",
    "# enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTr1j95HRoU4",
    "outputId": "3a48be59-6755-44c1-fcb0-bf63858dbb6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 2048])\n"
     ]
    }
   ],
   "source": [
    "# prompt = li\n",
    "# batch_ids = torch.tensor([prefix_ids, prefix_ids], device=subject.device)\n",
    "# with nnsight_model.trace(batch_ids) as tracer:\n",
    "#     resid = nnsight_model.model.layers[10].output[:].save()\n",
    "# print(resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o272BhXTUBnQ"
   },
   "outputs": [],
   "source": [
    "# resid[0].float().cpu().numpy()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mtJpdxJjVry8"
   },
   "outputs": [],
   "source": [
    "class CroppedTokenDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, prefix_ids, cropped_len=48, mode=\"train\"):\n",
    "        self.ds = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prefix = torch.tensor(prefix_ids, dtype=torch.long)\n",
    "        self.cropped_len = cropped_len\n",
    "        self.mode = mode\n",
    "\n",
    "    def __iter__(self):\n",
    "        info = get_worker_info()\n",
    "        wid = 0 if info is None else info.id\n",
    "        nw  = 1 if info is None else info.num_workers\n",
    "\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "        num_shards = world * nw\n",
    "        shard_index = rank * nw + wid\n",
    "        ds = self.ds.shard(num_shards=num_shards, index=shard_index)\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(0)\n",
    "\n",
    "        for item in ds:\n",
    "            text_ids = self.tokenizer(item[\"text\"], return_tensors=None, add_special_tokens=False)[\"input_ids\"]\n",
    "            if len(text_ids) >= self.cropped_len:\n",
    "                if self.mode == \"train\":\n",
    "                    start = int(torch.randint(0, len(text_ids) - self.cropped_len + 1, (1,), generator=g).item())\n",
    "                else:\n",
    "                    start = 0\n",
    "\n",
    "                cropped = torch.tensor(text_ids[start:start + self.cropped_len], dtype=torch.long)\n",
    "                yield torch.cat([self.prefix, cropped], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q6Nikwc6WihS"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_in=2048, multiplier=8, top_k=16):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.w_enc = nn.Linear(d_in, d_in * multiplier, bias=True)\n",
    "        self.w_emb = nn.Linear(d_in * multiplier, d_in, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            W = torch.randn_like(self.w_enc.weight)\n",
    "            W /= W.norm(dim=1, keepdim=True)\n",
    "            self.w_enc.weight.copy_(W)\n",
    "            self.w_enc.bias.zero_()\n",
    "            self.w_emb.weight.copy_(self.w_enc.weight.T)\n",
    "\n",
    "    def forward(self, x):  # (B, 16, d_in)\n",
    "        y = self.w_enc(x)  # (B, 16, d_in*mult)\n",
    "\n",
    "        idx = torch.topk(y, self.top_k, dim=-1).indices\n",
    "        mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "        mask.scatter_(-1, idx, True)\n",
    "        masked_y = y * mask.to(y.dtype)\n",
    "\n",
    "        out = self.w_emb(masked_y)  # (B, 16, d_in)\n",
    "        return out, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SYX0GX8lkZwX"
   },
   "outputs": [],
   "source": [
    "# def get_resid_stream_vector(layer, input_ids, prefix_ids, cropped_len):\n",
    "#     with nnsight_model.trace(input_ids):\n",
    "#         resid = nnsight_model.model.layers[layer].output[:]\n",
    "#         start = len(prefix_ids) + cropped_len // 3\n",
    "#         end = start + cropped_len // 3\n",
    "#         out = resid[:, start:end, :].save()\n",
    "#         return out\n",
    "\n",
    "def get_resid_stream_vector(model, input_ids, layer, start, end, attention_mask=None):\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "    resid = out.hidden_states[layer + 1]\n",
    "    return resid[:, start:end, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8fiGu8sQw1lk"
   },
   "outputs": [],
   "source": [
    "def get_resid_stream_vector_efficient(model, input_ids, layer, start, end, attention_mask=None):\n",
    "    saved = {}\n",
    "    def hook(module, inp, out):\n",
    "        saved[\"slice\"] = out[:, start:end, :].detach()\n",
    "\n",
    "    h = model.model.layers[layer].register_forward_hook(hook)\n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=False\n",
    "            )\n",
    "        return saved[\"slice\"]\n",
    "    finally:\n",
    "        h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "aEK_9UGhtEBI"
   },
   "outputs": [],
   "source": [
    "# test = get_resid_stream_vector(subject, torch.tensor([[2040,3520]], device=\"cuda\"),3,0,5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dr34CimygRvo"
   },
   "outputs": [],
   "source": [
    "decoder_base, _, _ = load_model_and_tokenizer(mode=\"train\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "decoder = get_peft_model(decoder_base, lora_cfg).to(device).train()\n",
    "\n",
    "d_model = decoder.config.hidden_size\n",
    "d_model_multiplier = 8\n",
    "encoder = Encoder(d_in=d_model, multiplier=d_model_multiplier, top_k=16).to(device).to(torch.bfloat16).train()\n",
    "\n",
    "decoder = DDP(decoder, device_ids=[local_rank], output_device=local_rank)\n",
    "encoder = DDP(encoder, device_ids=[local_rank], output_device=local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RC-LP6FuzVT",
    "outputId": "26862363-8396-4429-fb8c-a07999992344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mean/std: -0.00012136666919104755 0.012790604494512081\n",
      "B mean/std: 0.0 0.0\n",
      "B all zero: True\n"
     ]
    }
   ],
   "source": [
    "# A = decoder.base_model.model.model.layers[0].self_attn.q_proj.lora_A[\"default\"].weight.detach()\n",
    "# B = decoder.base_model.model.model.layers[0].self_attn.q_proj.lora_B[\"default\"].weight.detach()\n",
    "\n",
    "# print(\"A mean/std:\", A.mean().item(), A.std().item())\n",
    "# print(\"B mean/std:\", B.mean().item(), B.std().item())\n",
    "# print(\"B all zero:\", (B == 0).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Bv-6fwdbdEaJ"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "start_cropped_pos = len_prefix + cropped_len // 3\n",
    "end_cropped_pos = start_cropped_pos + cropped_len // 3\n",
    "layer = 8\n",
    "batch_size=64\n",
    "dummy = tokenizer(\n",
    "    \" X\" * (cropped_len // 3),\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ")[\"input_ids\"].expand(batch_size, -1).to(device)\n",
    "patch_idx = torch.arange(16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sZ84-pLS4ncU"
   },
   "outputs": [],
   "source": [
    "concepts_last_occ_by_seen_tokens = torch.full(\n",
    "    (d_model * d_model_multiplier,),\n",
    "    -1,\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "seen_tokens = 0\n",
    "inactive_concepts_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rHyL6eKfb4OB"
   },
   "outputs": [],
   "source": [
    "pcd_train_ds = CroppedTokenDataset(\n",
    "    hf_dataset=ds_train,\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_ids=prefix_ids,\n",
    "    cropped_len=48,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "pcd_val_ds = CroppedTokenDataset(\n",
    "    hf_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_ids=prefix_ids,\n",
    "    cropped_len=48,\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "# 3) dataloader\n",
    "train_loader = DataLoader(\n",
    "    pcd_train_ds,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    pcd_val_ds,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EqxbWH3YTX1z"
   },
   "outputs": [],
   "source": [
    "patch_state = {\"vecs\": None}\n",
    "def patch_resid_stream_hook(idx):\n",
    "    def hook(module, inp, out):\n",
    "        h = out.clone()\n",
    "        h[:, idx, :] = patch_state[\"vecs\"].to(h.dtype)\n",
    "        return h\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wSrLxcM2rN9p"
   },
   "outputs": [],
   "source": [
    "def train_step(subject_model, batch, layer, start_pos, end_pos,\n",
    "               include_aux_loss=True, update_last_occ=True, aux_thresh=2e5, eps_aux=1e-4, k_aux=250):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_in = get_resid_stream_vector_efficient(\n",
    "            subject_model, batch, layer, start_pos, end_pos\n",
    "        ).to(device)  # (B, 16, d_model)\n",
    "\n",
    "    encoder_out, idx = encoder(encoder_in)\n",
    "    suffix = batch[:, -16:]\n",
    "    decoder_in = torch.cat([dummy, suffix], dim=-1)\n",
    "\n",
    "    patch_state[\"vecs\"] = encoder_out\n",
    "\n",
    "    label_ids = decoder_in.clone()\n",
    "    label_ids[:, :dummy.size(-1)] = -100\n",
    "\n",
    "    out = decoder(\n",
    "        input_ids=decoder_in,\n",
    "        labels=label_ids,\n",
    "        use_cache=False\n",
    "    )\n",
    "    ce_loss = out.loss\n",
    "\n",
    "    recent_concepts = torch.unique(idx.reshape(-1))\n",
    "\n",
    "    if update_last_occ:\n",
    "        recent_mask = torch.zeros((d_model * d_model_multiplier), device=device, dtype=torch.uint8)  # (2048*8,)\n",
    "        recent_mask[recent_concepts] = 1\n",
    "        dist.all_reduce(recent_mask, op=dist.ReduceOp.MAX)\n",
    "        global_recent = recent_mask.nonzero().squeeze(1)  # (2048*8,1) --> squeeze --> (2048*8,)\n",
    "        concepts_last_occ_by_seen_tokens[global_recent] = seen_tokens\n",
    "\n",
    "    window_start = max(0, seen_tokens - aux_thresh)\n",
    "    inactive = concepts_last_occ_by_seen_tokens < window_start\n",
    "\n",
    "    num_inactive = inactive.sum().item()\n",
    "    aux_loss = 0.0\n",
    "\n",
    "    if include_aux_loss:\n",
    "\n",
    "        W_inactive = encoder.module.w_enc.weight[inactive]  # (#inactive, d_model)\n",
    "        num_for_aux = W_inactive.size(0)\n",
    "\n",
    "        if num_for_aux > 0:\n",
    "            x_flat = encoder_in.reshape(-1, encoder_in.size(-1))  # (B*16, d_model)\n",
    "            dot = x_flat @ W_inactive.T  # (B*16, #inactive)\n",
    "\n",
    "            k_eff = min(num_for_aux, k_aux)\n",
    "            top_vals = torch.topk(dot, k_eff, dim=1).values\n",
    "\n",
    "            aux_loss = -(eps_aux / k_eff) * top_vals.sum(dim=1).mean()\n",
    "\n",
    "    return ce_loss + aux_loss, num_inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sEU3yeF0jYYE",
    "outputId": "6ff1d2cb-ad3a-435d-bec0-4b227e76fbef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 3it [00:34, 11.36s/it, loss=7.27]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-3632968432.py\", line 19, in __iter__\n    for item in ds:\n                ^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2347, in __iter__\n    yield from self._iter_pytorch()\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2262, in _iter_pytorch\n    for key, example in ex_iterable:\n                        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1882, in __iter__\n    for key, pa_table in self._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1905, in _iter_arrow\n    for key, pa_table in self.ex_iterable._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 499, in _iter_arrow\n    for key, pa_table in iterator:\n                         ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 151, in _convert_to_arrow\n    for key, example in iterator:\n                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1745, in __iter__\n    for key_example in islice(self.ex_iterable, self.n - ex_iterable_num_taken):\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1558, in __iter__\n    for x in self.ex_iterable:\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 325, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/parquet/parquet.py\", line 87, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/track.py\", line 49, in __iter__\n    for x in self.generator(*self.args):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1359, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: hf://datasets/HuggingFaceFW/fineweb@9bb295ddab0e05d785b879661af7260fed5140fc/data/CC-MAIN-2022-33/004_00048.parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2344785483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-3632968432.py\", line 19, in __iter__\n    for item in ds:\n                ^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2347, in __iter__\n    yield from self._iter_pytorch()\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2262, in _iter_pytorch\n    for key, example in ex_iterable:\n                        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1882, in __iter__\n    for key, pa_table in self._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1905, in _iter_arrow\n    for key, pa_table in self.ex_iterable._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 499, in _iter_arrow\n    for key, pa_table in iterator:\n                         ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 151, in _convert_to_arrow\n    for key, example in iterator:\n                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1745, in __iter__\n    for key_example in islice(self.ex_iterable, self.n - ex_iterable_num_taken):\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1558, in __iter__\n    for x in self.ex_iterable:\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 325, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/parquet/parquet.py\", line 87, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/track.py\", line 49, in __iter__\n    for x in self.generator(*self.args):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1359, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: hf://datasets/HuggingFaceFW/fineweb@9bb295ddab0e05d785b879661af7260fed5140fc/data/CC-MAIN-2022-33/004_00048.parquet\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "patience = 5\n",
    "curr_bad = 0\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "handle = decoder.module.base_model.model.model.embed_tokens.register_forward_hook(\n",
    "    patch_resid_stream_hook(patch_idx)\n",
    ")\n",
    "\n",
    "every_n_steps = 200\n",
    "inactive_concepts_n_steps = 100\n",
    "total_inactive_concepts = 0\n",
    "global_step = 0\n",
    "count_steps = 0\n",
    "\n",
    "stop_training = False\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader, start=1), desc=f\"epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, train_batch in pbar:\n",
    "        global_step += 1\n",
    "\n",
    "        train_batch = train_batch.to(device, non_blocking=True)\n",
    "\n",
    "        loss, num_inact_concepts = train_step(\n",
    "            subject, train_batch, layer, start_cropped_pos, end_cropped_pos\n",
    "        )\n",
    "        count_steps += 1\n",
    "        total_inactive_concepts += num_inact_concepts\n",
    "        seen_tokens += train_batch.size(0) * (cropped_len // 3) * world_size\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "        if global_step % inactive_concepts_n_steps == 0:\n",
    "            inactive_concepts_tracker.append((seen_tokens, total_inactive_concepts / count_steps))\n",
    "            total_inactive_concepts = 0\n",
    "            count_steps = 0\n",
    "\n",
    "        if step % every_n_steps == 0:\n",
    "\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            total = 0.0\n",
    "            n = 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch in tqdm(val_loader, desc=\"val\", leave=False):\n",
    "                    val_batch = val_batch.to(device, non_blocking=True)\n",
    "                    val_loss, _ = train_step(\n",
    "                        subject, val_batch, layer, start_cropped_pos, end_cropped_pos,\n",
    "                        include_aux_loss=False, update_last_occ=False)\n",
    "\n",
    "                    total += val_loss.item()\n",
    "                    n += 1\n",
    "\n",
    "            t = torch.tensor([total, n], device=device, dtype=torch.float64)\n",
    "            dist.all_reduce(t, op=dist.ReduceOp.SUM)\n",
    "            \n",
    "            val_mean = (t[0] / t[1]).item()\n",
    "\n",
    "            if rank == 0:\n",
    "                \n",
    "                if val_mean < best_val:\n",
    "                    best_val = val_mean\n",
    "                    curr_bad = 0\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"encoder\": encoder.module.state_dict(),\n",
    "                            \"decoder\": decoder.module.state_dict(),\n",
    "                            \"optim\": optim.state_dict(),\n",
    "                            \"epoch\": epoch,\n",
    "                            \"step\": step,\n",
    "                            \"best_val\": best_val,\n",
    "                            \"curr_bad\": curr_bad,\n",
    "                            \"concepts_last_occ_by_seen_tokens\": concepts_last_occ_by_seen_tokens,\n",
    "                            \"seen_tokens\": seen_tokens,\n",
    "                            \"inactive_concepts_tracker\": inactive_concepts_tracker\n",
    "                        },\n",
    "                        \"best_checkpoint.pt\",\n",
    "                    )\n",
    "    \n",
    "                else:\n",
    "                    curr_bad += 1\n",
    "                    if curr_bad >= patience:\n",
    "                        stop_training = True\n",
    "\n",
    "                pbar.set_postfix(loss=float(loss.item()), val=float(val_mean), best_val=float(best_val))\n",
    "            else:\n",
    "                pbar.set_postfix(loss=float(loss.item()))\n",
    "            \n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "\n",
    "        stop_flag = torch.tensor([1 if stop_training else 0], device=device)\n",
    "        dist.broadcast(stop_flag, src=0)\n",
    "        stop_training = bool(stop_flag.item())\n",
    "        \n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "    if stop_training:\n",
    "        break\n",
    "\n",
    "handle.remove()\n",
    "dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cd1RpIHlyD99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 -> NVIDIA A40\n",
      "cuda:1 -> NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"cuda:{i} -> {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
