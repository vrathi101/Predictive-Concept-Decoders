{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aV-I6YQz3I3U"
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir -U \"transformers>=4.51.0\" accelerate datasets torch pandas tqdm nnsight huggingface_hub peft\n",
    "!pip install --no-cache-dir typing-extensions --upgrade\n",
    "!pip uninstall -y torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ptcRh6uv1z42"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "# from nnsight import LanguageModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # other option: Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "53931fd35e5e4b008f1ed365a68b71af",
      "10dfda429d3d4ed7b10d931b38bb672d",
      "287fad4781ac41ffa6540e2ee13752f7",
      "67bb0fae78884bb9bac6cbcbc4ca8871",
      "ecb150261cf24b4b9041a7446444bf32",
      "594cd08d0f904b8bb1796d04b5b14c71",
      "019057c06cdd4b44a337557fea6609bd",
      "661c0445090247db9940c61b5c3bf620",
      "a73b6a178a234d028935dbf9086955f4",
      "9bc801c2ef324241be7d1d359b533e9e",
      "3408a621e4214ce996f688dc3b178911",
      "f8c9c121f9ea4249a3e13a1df268ccba",
      "befcc2d602914a2ba87d278e584bdabd",
      "ba5c490caeee49d1983067926a007f38",
      "1607d4c338524071a733ce0d1e35476a",
      "7ef9ad12ef644c80badeead162a18e9d",
      "0099c4dbd08343d0b044d01969f8a7b1",
      "5efe6f553f144a2d99ab9d2e454519ed",
      "45e2a55b0cd243a08fb472a40df0c506",
      "5c6229ea95a64dff81f1c078a8cd4280"
     ]
    },
    "id": "aJHE57iZCSwb",
    "outputId": "1b2a42c1-ae72-45d2-c7ed-43a374f75f2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53931fd35e5e4b008f1ed365a68b71af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uE3mAo003-Kz"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name=\"meta-llama/Llama-3.2-1B-Instruct\", attn_implementation=\"sdpa\", mode=\"eval\", **kwargs):\n",
    "    \"\"\"Load model and tokenizer with standard setup.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, config_dict) where config_dict has num_layers, num_heads, head_dim\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=attn_implementation,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    head_dim = model.config.hidden_size // num_heads\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    config = {\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"head_dim\": head_dim,\n",
    "    }\n",
    "\n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wcHtMKLW6zTf"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_name=\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True, first_k=int(1e5), buffer_frac=0.1, val_frac=0.05):\n",
    "    ds_stream = load_dataset(\n",
    "        dataset_name,\n",
    "        split=split,\n",
    "        streaming=streaming\n",
    "    )\n",
    "    ds_stream = ds_stream.shuffle(buffer_size=int(first_k * buffer_frac), seed=0)\n",
    "    ds_train = ds_stream.take(first_k)\n",
    "    ds_val = ds_stream.skip(first_k).take(int(first_k * val_frac))\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RaTH4YrM9y_4"
   },
   "outputs": [],
   "source": [
    "def get_cropped_text_ids(dataset, tokenizer, prefix_ids, cropped_len=48):\n",
    "    for item in dataset:\n",
    "        text = item[\"text\"]\n",
    "        text_ids = tokenizer(\n",
    "            text,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=False\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        if len(text_ids) >= cropped_len:\n",
    "            start = rng.randint(0, len(text_ids) - cropped_len)\n",
    "            selected_ids = text_ids[start:start + cropped_len]\n",
    "            yield prefix_ids + selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "0942b1988c1c4eeda667c5db81bc04a7",
      "d70ed9c8408e4a1abe588ffebc84e8b6",
      "d6b06a0b03dd4ebc93a9437b57346e7f",
      "a3ff263f2cc54c60b4cc9a280dc63ca8",
      "d850fdeafb04413f8115429934af88f4",
      "7ac81d50c30143b68423b882e9d37e3c",
      "d1051bf354364a4d9d7c8ed1d63a2da4",
      "4c3716da30e448ed939d6f5631d025a6",
      "04919139298942b99a660f5bc61cce62",
      "50d5aaf79a034437bf85251670ba8356",
      "42f667218c4741b5be88988e28f1aa3c",
      "fe2654afbf9d4f838a63bdb136b2186e",
      "7c1f3c1c72514833969ccf238d4210f1",
      "3e955f9746ac4d13b11ad0213c194e42",
      "aa0b0c34bbaa4b2090e5724c89af16aa",
      "e9866e8ab44a405e86da879a761930ec",
      "f69f01552d1743d59bb6738672f53cad",
      "393a0a1d140444c7a44b11711363f28d",
      "e68357c4f2494ef9821ed6eb1e3337ee",
      "ee0eeebd787f466294f7b3536025614f",
      "66a3fb70f0f14a4e820714a8f495c0e1",
      "58989bf8a5244d418647c7c8711dc2bc",
      "ddc00c46ac694e9bb70d4b0ee84b51c9",
      "c87ccd015f6440338d9a37091fb04604",
      "eee46f7a4b10433ba465e9b152d08f07",
      "7f2d3612698b4004bb79ae459d8f34ee",
      "c1fc1500dbc24741906b5b4a426bf1ee",
      "ba016ceaf58d487994c53d301ae4e1b9",
      "7d81e4f25df84191b5b7be21fc1bd804",
      "9209c383ba57422f9cef98f6ec96c763",
      "42f65d05518c4852886bdfed61424f2c",
      "729730f0b74340e1895c90ef8edb2bb3",
      "d31c8a0340c74411a63f842e5bf891d1",
      "2e4a247a1f8746338d4b4f660865d255",
      "4d7cfbef6d5d41a0a29361f45728a2ff",
      "efc57584738a41e8b4078e00a1944bea",
      "fbd022d35f2246e0888c8d557821aea9",
      "9c0c0839f3bf4b3db6c596bdf5e2dbdb",
      "735041947c904ec4bcebcc1fa7c0091a",
      "7605ccbae741402eaf25afefa8a75369",
      "245cc983d58642d19a8f698c50fa9242",
      "65c2942536eb4be18e8a4ceb1146e8af",
      "de29e1b58ca04fc3818a6696fe855577",
      "278e1700058449d4a22b20d8a7255074",
      "970a32d00f0f4e0abcc6cc4ec0189a02",
      "5143368f2e314e2daea484da3b588545",
      "c6e11c909920432f81199e4f9b5bfd26",
      "41f8d00cb6a9484b97c84cf93998625f",
      "a753e1197ddf4efa8e92c4e17fc90921",
      "f9ae5f8760654ea2b1b88207c6182402",
      "6dfd3107421a4f308bcb01e9bd2d09b1",
      "da04eac95e7b4ba3bc2a78305e49488a",
      "616ad8bb34b64c03ad85ab3a869eba28",
      "b63cbc6d2191473c95f94971fdf9c141",
      "e6abed246b9f4e81a5c15bb70a01a6cb",
      "38165237ab894c1186a772aaa8c45497",
      "b6f97e2f86f04f82a52490a36704c588",
      "d09561eacd5043cda61d27bd45d023cd",
      "53b863bafee84f73ae4d1e4f68b90c01",
      "896f5afbb52d40ec97e0e8114ccee39c",
      "321575caef5a4e38bf0708435f75044e",
      "81a571bfdb5448d28aae0cb46b57c4c9",
      "9f58ddedae0746d98c6274b78d4c850f",
      "333d03e2488249ef9be61ddb5f0a5aaa",
      "ed7709e0619042c598b121da1076a6a4",
      "4189510eadaf4cdf9bd5e2dafd1037ee"
     ]
    },
    "id": "8W9UOBC67ov9",
    "outputId": "c00066b2-3e97-4178-e85e-e818a1f2b3aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0942b1988c1c4eeda667c5db81bc04a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2654afbf9d4f838a63bdb136b2186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc00c46ac694e9bb70d4b0ee84b51c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a247a1f8746338d4b4f660865d255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970a32d00f0f4e0abcc6cc4ec0189a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38165237ab894c1186a772aaa8c45497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject, tokenizer, config = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hfeJo-3Uf5GC"
   },
   "outputs": [],
   "source": [
    "for p in subject.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "bc12929f25584d68a4d62d02d8d16e55",
      "58eb49612aa044de988496b007fdb1c6",
      "25cd624f0085440aa0d605340b8758a4",
      "408564fb9c914cbaa8c8bdb2e00650f1",
      "b21c8c39c1a44155917aba8d3a73f03d",
      "346f6fdd05d4402ebced88b629e6c0a5",
      "971fa4cc1cac45b392788af1dd59cd9f",
      "b94ef5bd123846fbb63c75e2bbdeaffb",
      "2a738098d47441c386b6b8c275f0e2ea",
      "ae63695bd476498ead3a8be7dcac4ef0",
      "6ff3ba00c1e04e64b65deb14e1a18949",
      "b112e57902f4415a898fe634aa8793e2",
      "61e50ba122fe4a348eb61acd52ae8b51",
      "14feea9eee1642f3baecaeda6cbc2c26",
      "f081569d4dfe49c19ffa54072affd38e",
      "617b35f0d5a843cf939e728c37ae63b3",
      "7b436aab3e8a4e0992deb52008ef5175",
      "6544221ed92040cb957cfd3a33dcaf1f",
      "d2b54a2eb6b64caaaada818c4f88b549",
      "c1c4aeb62cd74303804a6c867dd24ee0",
      "c41580dd8b114dd89677c87c12a6285f",
      "a5552e3376e3482d86bfc7743e1b95d0"
     ]
    },
    "id": "Kkq4Povq74GY",
    "outputId": "93659674-13c5-4782-b71c-e6e2003bf675"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc12929f25584d68a4d62d02d8d16e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b112e57902f4415a898fe634aa8793e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_val = load_data(first_k=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Tyqh8dKjSZVn"
   },
   "outputs": [],
   "source": [
    "nnsight_model = LanguageModel(subject, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IDMfp_wDK73k"
   },
   "outputs": [],
   "source": [
    "INSTRUCT_PREFIX = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "Cutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "prefix_ids = tokenizer(\n",
    "    INSTRUCT_PREFIX,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=False\n",
    ")[\"input_ids\"]\n",
    "len_prefix = len(prefix_ids)\n",
    "\n",
    "cropped_len = 48\n",
    "\n",
    "rng = random.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdllgwjD9zn7"
   },
   "outputs": [],
   "source": [
    "text = INSTRUCT_PREFIX + next(iter(ds_val))[\"text\"][:200]\n",
    "enc = tokenizer(\n",
    "    text,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=False\n",
    ")\n",
    "input_ids = enc[\"input_ids\"]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for a, b in zip(tokens, input_ids):\n",
    "    print(a, \"    \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnYxMC4kMw6B"
   },
   "outputs": [],
   "source": [
    "text = INSTRUCT_PREFIX + next(iter(ds_val))[\"text\"][:20]\n",
    "enc = tokenizer(\n",
    "    text,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=False\n",
    ")\n",
    "input_ids = enc[\"input_ids\"]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for a, b in zip(tokens, input_ids):\n",
    "    print(a, \"    \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KAFh6dxi9DlA"
   },
   "outputs": [],
   "source": [
    "cropped_data_ids = get_cropped_text_ids(ds_val, tokenizer, prefix_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pu2O26HUPzCd",
    "outputId": "bdc91aba-f2e0-4d20-a291-10745c3e3294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 9125, 128007, 1432, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 1432, 128009, 128006, 882, 128007, 271, 527, 1274, 889, 1505, 5694, 19596, 14918, 4245, 311, 264, 26682, 12205, 315, 6677, 922, 2574, 12765, 304, 279, 3596, 8915, 1917, 13, 578, 22963, 1917, 11031, 779, 5043, 1606, 315, 279, 502, 2574, 12765, 1855, 323, 1475, 2046, 4028, 279, 1917, 627, 2181, 1587, 539, 1935, 25294]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = next(iter(cropped_data_ids))\n",
    "print(li)\n",
    "tokens = tokenizer.convert_ids_to_tokens(li)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2ZVQIFq9wt1"
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "HKx4UYUBNs33",
    "outputId": "d5d65836-b94e-4c3b-d66c-0860776c199b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' are people who find themselves floating mainly due to a shallow wealth of knowledge about things happening in the ever dynamic world. The soccer world moves so fast because of the new things happening each and every week across the world.\\nIt does not take rocket'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(li[30:], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXkKXwg9OMY-",
    "outputId": "011d7d0b-e996-4ba9-f7fe-0c2fe355599c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630,\n",
       " 1630]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer(\n",
    "    \" X\" * 16,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=True\n",
    ")[\"input_ids\"]\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTr1j95HRoU4",
    "outputId": "3a48be59-6755-44c1-fcb0-bf63858dbb6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 2048])\n"
     ]
    }
   ],
   "source": [
    "prompt = li\n",
    "batch_ids = torch.tensor([prefix_ids, prefix_ids], device=subject.device)\n",
    "with nnsight_model.trace(batch_ids) as tracer:\n",
    "    resid = nnsight_model.model.layers[10].output[:].save()\n",
    "print(resid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o272BhXTUBnQ"
   },
   "outputs": [],
   "source": [
    "resid[0].float().cpu().numpy()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mtJpdxJjVry8"
   },
   "outputs": [],
   "source": [
    "class CroppedTokenDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, prefix_ids, cropped_len=48, mode=\"train\"):\n",
    "        self.ds = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prefix = torch.tensor(prefix_ids, dtype=torch.long)\n",
    "        self.cropped_len = cropped_len\n",
    "        self.mode = mode\n",
    "\n",
    "    def __iter__(self):\n",
    "        info = get_worker_info()\n",
    "        wid = 0 if info is None else info.id\n",
    "        nw  = 1 if info is None else info.num_workers\n",
    "\n",
    "        ds = self.ds if nw == 1 else self.ds.shard(num_shards=nw, index=wid)\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(0 if info is None else info.seed)\n",
    "\n",
    "        for item in ds:\n",
    "            text_ids = self.tokenizer(item[\"text\"], return_tensors=None, add_special_tokens=False)[\"input_ids\"]\n",
    "            if len(text_ids) >= self.cropped_len:\n",
    "                if self.mode == \"train\":\n",
    "                    start = int(torch.randint(0, len(text_ids) - self.cropped_len + 1, (1,), generator=g).item())\n",
    "                else:\n",
    "                    start = 0\n",
    "\n",
    "                cropped = torch.tensor(text_ids[start:start + self.cropped_len], dtype=torch.long)\n",
    "                yield torch.cat([self.prefix, cropped], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q6Nikwc6WihS"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_in=2048, multiplier=8, top_k=16):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.w_enc = nn.Linear(d_in, d_in * multiplier, bias=True)\n",
    "        self.w_emb = nn.Linear(d_in * multiplier, d_in, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            W = torch.randn_like(self.w_enc.weight)\n",
    "            W /= W.norm(dim=1, keepdim=True)\n",
    "            self.w_enc.weight.copy_(W)\n",
    "            self.w_enc.bias.zero_()\n",
    "            self.w_emb.weight.copy_(self.w_enc.weight.T)\n",
    "\n",
    "    def forward(self, x):  # (B, 16, d_in)\n",
    "        y = self.w_enc(x)  # (B, 16, d_in*mult)\n",
    "\n",
    "        idx = torch.topk(y, self.top_k, dim=-1).indices\n",
    "        mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "        mask.scatter_(-1, idx, True)\n",
    "        masked_y = y * mask.to(y.dtype)\n",
    "\n",
    "        out = self.w_emb(masked_y)  # (B, 16, d_in)\n",
    "        return out, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SYX0GX8lkZwX"
   },
   "outputs": [],
   "source": [
    "# def get_resid_stream_vector(layer, input_ids, prefix_ids, cropped_len):\n",
    "#     with nnsight_model.trace(input_ids):\n",
    "#         resid = nnsight_model.model.layers[layer].output[:]\n",
    "#         start = len(prefix_ids) + cropped_len // 3\n",
    "#         end = start + cropped_len // 3\n",
    "#         out = resid[:, start:end, :].save()\n",
    "#         return out\n",
    "\n",
    "def get_resid_stream_vector(model, input_ids, layer, start, end, attention_mask=None):\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "    resid = out.hidden_states[layer + 1]\n",
    "    return resid[:, start:end, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8fiGu8sQw1lk"
   },
   "outputs": [],
   "source": [
    "def get_resid_stream_vector_efficient(model, input_ids, layer, start, end, attention_mask=None):\n",
    "    saved = {}\n",
    "    def hook(module, inp, out):\n",
    "        saved[\"slice\"] = out[:, start:end, :].detach()\n",
    "\n",
    "    h = model.model.layers[layer].register_forward_hook(hook)\n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=False\n",
    "            )\n",
    "        return saved[\"slice\"]\n",
    "    finally:\n",
    "        h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "aEK_9UGhtEBI"
   },
   "outputs": [],
   "source": [
    "test = get_resid_stream_vector(subject, torch.tensor([[2040,3520]], device=\"cuda\"),3,0,5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Dr34CimygRvo"
   },
   "outputs": [],
   "source": [
    "decoder_base, _, _ = load_model_and_tokenizer(mode=\"train\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "decoder = get_peft_model(decoder_base, lora_cfg).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RC-LP6FuzVT",
    "outputId": "26862363-8396-4429-fb8c-a07999992344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mean/std: -1.779676676960662e-05 0.012750617228448391\n",
      "B mean/std: 0.0 0.0\n",
      "B all zero: True\n"
     ]
    }
   ],
   "source": [
    "A = decoder.base_model.model.model.layers[0].self_attn.q_proj.lora_A[\"default\"].weight.detach()\n",
    "B = decoder.base_model.model.model.layers[0].self_attn.q_proj.lora_B[\"default\"].weight.detach()\n",
    "\n",
    "print(\"A mean/std:\", A.mean().item(), A.std().item())\n",
    "print(\"B mean/std:\", B.mean().item(), B.std().item())\n",
    "print(\"B all zero:\", (B == 0).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bv-6fwdbdEaJ"
   },
   "outputs": [],
   "source": [
    "d_model = decoder.config.hidden_size\n",
    "d_model_multiplier = 8\n",
    "encoder = Encoder(d_in=d_model, multiplier=d_model_multiplier, top_k=16).to(decoder.device).to(torch.bfloat16)\n",
    "optim = torch.optim.AdamW(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "device = decoder.device\n",
    "start_cropped_pos = len_prefix + cropped_len // 3\n",
    "end_cropped_pos = start_cropped_pos + cropped_len // 3\n",
    "layer = 8\n",
    "batch_size=64\n",
    "dummy = tokenizer(\n",
    "    \" X\" * (cropped_len // 3),\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ")[\"input_ids\"].expand(batch_size, -1).to(device)\n",
    "patch_idx = torch.arange(16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sZ84-pLS4ncU"
   },
   "outputs": [],
   "source": [
    "concepts_last_occ_by_seen_tokens = torch.full(\n",
    "    (d_model * d_model_multiplier,),\n",
    "    -1,\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "seen_tokens = 0\n",
    "inactive_concepts_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rHyL6eKfb4OB"
   },
   "outputs": [],
   "source": [
    "pcd_train_ds = CroppedTokenDataset(\n",
    "    hf_dataset=ds_train,\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_ids=prefix_ids,\n",
    "    cropped_len=48,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "pcd_val_ds = CroppedTokenDataset(\n",
    "    hf_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_ids=prefix_ids,\n",
    "    cropped_len=48,\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "# 3) dataloader\n",
    "train_loader = DataLoader(\n",
    "    pcd_train_ds,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    pcd_val_ds,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EqxbWH3YTX1z"
   },
   "outputs": [],
   "source": [
    "patch_state = {\"vecs\": None}\n",
    "def patch_resid_stream_hook(idx):\n",
    "    def hook(module, inp, out):\n",
    "        h = out.clone()\n",
    "        h[:, idx, :] = patch_state[\"vecs\"].to(h.dtype)\n",
    "        return h\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wSrLxcM2rN9p"
   },
   "outputs": [],
   "source": [
    "def train_step(subject_model, batch, layer, start_pos, end_pos,\n",
    "               include_aux_loss=True, update_last_occ=True, aux_thresh=2e5, eps_aux=1e-4, k_aux=250):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_in = get_resid_stream_vector_efficient(\n",
    "            subject_model, batch, layer, start_pos, end_pos\n",
    "        )  # (B, 16, d_model)\n",
    "\n",
    "    encoder_out, idx = encoder(encoder_in)\n",
    "    suffix = batch[:, -16:]\n",
    "    decoder_in = torch.cat([dummy, suffix], dim=-1)\n",
    "\n",
    "    patch_state[\"vecs\"] = encoder_out\n",
    "\n",
    "    label_ids = decoder_in.clone()\n",
    "    label_ids[:, :dummy.size(-1)] = -100\n",
    "\n",
    "    out = decoder(\n",
    "        input_ids=decoder_in,\n",
    "        labels=label_ids,\n",
    "        use_cache=False\n",
    "    )\n",
    "    ce_loss = out.loss\n",
    "\n",
    "    recent_concepts = torch.unique(idx.reshape(-1))\n",
    "\n",
    "    if update_last_occ:\n",
    "        concepts_last_occ_by_seen_tokens[recent_concepts] = seen_tokens\n",
    "\n",
    "    window_start = max(0, seen_tokens - aux_thresh)\n",
    "    inactive = concepts_last_occ_by_seen_tokens < window_start\n",
    "\n",
    "    num_inactive = inactive.sum().item()\n",
    "    aux_loss = 0.0\n",
    "\n",
    "    if include_aux_loss:\n",
    "\n",
    "        W_inactive = encoder.w_enc.weight[inactive]  # (#inactive, d_model)\n",
    "        num_for_aux = W_inactive.size(0)\n",
    "\n",
    "        if num_for_aux > 0:\n",
    "            x_flat = encoder_in.reshape(-1, encoder_in.size(-1))  # (B*16, d_model)\n",
    "            dot = x_flat @ W_inactive.T  # (B*16, #inactive)\n",
    "\n",
    "            k_eff = min(num_for_aux, k_aux)\n",
    "            top_vals = torch.topk(dot, k_eff, dim=1).values\n",
    "\n",
    "            aux_loss = -(eps_aux / k_eff) * top_vals.sum(dim=1).mean()\n",
    "\n",
    "    return ce_loss + aux_loss, num_inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sEU3yeF0jYYE",
    "outputId": "6ff1d2cb-ad3a-435d-bec0-4b227e76fbef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 3it [00:34, 11.36s/it, loss=7.27]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-3632968432.py\", line 19, in __iter__\n    for item in ds:\n                ^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2347, in __iter__\n    yield from self._iter_pytorch()\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2262, in _iter_pytorch\n    for key, example in ex_iterable:\n                        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1882, in __iter__\n    for key, pa_table in self._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1905, in _iter_arrow\n    for key, pa_table in self.ex_iterable._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 499, in _iter_arrow\n    for key, pa_table in iterator:\n                         ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 151, in _convert_to_arrow\n    for key, example in iterator:\n                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1745, in __iter__\n    for key_example in islice(self.ex_iterable, self.n - ex_iterable_num_taken):\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1558, in __iter__\n    for x in self.ex_iterable:\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 325, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/parquet/parquet.py\", line 87, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/track.py\", line 49, in __iter__\n    for x in self.generator(*self.args):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1359, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: hf://datasets/HuggingFaceFW/fineweb@9bb295ddab0e05d785b879661af7260fed5140fc/data/CC-MAIN-2022-33/004_00048.parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2344785483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-3632968432.py\", line 19, in __iter__\n    for item in ds:\n                ^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2347, in __iter__\n    yield from self._iter_pytorch()\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2262, in _iter_pytorch\n    for key, example in ex_iterable:\n                        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1882, in __iter__\n    for key, pa_table in self._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1905, in _iter_arrow\n    for key, pa_table in self.ex_iterable._iter_arrow():\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 499, in _iter_arrow\n    for key, pa_table in iterator:\n                         ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 151, in _convert_to_arrow\n    for key, example in iterator:\n                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1745, in __iter__\n    for key_example in islice(self.ex_iterable, self.n - ex_iterable_num_taken):\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 1558, in __iter__\n    for x in self.ex_iterable:\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 325, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/packaged_modules/parquet/parquet.py\", line 87, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/track.py\", line 49, in __iter__\n    for x in self.generator(*self.args):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1359, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: hf://datasets/HuggingFaceFW/fineweb@9bb295ddab0e05d785b879661af7260fed5140fc/data/CC-MAIN-2022-33/004_00048.parquet\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "patience = 5\n",
    "curr_bad = 0\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "handle = decoder.base_model.model.model.embed_tokens.register_forward_hook(\n",
    "    patch_resid_stream_hook(patch_idx)\n",
    ")\n",
    "\n",
    "every_n_steps = 200\n",
    "inactive_concepts_n_steps = 100\n",
    "total_inactive_concepts = 0\n",
    "global_step = 0\n",
    "count_steps = 0\n",
    "\n",
    "stop_training = False\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader, start=1), desc=f\"epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, train_batch in pbar:\n",
    "        global_step += 1\n",
    "\n",
    "        train_batch = train_batch.to(device, non_blocking=True)\n",
    "\n",
    "        loss, num_inact_concepts = train_step(\n",
    "            subject, train_batch, layer, start_cropped_pos, end_cropped_pos\n",
    "        )\n",
    "        count_steps += 1\n",
    "        total_inactive_concepts += num_inact_concepts\n",
    "        seen_tokens += train_batch.size(0) * (cropped_len // 3)\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "        if global_step % inactive_concepts_n_steps == 0:\n",
    "            inactive_concepts_tracker.append((seen_tokens, total_inactive_concepts / count_steps))\n",
    "            total_inactive_concepts = 0\n",
    "            count_steps = 0\n",
    "\n",
    "        if step % every_n_steps == 0:\n",
    "\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            total = 0.0\n",
    "            n = 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch in tqdm(val_loader, desc=\"val\", leave=False):\n",
    "                    val_batch = val_batch.to(device, non_blocking=True)\n",
    "                    val_loss, _ = train_step(\n",
    "                        subject, val_batch, layer, start_cropped_pos, end_cropped_pos,\n",
    "                        include_aux_loss=False, update_last_occ=False)\n",
    "\n",
    "                    total += val_loss.item()\n",
    "                    n += 1\n",
    "\n",
    "            val_mean = total / n\n",
    "\n",
    "            if val_mean < best_val:\n",
    "                best_val = val_mean\n",
    "                curr_bad = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"encoder\": encoder.state_dict(),\n",
    "                        \"decoder\": decoder.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": step,\n",
    "                        \"best_val\": best_val,\n",
    "                        \"curr_bad\": curr_bad,\n",
    "                    },\n",
    "                    \"best_checkpoint.pt\",\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                curr_bad += 1\n",
    "                if curr_bad >= patience:\n",
    "                    stop_training = True\n",
    "                    break\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            pbar.set_postfix(loss=float(loss.item()), val=float(val_mean), best_val=float(best_val))\n",
    "\n",
    "    if stop_training:\n",
    "        print(\"Stopping training...\")\n",
    "        break\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd1RpIHlyD99"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}