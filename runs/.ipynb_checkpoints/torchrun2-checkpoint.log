W0104 06:23:02.270000 10397 torch/distributed/run.py:803] 
W0104 06:23:02.270000 10397 torch/distributed/run.py:803] *****************************************
W0104 06:23:02.270000 10397 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0104 06:23:02.270000 10397 torch/distributed/run.py:803] *****************************************
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 1/100: 0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
epoch 1/100: 0it [00:00, ?it/s]Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Failed to connect to remote data host. Retrying in 5sec [1/20]
epoch 1/100: 0it [00:19, ?it/s, loss=7.48]epoch 1/100: 0it [00:19, ?it/s, loss=6.83]epoch 1/100: 1it [00:19, 19.03s/it, loss=7.48]epoch 1/100: 1it [00:19, 19.02s/it, loss=6.83]epoch 1/100: 1it [00:19, 19.03s/it, loss=6.8] epoch 1/100: 1it [00:19, 19.02s/it, loss=7.15]epoch 1/100: 2it [00:19,  8.07s/it, loss=6.8]epoch 1/100: 2it [00:19,  8.06s/it, loss=7.15]epoch 1/100: 2it [00:19,  8.07s/it, loss=6.62]epoch 1/100: 2it [00:19,  8.06s/it, loss=6.78]epoch 1/100: 3it [00:19,  4.56s/it, loss=6.62]epoch 1/100: 3it [00:19,  4.56s/it, loss=6.78]epoch 1/100: 3it [00:20,  4.56s/it, loss=6.27]epoch 1/100: 3it [00:20,  4.56s/it, loss=6.23]epoch 1/100: 4it [00:20,  2.91s/it, loss=6.23]epoch 1/100: 4it [00:20,  2.91s/it, loss=6.27]epoch 1/100: 4it [00:20,  2.91s/it, loss=6.26]epoch 1/100: 4it [00:20,  2.91s/it, loss=6.28]epoch 1/100: 5it [00:20,  2.00s/it, loss=6.26]epoch 1/100: 5it [00:20,  2.00s/it, loss=6.28]epoch 1/100: 5it [00:20,  2.00s/it, loss=6.04]epoch 1/100: 5it [00:20,  2.00s/it, loss=6.13]epoch 1/100: 6it [00:20,  1.45s/it, loss=6.04]epoch 1/100: 6it [00:20,  1.45s/it, loss=6.13]epoch 1/100: 6it [00:21,  1.45s/it, loss=6.3] epoch 1/100: 6it [00:21,  1.45s/it, loss=6.17]epoch 1/100: 7it [00:21,  1.10s/it, loss=6.3]epoch 1/100: 7it [00:21,  1.10s/it, loss=6.17]epoch 1/100: 7it [00:21,  1.10s/it, loss=6.04]epoch 1/100: 7it [00:21,  1.10s/it, loss=6.16]epoch 1/100: 8it [00:21,  1.15it/s, loss=6.16]epoch 1/100: 8it [00:21,  1.15it/s, loss=6.04]epoch 1/100: 8it [00:22,  1.15it/s, loss=6.11]epoch 1/100: 8it [00:22,  1.15it/s, loss=6.2] epoch 1/100: 9it [00:22,  1.39it/s, loss=6.11]epoch 1/100: 9it [00:22,  1.39it/s, loss=6.2]epoch 1/100: 9it [00:22,  1.39it/s, loss=5.81]epoch 1/100: 9it [00:22,  1.39it/s, loss=5.89]epoch 1/100: 10it [00:22,  1.62it/s, loss=5.81]epoch 1/100: 10it [00:22,  1.62it/s, loss=5.89]epoch 1/100: 10it [00:22,  1.62it/s, loss=5.88]epoch 1/100: 10it [00:22,  1.62it/s, loss=5.87]epoch 1/100: 11it [00:22,  1.84it/s, loss=5.87]epoch 1/100: 11it [00:22,  1.84it/s, loss=5.88]epoch 1/100: 11it [00:23,  1.84it/s, loss=5.97]epoch 1/100: 11it [00:23,  1.84it/s, loss=5.91]epoch 1/100: 12it [00:23,  2.02it/s, loss=5.97]epoch 1/100: 12it [00:23,  2.02it/s, loss=5.91]epoch 1/100: 12it [00:23,  2.02it/s, loss=5.97]epoch 1/100: 12it [00:23,  2.02it/s, loss=6]   epoch 1/100: 13it [00:23,  2.17it/s, loss=5.97]epoch 1/100: 13it [00:23,  2.17it/s, loss=6]epoch 1/100: 13it [00:24,  2.17it/s, loss=5.73]epoch 1/100: 13it [00:24,  2.17it/s, loss=5.98]epoch 1/100: 14it [00:24,  2.29it/s, loss=5.73]epoch 1/100: 14it [00:24,  2.29it/s, loss=5.98]epoch 1/100: 14it [00:24,  2.29it/s, loss=5.84]epoch 1/100: 14it [00:24,  2.29it/s, loss=5.66]epoch 1/100: 15it [00:24,  2.38it/s, loss=5.84]epoch 1/100: 15it [00:24,  2.38it/s, loss=5.66]epoch 1/100: 15it [00:24,  2.38it/s, loss=5.79]epoch 1/100: 15it [00:24,  2.38it/s, loss=5.7] epoch 1/100: 16it [00:24,  2.44it/s, loss=5.7]epoch 1/100: 16it [00:24,  2.44it/s, loss=5.79]epoch 1/100: 16it [00:25,  2.44it/s, loss=5.87]epoch 1/100: 16it [00:25,  2.44it/s, loss=5.69]epoch 1/100: 17it [00:25,  2.49it/s, loss=5.87]epoch 1/100: 17it [00:25,  2.49it/s, loss=5.69]epoch 1/100: 17it [00:25,  2.49it/s, loss=5.64]epoch 1/100: 17it [00:25,  2.49it/s, loss=5.87]epoch 1/100: 18it [00:25,  2.53it/s, loss=5.87]epoch 1/100: 18it [00:25,  2.53it/s, loss=5.64]epoch 1/100: 18it [00:25,  2.53it/s, loss=5.68]epoch 1/100: 18it [00:25,  2.53it/s, loss=5.82]epoch 1/100: 19it [00:25,  2.56it/s, loss=5.82]epoch 1/100: 19it [00:25,  2.56it/s, loss=5.68]epoch 1/100: 19it [00:26,  2.56it/s, loss=5.49]epoch 1/100: 19it [00:26,  2.56it/s, loss=5.7] epoch 1/100: 20it [00:26,  2.58it/s, loss=5.7]epoch 1/100: 20it [00:26,  2.58it/s, loss=5.49]epoch 1/100: 20it [00:26,  2.58it/s, loss=5.64]epoch 1/100: 20it [00:26,  2.58it/s, loss=5.52]epoch 1/100: 21it [00:26,  2.59it/s, loss=5.64]epoch 1/100: 21it [00:26,  2.59it/s, loss=5.52]epoch 1/100: 21it [00:27,  2.59it/s, loss=5.47]epoch 1/100: 21it [00:27,  2.59it/s, loss=5.4] epoch 1/100: 22it [00:27,  2.60it/s, loss=5.47]epoch 1/100: 22it [00:27,  2.60it/s, loss=5.4]epoch 1/100: 22it [00:27,  2.60it/s, loss=5.38]epoch 1/100: 22it [00:27,  2.60it/s, loss=5.57]epoch 1/100: 23it [00:27,  2.61it/s, loss=5.57]epoch 1/100: 23it [00:27,  2.61it/s, loss=5.38]epoch 1/100: 23it [00:27,  2.61it/s, loss=5.43]epoch 1/100: 23it [00:27,  2.61it/s, loss=5.48]epoch 1/100: 24it [00:27,  2.61it/s, loss=5.48]epoch 1/100: 24it [00:27,  2.61it/s, loss=5.43]epoch 1/100: 24it [00:28,  2.61it/s, loss=5.54]epoch 1/100: 24it [00:28,  2.61it/s, loss=5.5] 
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:18, 18.72s/it][A
val: 1it [00:18, 18.72s/it][A
val: 2it [00:18,  7.86s/it][A
val: 2it [00:18,  7.86s/it][A
val: 3it [00:19,  4.38s/it][A
val: 3it [00:19,  4.38s/it][A
val: 4it [00:19,  2.75s/it][A
val: 4it [00:19,  2.75s/it][A
val: 5it [00:19,  1.85s/it][A
val: 5it [00:19,  1.85s/it][A
val: 6it [00:19,  1.30s/it][A
val: 6it [00:19,  1.31s/it][A
val: 7it [00:20,  1.04it/s][A
val: 7it [00:20,  1.04it/s][A
val: 8it [00:20,  1.36it/s][A
val: 8it [00:20,  1.36it/s][A
val: 9it [00:20,  1.72it/s][A
val: 9it [00:20,  1.72it/s][A
val: 10it [00:20,  2.09it/s][A
val: 10it [00:20,  2.09it/s][A
val: 11it [00:21,  2.45it/s][A
val: 11it [00:21,  2.45it/s][A
val: 12it [00:21,  2.78it/s][A
val: 12it [00:21,  2.78it/s][A
val: 13it [00:21,  3.07it/s][A
val: 13it [00:21,  3.07it/s][A
val: 14it [00:21,  3.30it/s][A
val: 14it [00:21,  3.30it/s][A
val: 15it [00:22,  3.49it/s][A
val: 15it [00:22,  3.49it/s][A
val: 16it [00:22,  3.64it/s][A
val: 16it [00:22,  3.64it/s][A
val: 17it [00:22,  3.74it/s][A
val: 17it [00:22,  3.74it/s][A
val: 18it [00:22,  3.82it/s][A
val: 18it [00:22,  3.82it/s][A
val: 19it [00:23,  3.88it/s][A
val: 19it [00:23,  3.88it/s][A
val: 20it [00:23,  3.92it/s][A
val: 20it [00:23,  3.92it/s][A
val: 21it [00:23,  3.95it/s][A
val: 21it [00:23,  3.95it/s][A
val: 22it [00:23,  3.97it/s][A
val: 22it [00:23,  3.98it/s][A
val: 23it [00:24,  3.98it/s][A
val: 23it [00:24,  4.00it/s][A
val: 24it [00:24,  3.99it/s][A
val: 24it [00:24,  4.01it/s][A
                            [A
                            [Aepoch 1/100: 24it [00:52,  2.61it/s, loss=5.5]epoch 1/100: 24it [01:01,  2.61it/s, best_val=5.4, loss=5.54, val=5.4]epoch 1/100: 25it [01:01, 10.37s/it, loss=5.5]epoch 1/100: 25it [01:01, 10.37s/it, best_val=5.4, loss=5.54, val=5.4]epoch 1/100: 25it [01:01, 10.37s/it, loss=5.31]epoch 1/100: 25it [01:01, 10.37s/it, loss=5.15]                       epoch 1/100: 26it [01:01,  7.37s/it, loss=5.31]epoch 1/100: 26it [01:01,  7.37s/it, loss=5.15]epoch 1/100: 26it [01:02,  7.37s/it, loss=5.35]epoch 1/100: 26it [01:02,  7.37s/it, loss=5.39]epoch 1/100: 27it [01:02,  5.28s/it, loss=5.35]epoch 1/100: 27it [01:02,  5.28s/it, loss=5.39]epoch 1/100: 27it [01:02,  5.28s/it, loss=5.3] epoch 1/100: 27it [01:02,  5.28s/it, loss=5.46]epoch 1/100: 28it [01:02,  3.81s/it, loss=5.3]epoch 1/100: 28it [01:02,  3.81s/it, loss=5.46]epoch 1/100: 28it [01:03,  3.81s/it, loss=5.4]epoch 1/100: 28it [01:03,  3.81s/it, loss=5.34]epoch 1/100: 29it [01:03,  2.78s/it, loss=5.4]epoch 1/100: 29it [01:03,  2.78s/it, loss=5.34]epoch 1/100: 29it [01:03,  2.78s/it, loss=5.38]epoch 1/100: 29it [01:03,  2.78s/it, loss=5.36]epoch 1/100: 30it [01:03,  2.06s/it, loss=5.38]epoch 1/100: 30it [01:03,  2.06s/it, loss=5.36]epoch 1/100: 30it [01:03,  2.06s/it, loss=5.3] epoch 1/100: 30it [01:03,  2.06s/it, loss=5.45]epoch 1/100: 31it [01:03,  1.56s/it, loss=5.3]epoch 1/100: 31it [01:03,  1.56s/it, loss=5.45]epoch 1/100: 31it [01:04,  1.56s/it, loss=5.28]epoch 1/100: 31it [01:04,  1.56s/it, loss=5.18]epoch 1/100: 32it [01:04,  1.21s/it, loss=5.28]epoch 1/100: 32it [01:04,  1.21s/it, loss=5.18]epoch 1/100: 32it [01:04,  1.21s/it, loss=5.4] epoch 1/100: 32it [01:04,  1.21s/it, loss=5.38]epoch 1/100: 33it [01:04,  1.04it/s, loss=5.4]epoch 1/100: 33it [01:04,  1.04it/s, loss=5.38]epoch 1/100: 33it [01:04,  1.04it/s, loss=5.18]epoch 1/100: 33it [01:04,  1.04it/s, loss=5.32]epoch 1/100: 34it [01:04,  1.27it/s, loss=5.18]epoch 1/100: 34it [01:04,  1.27it/s, loss=5.32]epoch 1/100: 34it [01:05,  1.27it/s, loss=5.34]epoch 1/100: 34it [01:05,  1.27it/s, loss=5.3] epoch 1/100: 35it [01:05,  1.50it/s, loss=5.34]epoch 1/100: 35it [01:05,  1.50it/s, loss=5.3]epoch 1/100: 35it [01:05,  1.50it/s, loss=5.46]epoch 1/100: 35it [01:05,  1.50it/s, loss=5.22]epoch 1/100: 36it [01:05,  1.72it/s, loss=5.46]epoch 1/100: 36it [01:05,  1.72it/s, loss=5.22]epoch 1/100: 36it [01:06,  1.72it/s, loss=5.01]epoch 1/100: 36it [01:06,  1.72it/s, loss=5.43]epoch 1/100: 37it [01:06,  1.91it/s, loss=5.01]epoch 1/100: 37it [01:06,  1.91it/s, loss=5.43]epoch 1/100: 37it [01:06,  1.91it/s, loss=5.49]epoch 1/100: 37it [01:06,  1.91it/s, loss=5.05]epoch 1/100: 38it [01:06,  2.08it/s, loss=5.49]epoch 1/100: 38it [01:06,  2.08it/s, loss=5.05]epoch 1/100: 38it [01:06,  2.08it/s, loss=5.38]epoch 1/100: 38it [01:06,  2.08it/s, loss=4.95]epoch 1/100: 39it [01:06,  2.21it/s, loss=5.38]epoch 1/100: 39it [01:06,  2.21it/s, loss=4.95]epoch 1/100: 39it [01:07,  2.21it/s, loss=5.17]epoch 1/100: 39it [01:07,  2.21it/s, loss=4.97]epoch 1/100: 40it [01:07,  2.31it/s, loss=4.97]epoch 1/100: 40it [01:07,  2.31it/s, loss=5.17]epoch 1/100: 40it [01:07,  2.31it/s, loss=5.09]epoch 1/100: 40it [01:07,  2.31it/s, loss=5.41]epoch 1/100: 41it [01:07,  2.39it/s, loss=5.41]epoch 1/100: 41it [01:07,  2.39it/s, loss=5.09]epoch 1/100: 41it [01:08,  2.39it/s, loss=5.17]epoch 1/100: 41it [01:08,  2.39it/s, loss=5.01]epoch 1/100: 42it [01:08,  2.45it/s, loss=5.17]epoch 1/100: 42it [01:08,  2.45it/s, loss=5.01]epoch 1/100: 42it [01:08,  2.45it/s, loss=5.2] epoch 1/100: 42it [01:08,  2.45it/s, loss=5.32]epoch 1/100: 43it [01:08,  2.49it/s, loss=5.2]epoch 1/100: 43it [01:08,  2.49it/s, loss=5.32]epoch 1/100: 43it [01:08,  2.49it/s, loss=4.98]epoch 1/100: 43it [01:08,  2.49it/s, loss=5.1] epoch 1/100: 44it [01:08,  2.52it/s, loss=4.98]epoch 1/100: 44it [01:08,  2.52it/s, loss=5.1]epoch 1/100: 44it [01:09,  2.52it/s, loss=5.08]epoch 1/100: 44it [01:09,  2.52it/s, loss=5.4]epoch 1/100: 45it [01:09,  2.55it/s, loss=5.08]epoch 1/100: 45it [01:09,  2.55it/s, loss=5.4]epoch 1/100: 45it [01:09,  2.55it/s, loss=5.23]epoch 1/100: 45it [01:09,  2.55it/s, loss=4.85]epoch 1/100: 46it [01:09,  2.56it/s, loss=5.23]epoch 1/100: 46it [01:09,  2.56it/s, loss=4.85]epoch 1/100: 46it [01:09,  2.56it/s, loss=5.03]epoch 1/100: 46it [01:09,  2.56it/s, loss=5.2] epoch 1/100: 47it [01:09,  2.57it/s, loss=5.03]epoch 1/100: 47it [01:09,  2.57it/s, loss=5.2]epoch 1/100: 47it [01:10,  2.57it/s, loss=5.06]epoch 1/100: 47it [01:10,  2.57it/s, loss=4.95]epoch 1/100: 48it [01:10,  2.58it/s, loss=4.95]epoch 1/100: 48it [01:10,  2.58it/s, loss=5.06]epoch 1/100: 48it [01:10,  2.58it/s, loss=5.08]epoch 1/100: 48it [01:10,  2.58it/s, loss=5.17]epoch 1/100: 49it [01:10,  2.59it/s, loss=5.08]epoch 1/100: 49it [01:10,  2.59it/s, loss=5.17]epoch 1/100: 49it [01:11,  2.59it/s, loss=5.28]epoch 1/100: 49it [01:11,  2.59it/s, loss=5.18]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.07s/it][A
val: 1it [00:03,  3.31s/it][A
val: 2it [00:03,  1.41s/it][A
val: 3it [00:03,  1.14it/s][A
val: 4it [00:03,  1.59it/s][A
val: 5it [00:04,  2.03it/s][A
val: 6it [00:04,  2.44it/s][A
val: 7it [00:04,  2.80it/s][A
val: 8it [00:04,  3.10it/s][A
val: 9it [00:05,  3.34it/s][A
val: 10it [00:05,  3.51it/s][A
val: 11it [00:05,  3.65it/s][A
val: 12it [00:05,  3.75it/s][A
val: 13it [00:06,  3.82it/s][A
val: 14it [00:06,  3.87it/s][A
val: 15it [00:06,  3.91it/s][A
val: 16it [00:06,  3.94it/s][A
val: 17it [00:07,  3.96it/s][A
val: 2it [00:07,  3.58s/it][A
val: 18it [00:07,  3.98it/s][A
val: 3it [00:07,  2.06s/it][A
val: 19it [00:07,  3.99it/s][A
val: 4it [00:07,  1.35s/it][A
val: 20it [00:07,  3.99it/s][A
val: 5it [00:07,  1.05it/s][A
val: 21it [00:08,  4.00it/s][A
val: 6it [00:08,  1.40it/s][A
val: 22it [00:08,  4.00it/s][A
val: 7it [00:08,  1.78it/s][A
val: 23it [00:08,  4.01it/s][A
val: 8it [00:08,  2.16it/s][A
val: 24it [00:08,  4.00it/s][A
val: 9it [00:08,  2.52it/s][A
                            [A
val: 10it [00:09,  2.84it/s][A
val: 11it [00:09,  3.12it/s][A
val: 12it [00:09,  3.34it/s][A
val: 13it [00:09,  3.51it/s][A
val: 14it [00:10,  3.64it/s][A
val: 15it [00:10,  3.74it/s][A
val: 16it [00:10,  3.81it/s][A
val: 17it [00:10,  3.86it/s][A
val: 18it [00:11,  3.90it/s][A
val: 19it [00:11,  3.93it/s][A
val: 20it [00:11,  3.95it/s][A
val: 21it [00:11,  3.96it/s][A
val: 22it [00:12,  3.97it/s][A
val: 23it [00:12,  3.98it/s][A
val: 24it [00:12,  3.98it/s][A
                            [Aepoch 1/100: 49it [01:23,  2.59it/s, loss=5.18]epoch 1/100: 49it [01:33,  2.59it/s, best_val=5.07, loss=5.28, val=5.07]epoch 1/100: 50it [01:33,  6.96s/it, loss=5.18]epoch 1/100: 50it [01:33,  6.96s/it, best_val=5.07, loss=5.28, val=5.07]epoch 1/100: 50it [01:33,  6.96s/it, loss=4.99]                         epoch 1/100: 50it [01:33,  6.96s/it, loss=5.23]epoch 1/100: 51it [01:33,  4.99s/it, loss=5.23]epoch 1/100: 51it [01:33,  4.99s/it, loss=4.99]epoch 1/100: 51it [01:33,  4.99s/it, loss=5.31]epoch 1/100: 51it [01:33,  4.99s/it, loss=5.13]epoch 1/100: 52it [01:33,  3.61s/it, loss=5.13]epoch 1/100: 52it [01:33,  3.61s/it, loss=5.31]epoch 1/100: 52it [01:34,  3.61s/it, loss=5.03]epoch 1/100: 52it [01:34,  3.61s/it, loss=5.28]epoch 1/100: 53it [01:34,  2.64s/it, loss=5.03]epoch 1/100: 53it [01:34,  2.64s/it, loss=5.28]epoch 1/100: 53it [01:34,  2.64s/it, loss=4.9] epoch 1/100: 53it [01:34,  2.64s/it, loss=5.01]epoch 1/100: 54it [01:34,  1.96s/it, loss=5.01]epoch 1/100: 54it [01:34,  1.96s/it, loss=4.9]epoch 1/100: 54it [01:34,  1.96s/it, loss=5.14]epoch 1/100: 54it [01:34,  1.96s/it, loss=5.1]epoch 1/100: 55it [01:34,  1.49s/it, loss=5.14]epoch 1/100: 55it [01:34,  1.49s/it, loss=5.1]epoch 1/100: 55it [01:35,  1.49s/it, loss=5.08]epoch 1/100: 55it [01:35,  1.49s/it, loss=5.17]epoch 1/100: 56it [01:35,  1.16s/it, loss=5.17]epoch 1/100: 56it [01:35,  1.16s/it, loss=5.08]epoch 1/100: 56it [01:35,  1.16s/it, loss=4.85]epoch 1/100: 56it [01:35,  1.16s/it, loss=4.97]epoch 1/100: 57it [01:35,  1.08it/s, loss=4.85]epoch 1/100: 57it [01:35,  1.08it/s, loss=4.97]epoch 1/100: 57it [01:36,  1.08it/s, loss=5.13]epoch 1/100: 57it [01:36,  1.08it/s, loss=5.16]epoch 1/100: 58it [01:36,  1.31it/s, loss=5.16]epoch 1/100: 58it [01:36,  1.31it/s, loss=5.13]epoch 1/100: 58it [01:36,  1.31it/s, loss=5.01]epoch 1/100: 58it [01:36,  1.31it/s, loss=5.03]epoch 1/100: 59it [01:36,  1.54it/s, loss=5.03]epoch 1/100: 59it [01:36,  1.54it/s, loss=5.01]epoch 1/100: 59it [01:36,  1.54it/s, loss=5.08]epoch 1/100: 59it [01:36,  1.54it/s, loss=5.19]epoch 1/100: 60it [01:36,  1.76it/s, loss=5.19]epoch 1/100: 60it [01:36,  1.76it/s, loss=5.08]epoch 1/100: 60it [01:37,  1.76it/s, loss=4.9] epoch 1/100: 60it [01:37,  1.76it/s, loss=4.93]epoch 1/100: 61it [01:37,  1.95it/s, loss=4.9]epoch 1/100: 61it [01:37,  1.95it/s, loss=4.93]epoch 1/100: 61it [01:37,  1.95it/s, loss=5.07]epoch 1/100: 61it [01:37,  1.95it/s, loss=5.19]epoch 1/100: 62it [01:37,  2.11it/s, loss=5.07]epoch 1/100: 62it [01:37,  2.11it/s, loss=5.19]epoch 1/100: 62it [01:38,  2.11it/s, loss=4.94]epoch 1/100: 62it [01:38,  2.11it/s, loss=5.02]epoch 1/100: 63it [01:38,  2.24it/s, loss=5.02]epoch 1/100: 63it [01:38,  2.24it/s, loss=4.94]epoch 1/100: 63it [01:38,  2.24it/s, loss=5.12]epoch 1/100: 63it [01:38,  2.24it/s, loss=5.18]epoch 1/100: 64it [01:38,  2.34it/s, loss=5.12]epoch 1/100: 64it [01:38,  2.34it/s, loss=5.18]epoch 1/100: 64it [01:38,  2.34it/s, loss=5.09]epoch 1/100: 64it [01:38,  2.34it/s, loss=5]   epoch 1/100: 65it [01:38,  2.41it/s, loss=5]epoch 1/100: 65it [01:38,  2.41it/s, loss=5.09]epoch 1/100: 65it [01:39,  2.41it/s, loss=5.13]epoch 1/100: 65it [01:39,  2.41it/s, loss=4.92]epoch 1/100: 66it [01:39,  2.46it/s, loss=4.92]epoch 1/100: 66it [01:39,  2.46it/s, loss=5.13]epoch 1/100: 66it [01:39,  2.46it/s, loss=5]   epoch 1/100: 66it [01:39,  2.46it/s, loss=5.1] epoch 1/100: 67it [01:39,  2.51it/s, loss=5.1]epoch 1/100: 67it [01:39,  2.51it/s, loss=5]epoch 1/100: 67it [01:39,  2.51it/s, loss=4.94]epoch 1/100: 67it [01:39,  2.51it/s, loss=4.93]epoch 1/100: 68it [01:39,  2.53it/s, loss=4.93]epoch 1/100: 68it [01:39,  2.53it/s, loss=4.94]epoch 1/100: 68it [01:40,  2.53it/s, loss=4.85]epoch 1/100: 68it [01:40,  2.53it/s, loss=5.19]epoch 1/100: 69it [01:40,  2.56it/s, loss=4.85]epoch 1/100: 69it [01:40,  2.56it/s, loss=5.19]epoch 1/100: 69it [01:40,  2.56it/s, loss=4.97]epoch 1/100: 69it [01:40,  2.56it/s, loss=5.11]epoch 1/100: 70it [01:40,  2.57it/s, loss=4.97]epoch 1/100: 70it [01:40,  2.57it/s, loss=5.11]epoch 1/100: 70it [01:41,  2.57it/s, loss=5.12]epoch 1/100: 70it [01:41,  2.57it/s, loss=5.12]epoch 1/100: 71it [01:41,  2.58it/s, loss=5.12]epoch 1/100: 71it [01:41,  2.58it/s, loss=5.12]epoch 1/100: 71it [01:41,  2.58it/s, loss=5]   epoch 1/100: 71it [01:41,  2.58it/s, loss=4.82]epoch 1/100: 72it [01:41,  2.59it/s, loss=4.82]epoch 1/100: 72it [01:41,  2.59it/s, loss=5]epoch 1/100: 72it [01:41,  2.59it/s, loss=5.12]epoch 1/100: 72it [01:41,  2.59it/s, loss=4.81]epoch 1/100: 73it [01:41,  2.59it/s, loss=5.12]epoch 1/100: 73it [01:41,  2.59it/s, loss=4.81]epoch 1/100: 73it [01:42,  2.59it/s, loss=4.92]epoch 1/100: 73it [01:42,  2.59it/s, loss=5.01]epoch 1/100: 74it [01:42,  2.59it/s, loss=5.01]epoch 1/100: 74it [01:42,  2.59it/s, loss=4.92]epoch 1/100: 74it [01:42,  2.59it/s, loss=5.29]epoch 1/100: 74it [01:42,  2.59it/s, loss=5.08]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:04,  4.22s/it][A
val: 1it [00:04,  4.23s/it][A
val: 2it [00:04,  1.89s/it][A
val: 2it [00:04,  1.89s/it][A
val: 3it [00:04,  1.14s/it][A
val: 3it [00:04,  1.14s/it][A
val: 4it [00:04,  1.27it/s][A
val: 4it [00:04,  1.27it/s][A
val: 5it [00:05,  1.68it/s][A
val: 5it [00:05,  1.68it/s][A
val: 6it [00:05,  2.09it/s][A
val: 6it [00:05,  2.09it/s][A
val: 7it [00:05,  2.48it/s][A
val: 7it [00:05,  2.47it/s][A
val: 8it [00:05,  2.81it/s][A
val: 8it [00:05,  2.81it/s][A
val: 9it [00:06,  3.10it/s][A
val: 9it [00:06,  3.09it/s][A
val: 10it [00:06,  3.32it/s][A
val: 10it [00:06,  2.75it/s][A
val: 11it [00:06,  3.49it/s][A
val: 11it [00:06,  3.04it/s][A
val: 12it [00:06,  3.62it/s][A
val: 12it [00:07,  3.28it/s][A
val: 13it [00:07,  3.72it/s][A
val: 13it [00:07,  3.46it/s][A
val: 14it [00:07,  3.79it/s][A
val: 14it [00:07,  3.61it/s][A
val: 15it [00:07,  3.84it/s][A
val: 15it [00:07,  3.71it/s][A
val: 16it [00:08,  3.88it/s][A
val: 16it [00:08,  3.78it/s][A
val: 17it [00:08,  3.90it/s][A
val: 17it [00:08,  3.84it/s][A
val: 18it [00:08,  3.92it/s][A
val: 18it [00:08,  3.88it/s][A
val: 19it [00:08,  3.93it/s][A
val: 19it [00:08,  3.91it/s][A
val: 20it [00:09,  3.95it/s][A
val: 20it [00:09,  3.93it/s][A
val: 21it [00:09,  3.96it/s][A
val: 21it [00:09,  3.95it/s][A
val: 22it [00:09,  3.96it/s][A
val: 22it [00:09,  3.95it/s][A
val: 23it [00:09,  3.97it/s][A
val: 23it [00:09,  3.96it/s][A
val: 24it [00:10,  3.98it/s][A
                            [A
val: 24it [00:10,  3.97it/s][A
                            [Aepoch 1/100: 74it [01:53,  2.59it/s, loss=5.08]epoch 1/100: 74it [02:02,  2.59it/s, best_val=4.94, loss=5.29, val=4.94]epoch 1/100: 75it [02:02,  6.21s/it, loss=5.08]epoch 1/100: 75it [02:02,  6.21s/it, best_val=4.94, loss=5.29, val=4.94]epoch 1/100: 75it [02:02,  6.21s/it, loss=4.98]                         epoch 1/100: 75it [02:02,  6.21s/it, loss=4.66]epoch 1/100: 76it [02:02,  4.46s/it, loss=4.66]epoch 1/100: 76it [02:02,  4.46s/it, loss=4.98]epoch 1/100: 76it [02:02,  4.46s/it, loss=4.92]epoch 1/100: 76it [02:02,  4.46s/it, loss=4.76]epoch 1/100: 77it [02:02,  3.24s/it, loss=4.76]epoch 1/100: 77it [02:02,  3.24s/it, loss=4.92]epoch 1/100: 77it [02:03,  3.24s/it, loss=5.08]epoch 1/100: 77it [02:03,  3.24s/it, loss=5.14]epoch 1/100: 78it [02:03,  2.38s/it, loss=5.08]epoch 1/100: 78it [02:03,  2.38s/it, loss=5.14]epoch 1/100: 78it [02:03,  2.38s/it, loss=4.9] epoch 1/100: 78it [02:03,  2.38s/it, loss=4.99]epoch 1/100: 79it [02:03,  1.78s/it, loss=4.9]epoch 1/100: 79it [02:03,  1.78s/it, loss=4.99]epoch 1/100: 79it [02:03,  1.78s/it, loss=5.02]epoch 1/100: 79it [02:03,  1.78s/it, loss=5.03]epoch 1/100: 80it [02:03,  1.36s/it, loss=5.02]epoch 1/100: 80it [02:03,  1.36s/it, loss=5.03]epoch 1/100: 80it [02:04,  1.36s/it, loss=4.81]epoch 1/100: 80it [02:04,  1.36s/it, loss=4.88]epoch 1/100: 81it [02:04,  1.07s/it, loss=4.81]epoch 1/100: 81it [02:04,  1.07s/it, loss=4.88]epoch 1/100: 81it [02:04,  1.07s/it, loss=4.74]epoch 1/100: 81it [02:04,  1.07s/it, loss=4.79]epoch 1/100: 82it [02:04,  1.16it/s, loss=4.74]epoch 1/100: 82it [02:04,  1.16it/s, loss=4.79]epoch 1/100: 82it [02:05,  1.16it/s, loss=4.94]epoch 1/100: 82it [02:05,  1.16it/s, loss=4.96]epoch 1/100: 83it [02:05,  1.39it/s, loss=4.94]epoch 1/100: 83it [02:05,  1.39it/s, loss=4.96]epoch 1/100: 83it [02:05,  1.39it/s, loss=4.82]epoch 1/100: 83it [02:05,  1.39it/s, loss=4.96]epoch 1/100: 84it [02:05,  1.61it/s, loss=4.96]epoch 1/100: 84it [02:05,  1.61it/s, loss=4.82]epoch 1/100: 84it [02:05,  1.61it/s, loss=4.79]epoch 1/100: 84it [02:05,  1.61it/s, loss=4.83]epoch 1/100: 85it [02:05,  1.82it/s, loss=4.79]epoch 1/100: 85it [02:05,  1.82it/s, loss=4.83]epoch 1/100: 85it [02:06,  1.82it/s, loss=4.77]epoch 1/100: 85it [02:06,  1.82it/s, loss=4.97]epoch 1/100: 86it [02:06,  2.00it/s, loss=4.97]epoch 1/100: 86it [02:06,  2.00it/s, loss=4.77]epoch 1/100: 86it [02:06,  2.00it/s, loss=4.86]epoch 1/100: 86it [02:06,  2.00it/s, loss=4.97]epoch 1/100: 87it [02:06,  2.12it/s, loss=4.86]epoch 1/100: 87it [02:06,  2.12it/s, loss=4.97]epoch 1/100: 87it [02:07,  2.12it/s, loss=4.85]epoch 1/100: 87it [02:07,  2.12it/s, loss=4.98]epoch 1/100: 88it [02:07,  2.24it/s, loss=4.85]epoch 1/100: 88it [02:07,  2.24it/s, loss=4.98]epoch 1/100: 88it [02:07,  2.24it/s, loss=4.85]epoch 1/100: 88it [02:07,  2.24it/s, loss=4.9] epoch 1/100: 89it [02:07,  2.34it/s, loss=4.85]epoch 1/100: 89it [02:07,  2.33it/s, loss=4.9]epoch 1/100: 89it [02:07,  2.34it/s, loss=4.84]epoch 1/100: 89it [02:07,  2.33it/s, loss=4.95]epoch 1/100: 90it [02:07,  2.41it/s, loss=4.95]epoch 1/100: 90it [02:07,  2.41it/s, loss=4.84]epoch 1/100: 90it [02:08,  2.41it/s, loss=5.04]epoch 1/100: 90it [02:08,  2.41it/s, loss=5.04]epoch 1/100: 91it [02:08,  2.46it/s, loss=5.04]epoch 1/100: 91it [02:08,  2.46it/s, loss=5.04]epoch 1/100: 91it [02:08,  2.46it/s, loss=4.81]epoch 1/100: 91it [02:08,  2.46it/s, loss=4.95]epoch 1/100: 92it [02:08,  2.50it/s, loss=4.81]epoch 1/100: 92it [02:08,  2.50it/s, loss=4.95]epoch 1/100: 92it [02:09,  2.50it/s, loss=4.89]epoch 1/100: 92it [02:09,  2.50it/s, loss=4.97]epoch 1/100: 93it [02:09,  2.53it/s, loss=4.89]epoch 1/100: 93it [02:09,  2.53it/s, loss=4.97]epoch 1/100: 93it [02:09,  2.53it/s, loss=4.86]epoch 1/100: 93it [02:09,  2.53it/s, loss=4.65]epoch 1/100: 94it [02:09,  2.55it/s, loss=4.65]epoch 1/100: 94it [02:09,  2.55it/s, loss=4.86]epoch 1/100: 94it [02:09,  2.55it/s, loss=4.78]epoch 1/100: 94it [02:09,  2.55it/s, loss=4.83]epoch 1/100: 95it [02:09,  2.56it/s, loss=4.78]epoch 1/100: 95it [02:09,  2.56it/s, loss=4.83]epoch 1/100: 95it [02:10,  2.56it/s, loss=4.84]epoch 1/100: 95it [02:10,  2.56it/s, loss=4.79]epoch 1/100: 96it [02:10,  2.57it/s, loss=4.84]epoch 1/100: 96it [02:10,  2.57it/s, loss=4.79]epoch 1/100: 96it [02:10,  2.57it/s, loss=4.87]epoch 1/100: 96it [02:10,  2.57it/s, loss=4.89]epoch 1/100: 97it [02:10,  2.58it/s, loss=4.89]epoch 1/100: 97it [02:10,  2.58it/s, loss=4.87]epoch 1/100: 97it [02:10,  2.58it/s, loss=4.82]epoch 1/100: 97it [02:10,  2.58it/s, loss=5.05]epoch 1/100: 98it [02:10,  2.59it/s, loss=4.82]epoch 1/100: 98it [02:10,  2.59it/s, loss=5.05]epoch 1/100: 98it [02:11,  2.59it/s, loss=5.09]epoch 1/100: 98it [02:11,  2.59it/s, loss=4.87]epoch 1/100: 99it [02:11,  2.59it/s, loss=5.09]epoch 1/100: 99it [02:11,  2.59it/s, loss=4.87]epoch 1/100: 99it [02:11,  2.59it/s, loss=4.96]epoch 1/100: 99it [02:11,  2.59it/s, loss=4.89]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.06s/it][A
val: 1it [00:03,  3.14s/it][A
val: 2it [00:03,  1.41s/it][A
val: 2it [00:03,  1.44s/it][A
val: 3it [00:03,  1.14it/s][A
val: 3it [00:03,  1.11it/s][A
val: 4it [00:03,  1.58it/s][A
val: 4it [00:03,  1.56it/s][A
val: 5it [00:04,  2.02it/s][A
val: 5it [00:04,  2.00it/s][A
val: 6it [00:04,  2.43it/s][A
val: 6it [00:04,  2.40it/s][A
val: 7it [00:04,  2.79it/s][A
val: 7it [00:04,  2.76it/s][A
val: 8it [00:04,  3.09it/s][A
val: 8it [00:04,  3.06it/s][A
val: 9it [00:05,  3.32it/s][A
val: 9it [00:05,  3.30it/s][A
val: 10it [00:05,  3.50it/s][A
val: 10it [00:05,  3.48it/s][A
val: 11it [00:05,  3.64it/s][A
val: 11it [00:05,  3.62it/s][A
val: 12it [00:05,  3.74it/s][A
val: 12it [00:05,  3.73it/s][A
val: 13it [00:06,  3.82it/s][A
val: 13it [00:06,  3.80it/s][A
val: 14it [00:06,  3.87it/s][A
val: 14it [00:06,  3.86it/s][A
val: 15it [00:06,  3.90it/s][A
val: 15it [00:06,  3.89it/s][A
val: 16it [00:06,  3.93it/s][A
val: 16it [00:06,  3.92it/s][A
val: 17it [00:07,  3.95it/s][A
val: 17it [00:07,  3.94it/s][A
val: 18it [00:07,  3.96it/s][A
val: 18it [00:07,  3.95it/s][A
val: 19it [00:07,  3.97it/s][A
val: 19it [00:07,  3.96it/s][A
val: 20it [00:07,  3.98it/s][A
val: 20it [00:07,  3.97it/s][A
val: 21it [00:08,  3.99it/s][A
val: 21it [00:08,  3.97it/s][A
val: 22it [00:08,  3.99it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.99it/s][A
val: 23it [00:08,  3.98it/s][A
val: 24it [00:08,  3.99it/s][A
val: 24it [00:08,  3.98it/s][A
                            [A
                            [Aepoch 1/100: 99it [02:20,  2.59it/s, loss=4.89]epoch 1/100: 99it [02:29,  2.59it/s, best_val=4.86, loss=4.96, val=4.86]epoch 1/100: 100it [02:29,  5.71s/it, loss=4.89]epoch 1/100: 100it [02:29,  5.71s/it, best_val=4.86, loss=4.96, val=4.86]epoch 1/100: 100it [02:29,  5.71s/it, loss=5.13]epoch 1/100: 100it [02:29,  5.71s/it, loss=4.87]                         epoch 1/100: 101it [02:29,  4.11s/it, loss=5.13]epoch 1/100: 101it [02:29,  4.11s/it, loss=4.87]epoch 1/100: 101it [02:30,  4.11s/it, loss=5.11]epoch 1/100: 101it [02:30,  4.11s/it, loss=4.83]epoch 1/100: 102it [02:30,  3.00s/it, loss=5.11]epoch 1/100: 102it [02:30,  3.00s/it, loss=4.83]epoch 1/100: 102it [02:30,  3.00s/it, loss=4.77]epoch 1/100: 102it [02:30,  3.00s/it, loss=4.94]epoch 1/100: 103it [02:30,  2.21s/it, loss=4.77]epoch 1/100: 103it [02:30,  2.21s/it, loss=4.94]epoch 1/100: 103it [02:30,  2.21s/it, loss=4.69]epoch 1/100: 103it [02:30,  2.21s/it, loss=4.87]epoch 1/100: 104it [02:30,  1.66s/it, loss=4.69]epoch 1/100: 104it [02:30,  1.66s/it, loss=4.87]epoch 1/100: 104it [02:31,  1.66s/it, loss=4.95]epoch 1/100: 104it [02:31,  1.66s/it, loss=4.85]epoch 1/100: 105it [02:31,  1.28s/it, loss=4.95]epoch 1/100: 105it [02:31,  1.28s/it, loss=4.85]epoch 1/100: 105it [02:31,  1.28s/it, loss=4.91]epoch 1/100: 106it [02:31,  1.01s/it, loss=4.91]epoch 1/100: 105it [02:31,  1.28s/it, loss=4.99]epoch 1/100: 106it [02:31,  1.01s/it, loss=4.99]epoch 1/100: 106it [02:32,  1.01s/it, loss=4.99]epoch 1/100: 106it [02:32,  1.01s/it, loss=4.78]epoch 1/100: 107it [02:32,  1.21it/s, loss=4.99]epoch 1/100: 107it [02:32,  1.21it/s, loss=4.78]epoch 1/100: 107it [02:32,  1.21it/s, loss=4.75]epoch 1/100: 107it [02:32,  1.21it/s, loss=4.7] epoch 1/100: 108it [02:32,  1.44it/s, loss=4.7]epoch 1/100: 108it [02:32,  1.44it/s, loss=4.75]epoch 1/100: 108it [02:32,  1.44it/s, loss=4.9] epoch 1/100: 108it [02:32,  1.44it/s, loss=4.86]epoch 1/100: 109it [02:32,  1.67it/s, loss=4.86]epoch 1/100: 109it [02:32,  1.67it/s, loss=4.9]epoch 1/100: 109it [02:33,  1.67it/s, loss=4.89]epoch 1/100: 109it [02:33,  1.67it/s, loss=4.89]epoch 1/100: 110it [02:33,  1.87it/s, loss=4.89]epoch 1/100: 110it [02:33,  1.87it/s, loss=4.89]epoch 1/100: 110it [02:33,  1.87it/s, loss=4.92]epoch 1/100: 110it [02:33,  1.87it/s, loss=4.8] epoch 1/100: 111it [02:33,  2.04it/s, loss=4.92]epoch 1/100: 111it [02:33,  2.04it/s, loss=4.8]epoch 1/100: 111it [02:34,  2.04it/s, loss=4.9] epoch 1/100: 111it [02:34,  2.04it/s, loss=4.79]epoch 1/100: 112it [02:34,  2.18it/s, loss=4.9]epoch 1/100: 112it [02:34,  2.18it/s, loss=4.79]epoch 1/100: 112it [02:34,  2.18it/s, loss=4.62]epoch 1/100: 112it [02:34,  2.18it/s, loss=4.66]epoch 1/100: 113it [02:34,  2.29it/s, loss=4.62]epoch 1/100: 113it [02:34,  2.29it/s, loss=4.66]epoch 1/100: 113it [02:34,  2.29it/s, loss=5.02]epoch 1/100: 113it [02:34,  2.29it/s, loss=4.91]epoch 1/100: 114it [02:34,  2.38it/s, loss=5.02]epoch 1/100: 114it [02:34,  2.38it/s, loss=4.91]epoch 1/100: 114it [02:35,  2.38it/s, loss=4.91]epoch 1/100: 114it [02:35,  2.38it/s, loss=4.74]epoch 1/100: 115it [02:35,  2.44it/s, loss=4.91]epoch 1/100: 115it [02:35,  2.44it/s, loss=4.74]epoch 1/100: 115it [02:35,  2.44it/s, loss=4.67]epoch 1/100: 115it [02:35,  2.44it/s, loss=4.94]epoch 1/100: 116it [02:35,  2.48it/s, loss=4.67]epoch 1/100: 116it [02:35,  2.48it/s, loss=4.94]epoch 1/100: 116it [02:36,  2.48it/s, loss=4.75]epoch 1/100: 116it [02:35,  2.48it/s, loss=5.08]epoch 1/100: 117it [02:36,  2.52it/s, loss=4.75]epoch 1/100: 117it [02:35,  2.52it/s, loss=5.08]epoch 1/100: 117it [02:36,  2.52it/s, loss=4.7] epoch 1/100: 117it [02:36,  2.52it/s, loss=4.81]epoch 1/100: 118it [02:36,  2.54it/s, loss=4.7]epoch 1/100: 118it [02:36,  2.54it/s, loss=4.81]epoch 1/100: 118it [02:36,  2.54it/s, loss=4.84]epoch 1/100: 118it [02:36,  2.54it/s, loss=4.75]epoch 1/100: 119it [02:36,  2.56it/s, loss=4.84]epoch 1/100: 119it [02:36,  2.56it/s, loss=4.75]epoch 1/100: 119it [02:37,  2.56it/s, loss=4.88]epoch 1/100: 119it [02:37,  2.56it/s, loss=4.98]epoch 1/100: 120it [02:37,  2.57it/s, loss=4.88]epoch 1/100: 120it [02:37,  2.57it/s, loss=4.98]epoch 1/100: 120it [02:37,  2.57it/s, loss=4.83]epoch 1/100: 120it [02:37,  2.57it/s, loss=4.84]epoch 1/100: 121it [02:37,  2.58it/s, loss=4.84]epoch 1/100: 121it [02:37,  2.58it/s, loss=4.83]epoch 1/100: 121it [02:37,  2.58it/s, loss=4.65]epoch 1/100: 121it [02:37,  2.58it/s, loss=5]   epoch 1/100: 122it [02:37,  2.59it/s, loss=5]epoch 1/100: 122it [02:37,  2.59it/s, loss=4.65]epoch 1/100: 122it [02:38,  2.59it/s, loss=4.88]epoch 1/100: 122it [02:38,  2.59it/s, loss=4.87]epoch 1/100: 123it [02:38,  2.59it/s, loss=4.87]epoch 1/100: 123it [02:38,  2.59it/s, loss=4.88]epoch 1/100: 123it [02:38,  2.59it/s, loss=4.81]epoch 1/100: 123it [02:38,  2.59it/s, loss=4.96]epoch 1/100: 124it [02:38,  2.59it/s, loss=4.81]epoch 1/100: 124it [02:38,  2.59it/s, loss=4.96]epoch 1/100: 124it [02:39,  2.59it/s, loss=4.71]epoch 1/100: 124it [02:39,  2.59it/s, loss=4.85]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.29s/it][A
val: 2it [00:03,  1.50s/it][A
val: 3it [00:03,  1.07it/s][A
val: 4it [00:04,  1.51it/s][A
val: 5it [00:04,  1.94it/s][A
val: 6it [00:04,  2.35it/s][A
val: 7it [00:04,  2.71it/s][A
val: 8it [00:05,  3.01it/s][A
val: 9it [00:05,  3.26it/s][A
val: 10it [00:05,  3.45it/s][A
val: 11it [00:05,  3.60it/s][A
val: 12it [00:06,  3.71it/s][A
val: 13it [00:06,  3.78it/s][A
val: 14it [00:06,  3.83it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:07,  3.90it/s][A
val: 17it [00:07,  3.92it/s][A
val: 18it [00:07,  3.94it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:08,  3.95it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.97it/s][A
val: 24it [00:09,  3.98it/s][A
                            [A
val: 1it [00:11, 11.99s/it][A
val: 2it [00:12,  5.08s/it][A
val: 3it [00:12,  2.88s/it][A
val: 4it [00:12,  1.84s/it][A
val: 5it [00:12,  1.27s/it][A
val: 6it [00:13,  1.08it/s][A
val: 7it [00:13,  1.42it/s][A
val: 8it [00:13,  1.79it/s][A
val: 9it [00:14,  2.16it/s][A
val: 10it [00:14,  2.51it/s][A
val: 11it [00:14,  2.83it/s][A
val: 12it [00:14,  3.09it/s][A
val: 13it [00:15,  3.32it/s][A
val: 14it [00:15,  3.48it/s][A
val: 15it [00:15,  3.61it/s][A
val: 16it [00:15,  3.71it/s][A
val: 17it [00:16,  3.79it/s][A
val: 18it [00:16,  3.84it/s][A
val: 19it [00:16,  3.87it/s][A
val: 20it [00:16,  3.90it/s][A
val: 21it [00:17,  3.92it/s][A
val: 22it [00:17,  3.93it/s][A
val: 23it [00:17,  3.94it/s][A
val: 24it [00:17,  3.95it/s][A
                            [Aepoch 1/100: 124it [02:57,  2.59it/s, loss=4.85]epoch 1/100: 124it [03:06,  2.59it/s, best_val=4.8, loss=4.71, val=4.8]epoch 1/100: 125it [03:06,  8.60s/it, best_val=4.8, loss=4.71, val=4.8]epoch 1/100: 125it [03:06,  8.60s/it, loss=4.85]epoch 1/100: 125it [03:06,  8.60s/it, loss=4.95]                       epoch 1/100: 125it [03:06,  8.60s/it, loss=4.79]epoch 1/100: 126it [03:06,  6.13s/it, loss=4.95]epoch 1/100: 126it [03:06,  6.13s/it, loss=4.79]epoch 1/100: 126it [03:07,  6.13s/it, loss=4.88]epoch 1/100: 126it [03:07,  6.13s/it, loss=4.64]epoch 1/100: 127it [03:07,  4.41s/it, loss=4.88]epoch 1/100: 127it [03:07,  4.41s/it, loss=4.64]epoch 1/100: 127it [03:07,  4.41s/it, loss=4.52]epoch 1/100: 127it [03:07,  4.41s/it, loss=4.79]epoch 1/100: 128it [03:07,  3.20s/it, loss=4.52]epoch 1/100: 128it [03:07,  3.20s/it, loss=4.79]epoch 1/100: 128it [03:07,  3.20s/it, loss=4.96]epoch 1/100: 128it [03:07,  3.20s/it, loss=4.48]epoch 1/100: 129it [03:07,  2.36s/it, loss=4.96]epoch 1/100: 129it [03:07,  2.36s/it, loss=4.48]epoch 1/100: 129it [03:08,  2.36s/it, loss=4.85]epoch 1/100: 129it [03:08,  2.36s/it, loss=4.78]epoch 1/100: 130it [03:08,  1.77s/it, loss=4.85]epoch 1/100: 130it [03:08,  1.77s/it, loss=4.78]epoch 1/100: 130it [03:08,  1.77s/it, loss=4.79]epoch 1/100: 130it [03:08,  1.77s/it, loss=4.6] epoch 1/100: 131it [03:08,  1.35s/it, loss=4.79]epoch 1/100: 131it [03:08,  1.35s/it, loss=4.6]epoch 1/100: 131it [03:09,  1.35s/it, loss=4.86]epoch 1/100: 131it [03:09,  1.35s/it, loss=4.82]epoch 1/100: 132it [03:09,  1.06s/it, loss=4.86]epoch 1/100: 132it [03:09,  1.06s/it, loss=4.82]epoch 1/100: 132it [03:09,  1.06s/it, loss=4.87]epoch 1/100: 132it [03:09,  1.06s/it, loss=4.89]epoch 1/100: 133it [03:09,  1.16it/s, loss=4.87]epoch 1/100: 133it [03:09,  1.16it/s, loss=4.89]epoch 1/100: 133it [03:09,  1.16it/s, loss=4.82]epoch 1/100: 133it [03:09,  1.16it/s, loss=4.7] epoch 1/100: 134it [03:09,  1.40it/s, loss=4.82]epoch 1/100: 134it [03:09,  1.40it/s, loss=4.7]epoch 1/100: 134it [03:10,  1.40it/s, loss=4.85]epoch 1/100: 134it [03:10,  1.40it/s, loss=4.97]epoch 1/100: 135it [03:10,  1.62it/s, loss=4.97]epoch 1/100: 135it [03:10,  1.62it/s, loss=4.85]epoch 1/100: 135it [03:10,  1.62it/s, loss=4.74]epoch 1/100: 135it [03:10,  1.62it/s, loss=5]   epoch 1/100: 136it [03:10,  1.82it/s, loss=4.74]epoch 1/100: 136it [03:10,  1.82it/s, loss=5]epoch 1/100: 136it [03:11,  1.82it/s, loss=4.69]epoch 1/100: 136it [03:11,  1.82it/s, loss=4.87]epoch 1/100: 137it [03:11,  2.00it/s, loss=4.69]epoch 1/100: 137it [03:11,  2.00it/s, loss=4.87]epoch 1/100: 137it [03:11,  2.00it/s, loss=4.66]epoch 1/100: 137it [03:11,  2.00it/s, loss=4.67]epoch 1/100: 138it [03:11,  2.15it/s, loss=4.66]epoch 1/100: 138it [03:11,  2.15it/s, loss=4.67]epoch 1/100: 138it [03:11,  2.15it/s, loss=4.77]epoch 1/100: 138it [03:11,  2.15it/s, loss=4.81]epoch 1/100: 139it [03:11,  2.27it/s, loss=4.77]epoch 1/100: 139it [03:11,  2.27it/s, loss=4.81]epoch 1/100: 139it [03:12,  2.27it/s, loss=4.87]epoch 1/100: 139it [03:12,  2.27it/s, loss=4.81]epoch 1/100: 140it [03:12,  2.36it/s, loss=4.87]epoch 1/100: 140it [03:12,  2.36it/s, loss=4.81]epoch 1/100: 140it [03:12,  2.36it/s, loss=4.74]epoch 1/100: 140it [03:12,  2.36it/s, loss=4.82]epoch 1/100: 141it [03:12,  2.42it/s, loss=4.82]epoch 1/100: 141it [03:12,  2.42it/s, loss=4.74]epoch 1/100: 141it [03:12,  2.42it/s, loss=4.81]epoch 1/100: 141it [03:12,  2.42it/s, loss=4.92]epoch 1/100: 142it [03:12,  2.47it/s, loss=4.92]epoch 1/100: 142it [03:12,  2.47it/s, loss=4.81]epoch 1/100: 142it [03:13,  2.47it/s, loss=4.74]epoch 1/100: 142it [03:13,  2.47it/s, loss=4.93]epoch 1/100: 143it [03:13,  2.51it/s, loss=4.74]epoch 1/100: 143it [03:13,  2.51it/s, loss=4.93]epoch 1/100: 143it [03:13,  2.51it/s, loss=4.85]epoch 1/100: 143it [03:13,  2.51it/s, loss=4.62]epoch 1/100: 144it [03:13,  2.54it/s, loss=4.85]epoch 1/100: 144it [03:13,  2.54it/s, loss=4.62]epoch 1/100: 144it [03:14,  2.54it/s, loss=4.93]epoch 1/100: 144it [03:14,  2.54it/s, loss=4.74]epoch 1/100: 145it [03:14,  2.56it/s, loss=4.74]epoch 1/100: 145it [03:14,  2.56it/s, loss=4.93]epoch 1/100: 145it [03:14,  2.56it/s, loss=4.73]epoch 1/100: 145it [03:14,  2.56it/s, loss=4.95]epoch 1/100: 146it [03:14,  2.57it/s, loss=4.73]epoch 1/100: 146it [03:14,  2.57it/s, loss=4.95]epoch 1/100: 146it [03:14,  2.57it/s, loss=4.71]epoch 1/100: 146it [03:14,  2.57it/s, loss=4.88]epoch 1/100: 147it [03:14,  2.58it/s, loss=4.71]epoch 1/100: 147it [03:14,  2.58it/s, loss=4.88]epoch 1/100: 147it [03:15,  2.58it/s, loss=4.71]epoch 1/100: 147it [03:15,  2.58it/s, loss=4.84]epoch 1/100: 148it [03:15,  2.59it/s, loss=4.84]epoch 1/100: 148it [03:15,  2.59it/s, loss=4.71]epoch 1/100: 148it [03:15,  2.59it/s, loss=4.58]epoch 1/100: 148it [03:15,  2.59it/s, loss=4.97]epoch 1/100: 149it [03:15,  2.59it/s, loss=4.58]epoch 1/100: 149it [03:15,  2.59it/s, loss=4.97]epoch 1/100: 149it [03:16,  2.59it/s, loss=4.58]epoch 1/100: 149it [03:16,  2.59it/s, loss=4.72]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.17s/it][A
val: 1it [00:03,  3.47s/it][A
val: 2it [00:03,  1.58s/it][A
val: 3it [00:03,  1.03it/s][A
val: 4it [00:04,  1.45it/s][A
val: 5it [00:04,  1.88it/s][A
val: 2it [00:04,  2.12s/it][A
val: 6it [00:04,  2.30it/s][A
val: 3it [00:04,  1.27s/it][A
val: 7it [00:04,  2.66it/s][A
val: 4it [00:05,  1.16it/s][A
val: 8it [00:05,  2.98it/s][A
val: 5it [00:05,  1.55it/s][A
val: 9it [00:05,  3.23it/s][A
val: 6it [00:05,  1.96it/s][A
val: 10it [00:05,  3.43it/s][A
val: 7it [00:05,  2.35it/s][A
val: 11it [00:05,  3.58it/s][A
val: 8it [00:06,  2.70it/s][A
val: 12it [00:06,  3.69it/s][A
val: 9it [00:06,  3.01it/s][A
val: 13it [00:06,  3.77it/s][A
val: 10it [00:06,  3.25it/s][A
val: 14it [00:06,  3.83it/s][A
val: 11it [00:06,  3.45it/s][A
val: 15it [00:06,  3.87it/s][A
val: 12it [00:07,  3.59it/s][A
val: 16it [00:07,  3.91it/s][A
val: 13it [00:07,  3.70it/s][A
val: 17it [00:07,  3.93it/s][A
val: 14it [00:07,  3.78it/s][A
val: 18it [00:07,  3.94it/s][A
val: 15it [00:07,  3.84it/s][A
val: 19it [00:07,  3.95it/s][A
val: 16it [00:08,  3.88it/s][A
val: 20it [00:08,  3.96it/s][A
val: 17it [00:08,  3.91it/s][A
val: 21it [00:08,  3.97it/s][A
val: 18it [00:08,  3.93it/s][A
val: 22it [00:08,  3.97it/s][A
val: 19it [00:08,  3.95it/s][A
val: 23it [00:09,  3.97it/s][A
val: 20it [00:09,  3.97it/s][A
val: 24it [00:09,  3.97it/s][A
val: 21it [00:09,  3.97it/s][A
                            [A
val: 22it [00:09,  3.98it/s][A
val: 23it [00:09,  3.98it/s][A
val: 24it [00:10,  3.99it/s][A
                            [Aepoch 1/100: 149it [03:26,  2.59it/s, loss=4.72]epoch 1/100: 149it [03:35,  2.59it/s, best_val=4.75, loss=4.58, val=4.75]epoch 1/100: 150it [03:35,  6.20s/it, loss=4.72]epoch 1/100: 150it [03:35,  6.20s/it, best_val=4.75, loss=4.58, val=4.75]epoch 1/100: 150it [03:35,  6.20s/it, loss=4.55]epoch 1/100: 150it [03:35,  6.20s/it, loss=4.92]                         epoch 1/100: 151it [03:35,  4.46s/it, loss=4.55]epoch 1/100: 151it [03:35,  4.46s/it, loss=4.92]epoch 1/100: 151it [03:36,  4.46s/it, loss=4.84]epoch 1/100: 151it [03:36,  4.46s/it, loss=4.79]epoch 1/100: 152it [03:36,  3.24s/it, loss=4.84]epoch 1/100: 152it [03:36,  3.24s/it, loss=4.79]epoch 1/100: 152it [03:36,  3.24s/it, loss=4.59]epoch 1/100: 152it [03:36,  3.24s/it, loss=4.78]epoch 1/100: 153it [03:36,  2.38s/it, loss=4.78]epoch 1/100: 153it [03:36,  2.38s/it, loss=4.59]epoch 1/100: 153it [03:36,  2.38s/it, loss=4.81]epoch 1/100: 153it [03:36,  2.38s/it, loss=4.66]epoch 1/100: 154it [03:36,  1.78s/it, loss=4.66]epoch 1/100: 154it [03:36,  1.78s/it, loss=4.81]epoch 1/100: 154it [03:37,  1.78s/it, loss=4.73]epoch 1/100: 154it [03:37,  1.78s/it, loss=4.64]epoch 1/100: 155it [03:37,  1.36s/it, loss=4.73]epoch 1/100: 155it [03:37,  1.36s/it, loss=4.64]epoch 1/100: 155it [03:37,  1.36s/it, loss=4.66]epoch 1/100: 155it [03:37,  1.36s/it, loss=4.96]epoch 1/100: 156it [03:37,  1.07s/it, loss=4.66]epoch 1/100: 156it [03:37,  1.07s/it, loss=4.96]epoch 1/100: 156it [03:38,  1.07s/it, loss=4.84]epoch 1/100: 156it [03:38,  1.07s/it, loss=4.76]epoch 1/100: 157it [03:38,  1.16it/s, loss=4.76]epoch 1/100: 157it [03:38,  1.16it/s, loss=4.84]epoch 1/100: 157it [03:38,  1.16it/s, loss=4.58]epoch 1/100: 157it [03:38,  1.16it/s, loss=4.7] epoch 1/100: 158it [03:38,  1.39it/s, loss=4.7]epoch 1/100: 158it [03:38,  1.39it/s, loss=4.58]epoch 1/100: 158it [03:38,  1.39it/s, loss=4.73]epoch 1/100: 158it [03:38,  1.39it/s, loss=4.65]epoch 1/100: 159it [03:38,  1.61it/s, loss=4.73]epoch 1/100: 159it [03:38,  1.61it/s, loss=4.65]epoch 1/100: 159it [03:39,  1.61it/s, loss=4.66]epoch 1/100: 159it [03:39,  1.61it/s, loss=4.76]epoch 1/100: 160it [03:39,  1.82it/s, loss=4.66]epoch 1/100: 160it [03:39,  1.82it/s, loss=4.76]epoch 1/100: 160it [03:39,  1.82it/s, loss=4.58]epoch 1/100: 160it [03:39,  1.82it/s, loss=4.65]epoch 1/100: 161it [03:39,  2.00it/s, loss=4.58]epoch 1/100: 161it [03:39,  2.00it/s, loss=4.65]epoch 1/100: 161it [03:40,  2.00it/s, loss=4.64]epoch 1/100: 161it [03:40,  2.00it/s, loss=4.77]epoch 1/100: 162it [03:40,  2.15it/s, loss=4.77]epoch 1/100: 162it [03:40,  2.15it/s, loss=4.64]epoch 1/100: 162it [03:40,  2.15it/s, loss=4.97]epoch 1/100: 162it [03:40,  2.15it/s, loss=4.87]epoch 1/100: 163it [03:40,  2.27it/s, loss=4.97]epoch 1/100: 163it [03:40,  2.27it/s, loss=4.87]epoch 1/100: 163it [03:40,  2.27it/s, loss=4.66]epoch 1/100: 163it [03:40,  2.27it/s, loss=4.7] epoch 1/100: 164it [03:40,  2.36it/s, loss=4.66]epoch 1/100: 164it [03:40,  2.36it/s, loss=4.7]epoch 1/100: 164it [03:41,  2.36it/s, loss=4.81]epoch 1/100: 164it [03:41,  2.36it/s, loss=4.89]epoch 1/100: 165it [03:41,  2.43it/s, loss=4.81]epoch 1/100: 165it [03:41,  2.43it/s, loss=4.89]epoch 1/100: 165it [03:41,  2.43it/s, loss=4.82]epoch 1/100: 165it [03:41,  2.43it/s, loss=4.82]epoch 1/100: 166it [03:41,  2.48it/s, loss=4.82]epoch 1/100: 166it [03:41,  2.48it/s, loss=4.82]epoch 1/100: 166it [03:41,  2.48it/s, loss=4.59]epoch 1/100: 166it [03:41,  2.48it/s, loss=4.61]epoch 1/100: 167it [03:41,  2.51it/s, loss=4.61]epoch 1/100: 167it [03:41,  2.51it/s, loss=4.59]epoch 1/100: 167it [03:42,  2.51it/s, loss=4.91]epoch 1/100: 167it [03:42,  2.51it/s, loss=4.68]epoch 1/100: 168it [03:42,  2.54it/s, loss=4.91]epoch 1/100: 168it [03:42,  2.54it/s, loss=4.68]epoch 1/100: 168it [03:42,  2.54it/s, loss=4.66]epoch 1/100: 168it [03:42,  2.54it/s, loss=4.73]epoch 1/100: 169it [03:42,  2.56it/s, loss=4.66]epoch 1/100: 169it [03:42,  2.56it/s, loss=4.73]epoch 1/100: 169it [03:43,  2.56it/s, loss=5.1] epoch 1/100: 169it [03:43,  2.56it/s, loss=4.74]epoch 1/100: 170it [03:43,  2.57it/s, loss=4.74]epoch 1/100: 170it [03:43,  2.57it/s, loss=5.1]epoch 1/100: 170it [03:43,  2.57it/s, loss=4.83]epoch 1/100: 170it [03:43,  2.57it/s, loss=4.73]epoch 1/100: 171it [03:43,  2.58it/s, loss=4.83]epoch 1/100: 171it [03:43,  2.58it/s, loss=4.73]epoch 1/100: 171it [03:43,  2.58it/s, loss=4.75]epoch 1/100: 171it [03:43,  2.58it/s, loss=4.57]epoch 1/100: 172it [03:43,  2.59it/s, loss=4.75]epoch 1/100: 172it [03:43,  2.59it/s, loss=4.57]epoch 1/100: 172it [03:44,  2.59it/s, loss=4.98]epoch 1/100: 172it [03:44,  2.59it/s, loss=4.7] epoch 1/100: 173it [03:44,  2.59it/s, loss=4.98]epoch 1/100: 173it [03:44,  2.59it/s, loss=4.7]epoch 1/100: 173it [03:44,  2.59it/s, loss=4.83]epoch 1/100: 173it [03:44,  2.59it/s, loss=4.82]epoch 1/100: 174it [03:44,  2.59it/s, loss=4.82]epoch 1/100: 174it [03:44,  2.59it/s, loss=4.83]epoch 1/100: 174it [03:45,  2.59it/s, loss=4.89]epoch 1/100: 174it [03:45,  2.59it/s, loss=4.86]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.39s/it][A
val: 1it [00:04,  4.45s/it][A
val: 2it [00:04,  1.98s/it][A
val: 3it [00:04,  1.19s/it][A
val: 4it [00:05,  1.22it/s][A
val: 2it [00:05,  2.53s/it][A
val: 5it [00:05,  1.62it/s][A
val: 3it [00:05,  1.49s/it][A
val: 6it [00:05,  2.03it/s][A
val: 4it [00:05,  1.00it/s][A
val: 7it [00:05,  2.42it/s][A
val: 5it [00:06,  1.37it/s][A
val: 8it [00:06,  2.76it/s][A
val: 6it [00:06,  1.77it/s][A
val: 9it [00:06,  3.05it/s][A
val: 7it [00:06,  2.16it/s][A
val: 10it [00:06,  3.29it/s][A
val: 8it [00:06,  2.53it/s][A
val: 11it [00:06,  3.47it/s][A
val: 9it [00:07,  2.85it/s][A
val: 12it [00:07,  3.60it/s][A
val: 10it [00:07,  3.13it/s][A
val: 13it [00:07,  3.71it/s][A
val: 11it [00:07,  3.35it/s][A
val: 14it [00:07,  3.78it/s][A
val: 12it [00:07,  3.52it/s][A
val: 15it [00:07,  3.83it/s][A
val: 13it [00:08,  3.65it/s][A
val: 16it [00:08,  3.87it/s][A
val: 14it [00:08,  3.75it/s][A
val: 17it [00:08,  3.90it/s][A
val: 15it [00:08,  3.82it/s][A
val: 18it [00:08,  3.92it/s][A
val: 16it [00:08,  3.87it/s][A
val: 19it [00:08,  3.93it/s][A
val: 17it [00:09,  3.90it/s][A
val: 20it [00:09,  3.95it/s][A
val: 18it [00:09,  3.93it/s][A
val: 21it [00:09,  3.95it/s][A
val: 19it [00:09,  3.95it/s][A
val: 22it [00:09,  3.96it/s][A
val: 20it [00:09,  3.96it/s][A
val: 23it [00:09,  3.96it/s][A
val: 21it [00:10,  3.97it/s][A
val: 24it [00:10,  3.96it/s][A
val: 22it [00:10,  3.97it/s][A
                            [A
val: 23it [00:10,  3.98it/s][A
val: 24it [00:10,  3.98it/s][A
                            [Aepoch 1/100: 174it [03:56,  2.59it/s, loss=4.86]epoch 1/100: 174it [04:04,  2.59it/s, best_val=4.69, loss=4.89, val=4.69]epoch 1/100: 175it [04:04,  6.24s/it, loss=4.86]epoch 1/100: 175it [04:04,  6.24s/it, best_val=4.69, loss=4.89, val=4.69]epoch 1/100: 175it [04:04,  6.24s/it, loss=4.85]                         epoch 1/100: 175it [04:04,  6.24s/it, loss=4.81]epoch 1/100: 176it [04:04,  4.49s/it, loss=4.81]epoch 1/100: 176it [04:04,  4.49s/it, loss=4.85]epoch 1/100: 176it [04:05,  4.49s/it, loss=4.71]epoch 1/100: 176it [04:05,  4.49s/it, loss=4.9] epoch 1/100: 177it [04:05,  3.26s/it, loss=4.71]epoch 1/100: 177it [04:05,  3.26s/it, loss=4.9]epoch 1/100: 177it [04:05,  3.26s/it, loss=4.7] epoch 1/100: 177it [04:05,  3.26s/it, loss=4.78]epoch 1/100: 178it [04:05,  2.40s/it, loss=4.7]epoch 1/100: 178it [04:05,  2.40s/it, loss=4.78]epoch 1/100: 178it [04:06,  2.40s/it, loss=4.9]epoch 1/100: 178it [04:06,  2.40s/it, loss=4.61]epoch 1/100: 179it [04:06,  1.79s/it, loss=4.9]epoch 1/100: 179it [04:06,  1.79s/it, loss=4.61]epoch 1/100: 179it [04:06,  1.79s/it, loss=4.71]epoch 1/100: 179it [04:06,  1.79s/it, loss=4.83]epoch 1/100: 180it [04:06,  1.37s/it, loss=4.71]epoch 1/100: 180it [04:06,  1.37s/it, loss=4.83]epoch 1/100: 180it [04:06,  1.37s/it, loss=4.75]epoch 1/100: 180it [04:06,  1.37s/it, loss=4.63]epoch 1/100: 181it [04:06,  1.07s/it, loss=4.75]epoch 1/100: 181it [04:06,  1.07s/it, loss=4.63]epoch 1/100: 181it [04:07,  1.07s/it, loss=4.59]epoch 1/100: 181it [04:07,  1.07s/it, loss=4.55]epoch 1/100: 182it [04:07,  1.15it/s, loss=4.59]epoch 1/100: 182it [04:07,  1.15it/s, loss=4.55]epoch 1/100: 182it [04:07,  1.15it/s, loss=4.75]epoch 1/100: 182it [04:07,  1.15it/s, loss=4.86]epoch 1/100: 183it [04:07,  1.38it/s, loss=4.75]epoch 1/100: 183it [04:07,  1.38it/s, loss=4.86]epoch 1/100: 183it [04:08,  1.38it/s, loss=4.78]epoch 1/100: 183it [04:08,  1.38it/s, loss=4.86]epoch 1/100: 184it [04:08,  1.61it/s, loss=4.78]epoch 1/100: 184it [04:08,  1.61it/s, loss=4.86]epoch 1/100: 184it [04:08,  1.61it/s, loss=4.83]epoch 1/100: 184it [04:08,  1.61it/s, loss=4.78]epoch 1/100: 185it [04:08,  1.82it/s, loss=4.83]epoch 1/100: 185it [04:08,  1.82it/s, loss=4.78]epoch 1/100: 185it [04:08,  1.82it/s, loss=4.67]epoch 1/100: 185it [04:08,  1.82it/s, loss=4.64]epoch 1/100: 186it [04:08,  2.00it/s, loss=4.67]epoch 1/100: 186it [04:08,  2.00it/s, loss=4.64]epoch 1/100: 186it [04:09,  2.00it/s, loss=5.01]epoch 1/100: 186it [04:09,  2.00it/s, loss=4.79]epoch 1/100: 187it [04:09,  2.15it/s, loss=4.79]epoch 1/100: 187it [04:09,  2.15it/s, loss=5.01]epoch 1/100: 187it [04:09,  2.15it/s, loss=4.65]epoch 1/100: 187it [04:09,  2.15it/s, loss=4.61]epoch 1/100: 188it [04:09,  2.26it/s, loss=4.61]epoch 1/100: 188it [04:09,  2.26it/s, loss=4.65]epoch 1/100: 188it [04:09,  2.26it/s, loss=4.68]epoch 1/100: 188it [04:09,  2.26it/s, loss=4.77]epoch 1/100: 189it [04:09,  2.36it/s, loss=4.68]epoch 1/100: 189it [04:09,  2.36it/s, loss=4.77]epoch 1/100: 189it [04:10,  2.36it/s, loss=4.62]epoch 1/100: 189it [04:10,  2.36it/s, loss=4.65]epoch 1/100: 190it [04:10,  2.43it/s, loss=4.62]epoch 1/100: 190it [04:10,  2.43it/s, loss=4.65]epoch 1/100: 190it [04:10,  2.43it/s, loss=4.76]epoch 1/100: 190it [04:10,  2.43it/s, loss=4.54]epoch 1/100: 191it [04:10,  2.47it/s, loss=4.76]epoch 1/100: 191it [04:10,  2.47it/s, loss=4.54]epoch 1/100: 191it [04:11,  2.47it/s, loss=4.72]epoch 1/100: 191it [04:11,  2.47it/s, loss=4.64]epoch 1/100: 192it [04:11,  2.51it/s, loss=4.72]epoch 1/100: 192it [04:11,  2.51it/s, loss=4.64]epoch 1/100: 192it [04:11,  2.51it/s, loss=4.6] epoch 1/100: 192it [04:11,  2.51it/s, loss=4.88]epoch 1/100: 193it [04:11,  2.53it/s, loss=4.88]epoch 1/100: 193it [04:11,  2.53it/s, loss=4.6]epoch 1/100: 193it [04:11,  2.53it/s, loss=4.8]epoch 1/100: 193it [04:11,  2.53it/s, loss=4.73]epoch 1/100: 194it [04:11,  2.55it/s, loss=4.8]epoch 1/100: 194it [04:11,  2.55it/s, loss=4.73]epoch 1/100: 194it [04:12,  2.55it/s, loss=4.85]epoch 1/100: 194it [04:12,  2.55it/s, loss=4.7] epoch 1/100: 195it [04:12,  2.57it/s, loss=4.85]epoch 1/100: 195it [04:12,  2.57it/s, loss=4.7]epoch 1/100: 195it [04:12,  2.57it/s, loss=4.67]epoch 1/100: 195it [04:12,  2.57it/s, loss=4.6] epoch 1/100: 196it [04:12,  2.57it/s, loss=4.67]epoch 1/100: 196it [04:12,  2.57it/s, loss=4.6]epoch 1/100: 196it [04:13,  2.57it/s, loss=4.74]epoch 1/100: 196it [04:13,  2.57it/s, loss=4.81]epoch 1/100: 197it [04:13,  2.58it/s, loss=4.74]epoch 1/100: 197it [04:13,  2.58it/s, loss=4.81]epoch 1/100: 197it [04:13,  2.58it/s, loss=4.7] epoch 1/100: 197it [04:13,  2.58it/s, loss=4.84]epoch 1/100: 198it [04:13,  2.58it/s, loss=4.7]epoch 1/100: 198it [04:13,  2.58it/s, loss=4.84]epoch 1/100: 198it [04:13,  2.58it/s, loss=4.71]epoch 1/100: 198it [04:13,  2.58it/s, loss=4.65]epoch 1/100: 199it [04:13,  2.59it/s, loss=4.71]epoch 1/100: 199it [04:13,  2.59it/s, loss=4.65]epoch 1/100: 199it [04:14,  2.59it/s, loss=4.53]epoch 1/100: 199it [04:14,  2.59it/s, loss=4.79]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.73s/it][A
val: 1it [00:03,  3.74s/it][A
val: 2it [00:03,  1.69s/it][A
val: 2it [00:03,  1.69s/it][A
val: 3it [00:04,  1.03s/it][A
val: 3it [00:04,  1.03s/it][A
val: 4it [00:04,  1.38it/s][A
val: 4it [00:04,  1.38it/s][A
val: 5it [00:04,  1.81it/s][A
val: 5it [00:04,  1.81it/s][A
val: 6it [00:04,  2.22it/s][A
val: 6it [00:04,  2.22it/s][A
val: 7it [00:05,  2.59it/s][A
val: 7it [00:05,  2.59it/s][A
val: 8it [00:05,  2.92it/s][A
val: 8it [00:05,  2.91it/s][A
val: 9it [00:05,  3.18it/s][A
val: 9it [00:05,  3.17it/s][A
val: 10it [00:05,  3.39it/s][A
val: 10it [00:06,  3.38it/s][A
val: 11it [00:06,  3.55it/s][A
val: 11it [00:06,  3.54it/s][A
val: 12it [00:06,  3.66it/s][A
val: 12it [00:06,  3.66it/s][A
val: 13it [00:06,  3.74it/s][A
val: 13it [00:06,  3.74it/s][A
val: 14it [00:07,  3.81it/s][A
val: 14it [00:07,  3.81it/s][A
val: 15it [00:07,  3.86it/s][A
val: 15it [00:07,  3.85it/s][A
val: 16it [00:07,  3.89it/s][A
val: 16it [00:07,  3.88it/s][A
val: 17it [00:07,  3.91it/s][A
val: 17it [00:07,  3.91it/s][A
val: 18it [00:08,  3.93it/s][A
val: 18it [00:08,  3.93it/s][A
val: 19it [00:08,  3.94it/s][A
val: 19it [00:08,  3.94it/s][A
val: 20it [00:08,  3.95it/s][A
val: 20it [00:08,  3.95it/s][A
val: 21it [00:08,  3.96it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:09,  3.97it/s][A
val: 22it [00:09,  3.97it/s][A
val: 23it [00:09,  3.98it/s][A
val: 23it [00:09,  3.97it/s][A
val: 24it [00:09,  3.98it/s][A
val: 24it [00:09,  3.97it/s][A
                            [A
                            [Aepoch 1/100: 199it [04:23,  2.59it/s, loss=4.79]epoch 1/100: 199it [04:33,  2.59it/s, best_val=4.65, loss=4.53, val=4.65]epoch 1/100: 200it [04:33,  6.07s/it, loss=4.79]epoch 1/100: 200it [04:33,  6.07s/it, best_val=4.65, loss=4.53, val=4.65]epoch 1/100: 200it [04:33,  6.07s/it, loss=4.76]epoch 1/100: 200it [04:33,  6.07s/it, loss=4.46]                         epoch 1/100: 201it [04:33,  4.37s/it, loss=4.76]epoch 1/100: 201it [04:33,  4.37s/it, loss=4.46]epoch 1/100: 201it [04:33,  4.37s/it, loss=4.57]epoch 1/100: 201it [04:33,  4.37s/it, loss=4.69]epoch 1/100: 202it [04:33,  3.17s/it, loss=4.57]epoch 1/100: 202it [04:33,  3.17s/it, loss=4.69]epoch 1/100: 202it [04:34,  3.17s/it, loss=4.82]epoch 1/100: 202it [04:34,  3.17s/it, loss=4.61]epoch 1/100: 203it [04:34,  2.34s/it, loss=4.82]epoch 1/100: 203it [04:34,  2.34s/it, loss=4.61]epoch 1/100: 203it [04:34,  2.34s/it, loss=4.68]epoch 1/100: 203it [04:34,  2.34s/it, loss=4.74]epoch 1/100: 204it [04:34,  1.75s/it, loss=4.68]epoch 1/100: 204it [04:34,  1.75s/it, loss=4.74]epoch 1/100: 204it [04:35,  1.75s/it, loss=4.72]epoch 1/100: 204it [04:35,  1.75s/it, loss=4.55]epoch 1/100: 205it [04:35,  1.34s/it, loss=4.72]epoch 1/100: 205it [04:35,  1.34s/it, loss=4.55]epoch 1/100: 205it [04:35,  1.34s/it, loss=4.41]epoch 1/100: 205it [04:35,  1.34s/it, loss=4.49]epoch 1/100: 206it [04:35,  1.05s/it, loss=4.41]epoch 1/100: 206it [04:35,  1.05s/it, loss=4.49]epoch 1/100: 206it [04:35,  1.05s/it, loss=4.71]epoch 1/100: 206it [04:35,  1.05s/it, loss=4.44]epoch 1/100: 207it [04:35,  1.17it/s, loss=4.71]epoch 1/100: 207it [04:35,  1.17it/s, loss=4.44]epoch 1/100: 207it [04:36,  1.17it/s, loss=4.5] epoch 1/100: 207it [04:36,  1.17it/s, loss=4.6] epoch 1/100: 208it [04:36,  1.40it/s, loss=4.5]epoch 1/100: 208it [04:36,  1.40it/s, loss=4.6]epoch 1/100: 208it [04:36,  1.40it/s, loss=4.78]epoch 1/100: 208it [04:36,  1.40it/s, loss=4.59]epoch 1/100: 209it [04:36,  1.63it/s, loss=4.78]epoch 1/100: 209it [04:36,  1.63it/s, loss=4.59]epoch 1/100: 209it [04:37,  1.63it/s, loss=4.65]epoch 1/100: 209it [04:37,  1.63it/s, loss=4.77]epoch 1/100: 210it [04:37,  1.83it/s, loss=4.65]epoch 1/100: 210it [04:37,  1.83it/s, loss=4.77]epoch 1/100: 210it [04:37,  1.83it/s, loss=4.63]epoch 1/100: 210it [04:37,  1.83it/s, loss=4.76]epoch 1/100: 211it [04:37,  2.01it/s, loss=4.63]epoch 1/100: 211it [04:37,  2.01it/s, loss=4.76]epoch 1/100: 211it [04:37,  2.01it/s, loss=4.75]epoch 1/100: 211it [04:37,  2.01it/s, loss=4.44]epoch 1/100: 212it [04:37,  2.16it/s, loss=4.75]epoch 1/100: 212it [04:37,  2.16it/s, loss=4.44]epoch 1/100: 212it [04:38,  2.16it/s, loss=4.8] epoch 1/100: 212it [04:38,  2.16it/s, loss=4.75]epoch 1/100: 213it [04:38,  2.27it/s, loss=4.8]epoch 1/100: 213it [04:38,  2.27it/s, loss=4.75]epoch 1/100: 213it [04:38,  2.27it/s, loss=4.72]epoch 1/100: 213it [04:38,  2.27it/s, loss=4.67]epoch 1/100: 214it [04:38,  2.36it/s, loss=4.67]epoch 1/100: 214it [04:38,  2.36it/s, loss=4.72]epoch 1/100: 214it [04:38,  2.36it/s, loss=4.67]epoch 1/100: 214it [04:38,  2.36it/s, loss=4.67]epoch 1/100: 215it [04:38,  2.43it/s, loss=4.67]epoch 1/100: 215it [04:38,  2.43it/s, loss=4.67]epoch 1/100: 215it [04:39,  2.43it/s, loss=4.61]epoch 1/100: 215it [04:39,  2.43it/s, loss=4.69]epoch 1/100: 216it [04:39,  2.48it/s, loss=4.61]epoch 1/100: 216it [04:39,  2.48it/s, loss=4.69]epoch 1/100: 216it [04:39,  2.48it/s, loss=4.62]epoch 1/100: 216it [04:39,  2.48it/s, loss=4.61]epoch 1/100: 217it [04:39,  2.51it/s, loss=4.62]epoch 1/100: 217it [04:39,  2.51it/s, loss=4.61]epoch 1/100: 217it [04:40,  2.51it/s, loss=4.53]epoch 1/100: 217it [04:40,  2.51it/s, loss=4.64]epoch 1/100: 218it [04:40,  2.54it/s, loss=4.64]epoch 1/100: 218it [04:40,  2.54it/s, loss=4.53]epoch 1/100: 218it [04:40,  2.54it/s, loss=4.75]epoch 1/100: 218it [04:40,  2.54it/s, loss=4.63]epoch 1/100: 219it [04:40,  2.55it/s, loss=4.75]epoch 1/100: 219it [04:40,  2.55it/s, loss=4.63]epoch 1/100: 219it [04:40,  2.55it/s, loss=4.71]epoch 1/100: 219it [04:40,  2.55it/s, loss=4.74]epoch 1/100: 220it [04:40,  2.57it/s, loss=4.71]epoch 1/100: 220it [04:40,  2.57it/s, loss=4.74]epoch 1/100: 220it [04:41,  2.57it/s, loss=4.51]epoch 1/100: 220it [04:41,  2.57it/s, loss=4.72]epoch 1/100: 221it [04:41,  2.58it/s, loss=4.51]epoch 1/100: 221it [04:41,  2.58it/s, loss=4.72]epoch 1/100: 221it [04:41,  2.58it/s, loss=4.7] epoch 1/100: 221it [04:41,  2.58it/s, loss=4.66]epoch 1/100: 222it [04:41,  2.58it/s, loss=4.7]epoch 1/100: 222it [04:41,  2.58it/s, loss=4.66]epoch 1/100: 222it [04:42,  2.58it/s, loss=4.61]epoch 1/100: 222it [04:42,  2.58it/s, loss=4.38]epoch 1/100: 223it [04:42,  2.59it/s, loss=4.61]epoch 1/100: 223it [04:42,  2.59it/s, loss=4.38]epoch 1/100: 223it [04:42,  2.59it/s, loss=4.71]epoch 1/100: 223it [04:42,  2.59it/s, loss=4.73]epoch 1/100: 224it [04:42,  2.59it/s, loss=4.71]epoch 1/100: 224it [04:42,  2.59it/s, loss=4.73]epoch 1/100: 224it [04:42,  2.59it/s, loss=4.68]epoch 1/100: 224it [04:42,  2.59it/s, loss=4.68]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.05s/it][A
val: 1it [00:03,  3.38s/it][A
val: 2it [00:03,  1.51s/it][A
val: 3it [00:03,  1.07it/s][A
val: 4it [00:03,  1.50it/s][A
val: 5it [00:04,  1.93it/s][A
val: 6it [00:04,  2.34it/s][A
val: 7it [00:04,  2.71it/s][A
val: 8it [00:04,  3.01it/s][A
val: 2it [00:05,  2.47s/it][A
val: 9it [00:05,  3.26it/s][A
val: 3it [00:05,  1.46s/it][A
val: 10it [00:05,  3.45it/s][A
val: 4it [00:05,  1.02it/s][A
val: 11it [00:05,  3.60it/s][A
val: 5it [00:05,  1.39it/s][A
val: 12it [00:06,  3.70it/s][A
val: 6it [00:06,  1.78it/s][A
val: 13it [00:06,  3.78it/s][A
val: 7it [00:06,  2.18it/s][A
val: 14it [00:06,  3.84it/s][A
val: 8it [00:06,  2.54it/s][A
val: 15it [00:06,  3.89it/s][A
val: 9it [00:06,  2.86it/s][A
val: 16it [00:07,  3.92it/s][A
val: 10it [00:07,  3.13it/s][A
val: 17it [00:07,  3.94it/s][A
val: 11it [00:07,  3.34it/s][A
val: 18it [00:07,  3.95it/s][A
val: 12it [00:07,  3.51it/s][A
val: 19it [00:07,  3.97it/s][A
val: 13it [00:07,  3.64it/s][A
val: 20it [00:08,  3.97it/s][A
val: 14it [00:08,  3.73it/s][A
val: 21it [00:08,  3.98it/s][A
val: 15it [00:08,  3.80it/s][A
val: 22it [00:08,  3.98it/s][A
val: 16it [00:08,  3.85it/s][A
val: 23it [00:08,  3.98it/s][A
val: 17it [00:09,  3.88it/s][A
val: 24it [00:09,  3.98it/s][A
                            [A
val: 18it [00:09,  3.90it/s][A
val: 19it [00:09,  3.92it/s][A
val: 20it [00:09,  3.93it/s][A
val: 21it [00:10,  3.94it/s][A
val: 22it [00:10,  3.95it/s][A
val: 23it [00:10,  3.95it/s][A
val: 24it [00:10,  3.96it/s][A
                            [Aepoch 1/100: 224it [04:53,  2.59it/s, loss=4.68]epoch 1/100: 224it [05:02,  2.59it/s, best_val=4.61, loss=4.68, val=4.61]epoch 1/100: 225it [05:02,  6.43s/it, loss=4.68]epoch 1/100: 225it [05:02,  6.43s/it, best_val=4.61, loss=4.68, val=4.61]epoch 1/100: 225it [05:03,  6.43s/it, loss=4.55]epoch 1/100: 225it [05:03,  6.43s/it, loss=4.71]                         epoch 1/100: 226it [05:03,  4.62s/it, loss=4.55]epoch 1/100: 226it [05:03,  4.62s/it, loss=4.71]epoch 1/100: 226it [05:03,  4.62s/it, loss=4.59]epoch 1/100: 226it [05:03,  4.62s/it, loss=4.57]epoch 1/100: 227it [05:03,  3.35s/it, loss=4.59]epoch 1/100: 227it [05:03,  3.35s/it, loss=4.57]epoch 1/100: 227it [05:04,  3.35s/it, loss=4.73]epoch 1/100: 227it [05:04,  3.35s/it, loss=4.69]epoch 1/100: 228it [05:04,  2.46s/it, loss=4.73]epoch 1/100: 228it [05:04,  2.46s/it, loss=4.69]epoch 1/100: 228it [05:04,  2.46s/it, loss=4.6] epoch 1/100: 228it [05:04,  2.46s/it, loss=4.57]epoch 1/100: 229it [05:04,  1.84s/it, loss=4.6]epoch 1/100: 229it [05:04,  1.84s/it, loss=4.57]epoch 1/100: 229it [05:04,  1.84s/it, loss=4.54]epoch 1/100: 229it [05:04,  1.84s/it, loss=4.65]epoch 1/100: 230it [05:04,  1.40s/it, loss=4.54]epoch 1/100: 230it [05:04,  1.40s/it, loss=4.65]epoch 1/100: 230it [05:05,  1.40s/it, loss=4.72]epoch 1/100: 230it [05:05,  1.40s/it, loss=4.6] epoch 1/100: 231it [05:05,  1.10s/it, loss=4.72]epoch 1/100: 231it [05:05,  1.10s/it, loss=4.6]epoch 1/100: 231it [05:05,  1.10s/it, loss=4.47]epoch 1/100: 231it [05:05,  1.10s/it, loss=4.5]epoch 1/100: 232it [05:05,  1.13it/s, loss=4.47]epoch 1/100: 232it [05:05,  1.13it/s, loss=4.5]epoch 1/100: 232it [05:06,  1.13it/s, loss=4.81]epoch 1/100: 232it [05:06,  1.13it/s, loss=4.67]epoch 1/100: 233it [05:06,  1.36it/s, loss=4.81]epoch 1/100: 233it [05:06,  1.36it/s, loss=4.67]epoch 1/100: 233it [05:06,  1.36it/s, loss=4.42]epoch 1/100: 233it [05:06,  1.36it/s, loss=4.44]epoch 1/100: 234it [05:06,  1.59it/s, loss=4.42]epoch 1/100: 234it [05:06,  1.59it/s, loss=4.44]epoch 1/100: 234it [05:06,  1.59it/s, loss=4.51]epoch 1/100: 234it [05:06,  1.59it/s, loss=4.7] epoch 1/100: 235it [05:06,  1.80it/s, loss=4.51]epoch 1/100: 235it [05:06,  1.80it/s, loss=4.7]epoch 1/100: 235it [05:07,  1.80it/s, loss=4.52]epoch 1/100: 235it [05:07,  1.80it/s, loss=4.81]epoch 1/100: 236it [05:07,  1.98it/s, loss=4.52]epoch 1/100: 236it [05:07,  1.98it/s, loss=4.81]epoch 1/100: 236it [05:07,  1.98it/s, loss=4.56]epoch 1/100: 236it [05:07,  1.98it/s, loss=4.76]epoch 1/100: 237it [05:07,  2.13it/s, loss=4.56]epoch 1/100: 237it [05:07,  2.13it/s, loss=4.76]epoch 1/100: 237it [05:07,  2.13it/s, loss=4.63]epoch 1/100: 237it [05:07,  2.13it/s, loss=4.6] epoch 1/100: 238it [05:07,  2.25it/s, loss=4.63]epoch 1/100: 238it [05:07,  2.25it/s, loss=4.6]epoch 1/100: 238it [05:08,  2.25it/s, loss=4.72]epoch 1/100: 238it [05:08,  2.25it/s, loss=4.59]epoch 1/100: 239it [05:08,  2.34it/s, loss=4.72]epoch 1/100: 239it [05:08,  2.34it/s, loss=4.59]epoch 1/100: 239it [05:08,  2.34it/s, loss=4.56]epoch 1/100: 239it [05:08,  2.34it/s, loss=4.73]epoch 1/100: 240it [05:08,  2.41it/s, loss=4.73]epoch 1/100: 240it [05:08,  2.41it/s, loss=4.56]epoch 1/100: 240it [05:09,  2.41it/s, loss=4.73]epoch 1/100: 240it [05:09,  2.41it/s, loss=4.65]epoch 1/100: 241it [05:09,  2.46it/s, loss=4.73]epoch 1/100: 241it [05:09,  2.46it/s, loss=4.65]epoch 1/100: 241it [05:09,  2.46it/s, loss=4.56]epoch 1/100: 241it [05:09,  2.46it/s, loss=4.76]epoch 1/100: 242it [05:09,  2.50it/s, loss=4.56]epoch 1/100: 242it [05:09,  2.50it/s, loss=4.76]epoch 1/100: 242it [05:09,  2.50it/s, loss=4.73]epoch 1/100: 242it [05:09,  2.50it/s, loss=4.63]epoch 1/100: 243it [05:09,  2.53it/s, loss=4.73]epoch 1/100: 243it [05:09,  2.53it/s, loss=4.63]epoch 1/100: 243it [05:10,  2.53it/s, loss=4.45]epoch 1/100: 243it [05:10,  2.53it/s, loss=4.65]epoch 1/100: 244it [05:10,  2.55it/s, loss=4.45]epoch 1/100: 244it [05:10,  2.55it/s, loss=4.65]epoch 1/100: 244it [05:10,  2.55it/s, loss=4.7] epoch 1/100: 244it [05:10,  2.55it/s, loss=4.58]epoch 1/100: 245it [05:10,  2.56it/s, loss=4.58]epoch 1/100: 245it [05:10,  2.56it/s, loss=4.7]epoch 1/100: 245it [05:11,  2.56it/s, loss=4.71]epoch 1/100: 245it [05:11,  2.56it/s, loss=4.68]epoch 1/100: 246it [05:11,  2.57it/s, loss=4.71]epoch 1/100: 246it [05:11,  2.57it/s, loss=4.68]epoch 1/100: 246it [05:11,  2.57it/s, loss=4.91]epoch 1/100: 246it [05:11,  2.57it/s, loss=4.58]epoch 1/100: 247it [05:11,  2.57it/s, loss=4.58]epoch 1/100: 247it [05:11,  2.57it/s, loss=4.91]epoch 1/100: 247it [05:11,  2.57it/s, loss=4.4] epoch 1/100: 247it [05:11,  2.57it/s, loss=4.43]epoch 1/100: 248it [05:11,  2.58it/s, loss=4.4]epoch 1/100: 248it [05:11,  2.58it/s, loss=4.43]epoch 1/100: 248it [05:12,  2.58it/s, loss=4.51]epoch 1/100: 248it [05:12,  2.58it/s, loss=4.46]epoch 1/100: 249it [05:12,  2.58it/s, loss=4.51]epoch 1/100: 249it [05:12,  2.58it/s, loss=4.46]epoch 1/100: 249it [05:12,  2.58it/s, loss=4.56]epoch 1/100: 249it [05:12,  2.58it/s, loss=4.45]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.27s/it][A
val: 1it [00:03,  3.46s/it][A
val: 2it [00:03,  1.57s/it][A
val: 3it [00:03,  1.03it/s][A
val: 4it [00:04,  1.46it/s][A
val: 5it [00:04,  1.89it/s][A
val: 6it [00:04,  2.29it/s][A
val: 7it [00:04,  2.66it/s][A
val: 8it [00:05,  2.97it/s][A
val: 9it [00:05,  3.23it/s][A
val: 10it [00:05,  3.42it/s][A
val: 11it [00:05,  3.57it/s][A
val: 12it [00:06,  3.68it/s][A
val: 13it [00:06,  3.76it/s][A
val: 14it [00:06,  3.82it/s][A
val: 15it [00:06,  3.86it/s][A
val: 16it [00:07,  3.89it/s][A
val: 17it [00:07,  3.91it/s][A
val: 18it [00:07,  3.93it/s][A
val: 19it [00:08,  3.94it/s][A
val: 20it [00:08,  3.94it/s][A
val: 21it [00:08,  3.95it/s][A
val: 22it [00:08,  3.96it/s][A
val: 23it [00:09,  3.96it/s][A
val: 24it [00:09,  3.96it/s][A
                            [AFailed to connect to remote data host. Retrying in 5sec [1/20]

val: 2it [00:18, 10.37s/it][A
val: 3it [00:18,  5.75s/it][A
val: 4it [00:19,  3.58s/it][A
val: 5it [00:19,  2.38s/it][A
val: 6it [00:19,  1.65s/it][A
val: 7it [00:19,  1.19s/it][A
val: 8it [00:20,  1.12it/s][A
val: 9it [00:20,  1.44it/s][A
val: 10it [00:20,  1.80it/s][A
val: 11it [00:20,  2.16it/s][A
val: 12it [00:21,  2.51it/s][A
val: 13it [00:21,  2.83it/s][A
val: 14it [00:21,  3.11it/s][A
val: 15it [00:21,  3.33it/s][A
val: 16it [00:22,  3.51it/s][A
val: 17it [00:22,  3.64it/s][A
val: 18it [00:22,  3.74it/s][A
val: 19it [00:22,  3.82it/s][A
val: 20it [00:23,  3.87it/s][A
val: 21it [00:23,  3.91it/s][A
val: 22it [00:23,  3.94it/s][A
val: 23it [00:23,  3.96it/s][A
val: 24it [00:24,  3.97it/s][A
                            [Aepoch 1/100: 249it [05:36,  2.58it/s, loss=4.56]epoch 1/100: 249it [05:46,  2.58it/s, best_val=4.57, loss=4.45, val=4.57]epoch 1/100: 250it [05:46, 10.47s/it, loss=4.56]epoch 1/100: 250it [05:46, 10.47s/it, best_val=4.57, loss=4.45, val=4.57]epoch 1/100: 250it [05:46, 10.47s/it, loss=4.72]                         epoch 1/100: 250it [05:46, 10.47s/it, loss=4.75]epoch 1/100: 251it [05:46,  7.45s/it, loss=4.75]epoch 1/100: 251it [05:46,  7.45s/it, loss=4.72]epoch 1/100: 251it [05:46,  7.45s/it, loss=4.59]epoch 1/100: 251it [05:46,  7.45s/it, loss=4.56]epoch 1/100: 252it [05:46,  5.33s/it, loss=4.59]epoch 1/100: 252it [05:46,  5.33s/it, loss=4.56]epoch 1/100: 252it [05:47,  5.33s/it, loss=4.6] epoch 1/100: 252it [05:47,  5.33s/it, loss=4.42]epoch 1/100: 253it [05:47,  3.84s/it, loss=4.6]epoch 1/100: 253it [05:47,  3.84s/it, loss=4.42]epoch 1/100: 253it [05:47,  3.84s/it, loss=4.79]epoch 1/100: 253it [05:47,  3.84s/it, loss=4.64]epoch 1/100: 254it [05:47,  2.81s/it, loss=4.79]epoch 1/100: 254it [05:47,  2.81s/it, loss=4.64]epoch 1/100: 254it [05:48,  2.81s/it, loss=4.57]epoch 1/100: 254it [05:48,  2.81s/it, loss=4.68]epoch 1/100: 255it [05:48,  2.08s/it, loss=4.57]epoch 1/100: 255it [05:48,  2.08s/it, loss=4.68]epoch 1/100: 255it [05:48,  2.08s/it, loss=4.6] epoch 1/100: 255it [05:48,  2.08s/it, loss=4.73]epoch 1/100: 256it [05:48,  1.57s/it, loss=4.73]epoch 1/100: 256it [05:48,  1.57s/it, loss=4.6]epoch 1/100: 256it [05:48,  1.57s/it, loss=4.69]epoch 1/100: 256it [05:48,  1.57s/it, loss=4.68]epoch 1/100: 257it [05:48,  1.22s/it, loss=4.69]epoch 1/100: 257it [05:48,  1.22s/it, loss=4.68]epoch 1/100: 257it [05:49,  1.22s/it, loss=4.75]epoch 1/100: 257it [05:49,  1.22s/it, loss=4.61]epoch 1/100: 258it [05:49,  1.03it/s, loss=4.61]epoch 1/100: 258it [05:49,  1.03it/s, loss=4.75]epoch 1/100: 258it [05:49,  1.03it/s, loss=4.77]epoch 1/100: 258it [05:49,  1.03it/s, loss=4.7] epoch 1/100: 259it [05:49,  1.26it/s, loss=4.77]epoch 1/100: 259it [05:49,  1.26it/s, loss=4.7]epoch 1/100: 259it [05:50,  1.26it/s, loss=4.45]epoch 1/100: 259it [05:50,  1.26it/s, loss=4.51]epoch 1/100: 260it [05:50,  1.49it/s, loss=4.45]epoch 1/100: 260it [05:50,  1.49it/s, loss=4.51]epoch 1/100: 260it [05:50,  1.49it/s, loss=4.87]epoch 1/100: 260it [05:50,  1.49it/s, loss=4.52]epoch 1/100: 261it [05:50,  1.71it/s, loss=4.87]epoch 1/100: 261it [05:50,  1.71it/s, loss=4.52]epoch 1/100: 261it [05:50,  1.71it/s, loss=4.69]epoch 1/100: 261it [05:50,  1.71it/s, loss=4.41]epoch 1/100: 262it [05:50,  1.90it/s, loss=4.41]epoch 1/100: 262it [05:50,  1.90it/s, loss=4.69]epoch 1/100: 262it [05:51,  1.90it/s, loss=4.46]epoch 1/100: 262it [05:51,  1.90it/s, loss=4.61]epoch 1/100: 263it [05:51,  2.07it/s, loss=4.46]epoch 1/100: 263it [05:51,  2.07it/s, loss=4.61]epoch 1/100: 263it [05:51,  2.07it/s, loss=4.54]epoch 1/100: 263it [05:51,  2.07it/s, loss=4.76]epoch 1/100: 264it [05:51,  2.20it/s, loss=4.76]epoch 1/100: 264it [05:51,  2.20it/s, loss=4.54]epoch 1/100: 264it [05:51,  2.20it/s, loss=4.5] epoch 1/100: 264it [05:51,  2.20it/s, loss=4.55]epoch 1/100: 265it [05:51,  2.31it/s, loss=4.5]epoch 1/100: 265it [05:51,  2.31it/s, loss=4.55]epoch 1/100: 265it [05:52,  2.31it/s, loss=4.52]epoch 1/100: 265it [05:52,  2.31it/s, loss=4.71]epoch 1/100: 266it [05:52,  2.39it/s, loss=4.52]epoch 1/100: 266it [05:52,  2.39it/s, loss=4.71]epoch 1/100: 266it [05:52,  2.39it/s, loss=4.47]epoch 1/100: 266it [05:52,  2.39it/s, loss=4.63]epoch 1/100: 267it [05:52,  2.45it/s, loss=4.47]epoch 1/100: 267it [05:52,  2.45it/s, loss=4.63]epoch 1/100: 267it [05:53,  2.45it/s, loss=4.68]epoch 1/100: 267it [05:53,  2.45it/s, loss=4.72]epoch 1/100: 268it [05:53,  2.49it/s, loss=4.68]epoch 1/100: 268it [05:53,  2.49it/s, loss=4.72]epoch 1/100: 268it [05:53,  2.49it/s, loss=4.53]epoch 1/100: 268it [05:53,  2.49it/s, loss=4.46]epoch 1/100: 269it [05:53,  2.52it/s, loss=4.53]epoch 1/100: 269it [05:53,  2.52it/s, loss=4.46]epoch 1/100: 269it [05:53,  2.52it/s, loss=4.4] epoch 1/100: 269it [05:53,  2.52it/s, loss=4.63]epoch 1/100: 270it [05:53,  2.54it/s, loss=4.4]epoch 1/100: 270it [05:53,  2.54it/s, loss=4.63]epoch 1/100: 270it [05:54,  2.54it/s, loss=4.73]epoch 1/100: 270it [05:54,  2.54it/s, loss=4.67]epoch 1/100: 271it [05:54,  2.56it/s, loss=4.67]epoch 1/100: 271it [05:54,  2.56it/s, loss=4.73]epoch 1/100: 271it [05:54,  2.56it/s, loss=4.32]epoch 1/100: 271it [05:54,  2.56it/s, loss=4.7] epoch 1/100: 272it [05:54,  2.57it/s, loss=4.32]epoch 1/100: 272it [05:54,  2.57it/s, loss=4.7]epoch 1/100: 272it [05:55,  2.57it/s, loss=4.72]epoch 1/100: 272it [05:55,  2.57it/s, loss=4.63]epoch 1/100: 273it [05:55,  2.58it/s, loss=4.72]epoch 1/100: 273it [05:55,  2.58it/s, loss=4.63]epoch 1/100: 273it [05:55,  2.58it/s, loss=4.61]epoch 1/100: 273it [05:55,  2.58it/s, loss=4.5] epoch 1/100: 274it [05:55,  2.58it/s, loss=4.61]epoch 1/100: 274it [05:55,  2.58it/s, loss=4.5]epoch 1/100: 274it [05:55,  2.58it/s, loss=4.53]epoch 1/100: 274it [05:55,  2.58it/s, loss=4.8] 

val: 0it [00:00, ?it/s]val: 0it [00:00, ?it/s][A[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.47s/it][A
val: 1it [00:03,  3.67s/it][A
val: 2it [00:03,  1.66s/it][A
val: 3it [00:04,  1.02s/it][A
val: 4it [00:04,  1.40it/s][A
val: 5it [00:04,  1.83it/s][A
val: 6it [00:04,  2.24it/s][A
val: 7it [00:05,  2.60it/s][A
val: 2it [00:05,  2.51s/it][A
val: 8it [00:05,  2.92it/s][A
val: 3it [00:05,  1.48s/it][A
val: 9it [00:05,  3.19it/s][A
val: 4it [00:05,  1.01it/s][A
val: 10it [00:05,  3.40it/s][A
val: 5it [00:06,  1.38it/s][A
val: 11it [00:06,  3.55it/s][A
val: 6it [00:06,  1.77it/s][A
val: 12it [00:06,  3.66it/s][A
val: 7it [00:06,  2.16it/s][A
val: 13it [00:06,  3.75it/s][A
val: 8it [00:06,  2.53it/s][A
val: 14it [00:06,  3.82it/s][A
val: 9it [00:07,  2.86it/s][A
val: 15it [00:07,  3.86it/s][A
val: 10it [00:07,  3.13it/s][A
val: 16it [00:07,  3.89it/s][A
val: 11it [00:07,  3.35it/s][A
val: 17it [00:07,  3.91it/s][A
val: 12it [00:07,  3.52it/s][A
val: 18it [00:07,  3.93it/s][A
val: 13it [00:08,  3.65it/s][A
val: 19it [00:08,  3.94it/s][A
val: 14it [00:08,  3.74it/s][A
val: 20it [00:08,  3.95it/s][A
val: 15it [00:08,  3.81it/s][A
val: 21it [00:08,  3.95it/s][A
val: 16it [00:08,  3.86it/s][A
val: 22it [00:08,  3.96it/s][A
val: 17it [00:09,  3.90it/s][A
val: 23it [00:09,  3.96it/s][A
val: 18it [00:09,  3.92it/s][A
val: 24it [00:09,  3.96it/s][A
val: 19it [00:09,  3.94it/s][A
                            [A
val: 20it [00:09,  3.95it/s][A
val: 21it [00:10,  3.96it/s][A
val: 22it [00:10,  3.97it/s][A
val: 23it [00:10,  3.98it/s][A
val: 24it [00:10,  3.98it/s][A
                            [Aepoch 1/100: 274it [06:06,  2.58it/s, loss=4.53]epoch 1/100: 274it [06:16,  2.58it/s, best_val=4.53, loss=4.8, val=4.53]epoch 1/100: 275it [06:16,  6.48s/it, loss=4.53]epoch 1/100: 275it [06:16,  6.48s/it, best_val=4.53, loss=4.8, val=4.53]epoch 1/100: 275it [06:16,  6.48s/it, loss=4.24]epoch 1/100: 275it [06:16,  6.48s/it, loss=4.61]                        epoch 1/100: 276it [06:16,  4.65s/it, loss=4.24]epoch 1/100: 276it [06:16,  4.65s/it, loss=4.61]epoch 1/100: 276it [06:16,  4.65s/it, loss=4.62]epoch 1/100: 276it [06:16,  4.65s/it, loss=4.56]epoch 1/100: 277it [06:16,  3.37s/it, loss=4.62]epoch 1/100: 277it [06:16,  3.37s/it, loss=4.56]epoch 1/100: 277it [06:17,  3.37s/it, loss=4.53]epoch 1/100: 277it [06:17,  3.37s/it, loss=4.78]epoch 1/100: 278it [06:17,  2.48s/it, loss=4.53]epoch 1/100: 278it [06:17,  2.48s/it, loss=4.78]epoch 1/100: 278it [06:17,  2.48s/it, loss=4.55]epoch 1/100: 278it [06:17,  2.48s/it, loss=4.6] epoch 1/100: 279it [06:17,  1.85s/it, loss=4.55]epoch 1/100: 279it [06:17,  1.85s/it, loss=4.6]epoch 1/100: 279it [06:18,  1.85s/it, loss=4.62]epoch 1/100: 279it [06:18,  1.85s/it, loss=4.72]epoch 1/100: 280it [06:18,  1.41s/it, loss=4.62]epoch 1/100: 280it [06:18,  1.41s/it, loss=4.72]epoch 1/100: 280it [06:18,  1.41s/it, loss=4.47]epoch 1/100: 280it [06:18,  1.41s/it, loss=4.45]epoch 1/100: 281it [06:18,  1.10s/it, loss=4.47]epoch 1/100: 281it [06:18,  1.10s/it, loss=4.45]epoch 1/100: 281it [06:18,  1.10s/it, loss=4.64]epoch 1/100: 281it [06:18,  1.10s/it, loss=4.44]epoch 1/100: 282it [06:18,  1.13it/s, loss=4.64]epoch 1/100: 282it [06:18,  1.13it/s, loss=4.44]epoch 1/100: 282it [06:19,  1.13it/s, loss=4.49]epoch 1/100: 282it [06:19,  1.13it/s, loss=4.4] epoch 1/100: 283it [06:19,  1.36it/s, loss=4.49]epoch 1/100: 283it [06:19,  1.36it/s, loss=4.4]epoch 1/100: 283it [06:19,  1.36it/s, loss=4.47]epoch 1/100: 283it [06:19,  1.36it/s, loss=4.34]epoch 1/100: 284it [06:19,  1.59it/s, loss=4.47]epoch 1/100: 284it [06:19,  1.59it/s, loss=4.34]epoch 1/100: 284it [06:20,  1.59it/s, loss=4.62]epoch 1/100: 284it [06:20,  1.59it/s, loss=4.51]epoch 1/100: 285it [06:20,  1.80it/s, loss=4.62]epoch 1/100: 285it [06:20,  1.80it/s, loss=4.51]epoch 1/100: 285it [06:20,  1.80it/s, loss=4.42]epoch 1/100: 285it [06:20,  1.80it/s, loss=4.62]epoch 1/100: 286it [06:20,  1.98it/s, loss=4.42]epoch 1/100: 286it [06:20,  1.98it/s, loss=4.62]epoch 1/100: 286it [06:20,  1.98it/s, loss=4.52]epoch 1/100: 286it [06:20,  1.98it/s, loss=4.39]epoch 1/100: 287it [06:20,  2.13it/s, loss=4.39]epoch 1/100: 287it [06:20,  2.13it/s, loss=4.52]epoch 1/100: 287it [06:21,  2.13it/s, loss=4.43]epoch 1/100: 287it [06:21,  2.13it/s, loss=4.36]epoch 1/100: 288it [06:21,  2.25it/s, loss=4.36]epoch 1/100: 288it [06:21,  2.25it/s, loss=4.43]epoch 1/100: 288it [06:21,  2.25it/s, loss=4.63]epoch 1/100: 288it [06:21,  2.25it/s, loss=4.41]epoch 1/100: 289it [06:21,  2.35it/s, loss=4.63]epoch 1/100: 289it [06:21,  2.35it/s, loss=4.41]epoch 1/100: 289it [06:21,  2.35it/s, loss=4.59]epoch 1/100: 289it [06:21,  2.35it/s, loss=4.56]epoch 1/100: 290it [06:21,  2.42it/s, loss=4.59]epoch 1/100: 290it [06:21,  2.42it/s, loss=4.56]epoch 1/100: 290it [06:22,  2.42it/s, loss=4.6] epoch 1/100: 290it [06:22,  2.42it/s, loss=4.68]epoch 1/100: 291it [06:22,  2.47it/s, loss=4.6]epoch 1/100: 291it [06:22,  2.47it/s, loss=4.68]epoch 1/100: 291it [06:22,  2.47it/s, loss=4.55]epoch 1/100: 291it [06:22,  2.47it/s, loss=4.44]epoch 1/100: 292it [06:22,  2.50it/s, loss=4.44]epoch 1/100: 292it [06:22,  2.50it/s, loss=4.55]epoch 1/100: 292it [06:23,  2.50it/s, loss=4.62]epoch 1/100: 292it [06:23,  2.50it/s, loss=4.58]epoch 1/100: 293it [06:23,  2.53it/s, loss=4.62]epoch 1/100: 293it [06:23,  2.53it/s, loss=4.58]epoch 1/100: 293it [06:23,  2.53it/s, loss=4.74]epoch 1/100: 293it [06:23,  2.53it/s, loss=4.42]epoch 1/100: 294it [06:23,  2.55it/s, loss=4.74]epoch 1/100: 294it [06:23,  2.55it/s, loss=4.42]epoch 1/100: 294it [06:23,  2.55it/s, loss=4.75]epoch 1/100: 294it [06:23,  2.55it/s, loss=4.51]epoch 1/100: 295it [06:23,  2.56it/s, loss=4.75]epoch 1/100: 295it [06:23,  2.56it/s, loss=4.51]epoch 1/100: 295it [06:24,  2.56it/s, loss=4.63]epoch 1/100: 295it [06:24,  2.56it/s, loss=4.45]epoch 1/100: 296it [06:24,  2.57it/s, loss=4.63]epoch 1/100: 296it [06:24,  2.57it/s, loss=4.45]epoch 1/100: 296it [06:24,  2.57it/s, loss=4.49]epoch 1/100: 296it [06:24,  2.57it/s, loss=4.62]epoch 1/100: 297it [06:24,  2.58it/s, loss=4.62]epoch 1/100: 297it [06:24,  2.58it/s, loss=4.49]epoch 1/100: 297it [06:25,  2.58it/s, loss=4.49]epoch 1/100: 297it [06:25,  2.58it/s, loss=4.39]epoch 1/100: 298it [06:25,  2.59it/s, loss=4.49]epoch 1/100: 298it [06:25,  2.59it/s, loss=4.39]epoch 1/100: 298it [06:25,  2.59it/s, loss=4.53]epoch 1/100: 298it [06:25,  2.59it/s, loss=4.76]epoch 1/100: 299it [06:25,  2.59it/s, loss=4.53]epoch 1/100: 299it [06:25,  2.59it/s, loss=4.76]epoch 1/100: 299it [06:25,  2.59it/s, loss=4.49]epoch 1/100: 299it [06:25,  2.59it/s, loss=4.57]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.12s/it][A
val: 1it [00:03,  3.28s/it][A
val: 2it [00:03,  1.43s/it][A
val: 2it [00:03,  1.50s/it][A
val: 3it [00:03,  1.12it/s][A
val: 3it [00:03,  1.08it/s][A
val: 4it [00:03,  1.56it/s][A
val: 4it [00:04,  1.51it/s][A
val: 5it [00:04,  2.00it/s][A
val: 5it [00:04,  1.94it/s][A
val: 6it [00:04,  2.41it/s][A
val: 6it [00:04,  2.35it/s][A
val: 7it [00:04,  2.77it/s][A
val: 7it [00:04,  2.71it/s][A
val: 8it [00:04,  3.06it/s][A
val: 8it [00:05,  3.01it/s][A
val: 9it [00:05,  3.30it/s][A
val: 9it [00:05,  3.25it/s][A
val: 10it [00:05,  3.49it/s][A
val: 10it [00:05,  3.44it/s][A
val: 11it [00:05,  3.62it/s][A
val: 11it [00:05,  3.59it/s][A
val: 12it [00:05,  3.72it/s][A
val: 12it [00:06,  3.69it/s][A
val: 13it [00:06,  3.80it/s][A
val: 13it [00:06,  3.78it/s][A
val: 14it [00:06,  3.85it/s][A
val: 14it [00:06,  3.83it/s][A
val: 15it [00:06,  3.90it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:06,  3.92it/s][A
val: 16it [00:07,  3.90it/s][A
val: 17it [00:07,  3.94it/s][A
val: 17it [00:07,  3.92it/s][A
val: 18it [00:07,  3.95it/s][A
val: 18it [00:07,  3.94it/s][A
val: 19it [00:07,  3.96it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:07,  3.96it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.97it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:08,  3.98it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.98it/s][A
val: 23it [00:08,  3.97it/s][A
val: 24it [00:08,  3.98it/s][A
                            [A
val: 24it [00:09,  3.97it/s][A
                            [Aepoch 1/100: 299it [06:35,  2.59it/s, loss=4.57]epoch 1/100: 299it [06:44,  2.59it/s, best_val=4.51, loss=4.49, val=4.51]epoch 1/100: 300it [06:44,  6.05s/it, loss=4.57]epoch 1/100: 300it [06:44,  6.05s/it, best_val=4.51, loss=4.49, val=4.51]epoch 1/100: 300it [06:45,  6.05s/it, loss=4.57]epoch 1/100: 300it [06:45,  6.05s/it, loss=4.61]                         epoch 1/100: 301it [06:45,  4.35s/it, loss=4.57]epoch 1/100: 301it [06:45,  4.35s/it, loss=4.61]epoch 1/100: 301it [06:45,  4.35s/it, loss=4.4] epoch 1/100: 301it [06:45,  4.35s/it, loss=4.63]epoch 1/100: 302it [06:45,  3.16s/it, loss=4.4]epoch 1/100: 302it [06:45,  3.16s/it, loss=4.63]epoch 1/100: 302it [06:45,  3.16s/it, loss=4.52]epoch 1/100: 302it [06:45,  3.16s/it, loss=4.26]epoch 1/100: 303it [06:45,  2.33s/it, loss=4.52]epoch 1/100: 303it [06:45,  2.33s/it, loss=4.26]epoch 1/100: 303it [06:46,  2.33s/it, loss=4.38]epoch 1/100: 303it [06:46,  2.33s/it, loss=4.59]epoch 1/100: 304it [06:46,  1.75s/it, loss=4.38]epoch 1/100: 304it [06:46,  1.75s/it, loss=4.59]epoch 1/100: 304it [06:46,  1.75s/it, loss=4.48]epoch 1/100: 304it [06:46,  1.75s/it, loss=4.67]epoch 1/100: 305it [06:46,  1.34s/it, loss=4.48]epoch 1/100: 305it [06:46,  1.34s/it, loss=4.67]epoch 1/100: 305it [06:46,  1.34s/it, loss=4.72]epoch 1/100: 305it [06:46,  1.34s/it, loss=4.44]epoch 1/100: 306it [06:46,  1.05s/it, loss=4.44]epoch 1/100: 306it [06:46,  1.05s/it, loss=4.72]epoch 1/100: 306it [06:47,  1.05s/it, loss=4.51]epoch 1/100: 306it [06:47,  1.05s/it, loss=4.71]epoch 1/100: 307it [06:47,  1.17it/s, loss=4.51]epoch 1/100: 307it [06:47,  1.17it/s, loss=4.71]epoch 1/100: 307it [06:47,  1.17it/s, loss=4.74]epoch 1/100: 307it [06:47,  1.17it/s, loss=4.54]epoch 1/100: 308it [06:47,  1.41it/s, loss=4.54]epoch 1/100: 308it [06:47,  1.41it/s, loss=4.74]epoch 1/100: 308it [06:48,  1.41it/s, loss=4.71]epoch 1/100: 308it [06:48,  1.41it/s, loss=4.5] epoch 1/100: 309it [06:48,  1.63it/s, loss=4.71]epoch 1/100: 309it [06:48,  1.63it/s, loss=4.5]epoch 1/100: 309it [06:48,  1.63it/s, loss=4.5] epoch 1/100: 309it [06:48,  1.63it/s, loss=4.49]epoch 1/100: 310it [06:48,  1.83it/s, loss=4.5]epoch 1/100: 310it [06:48,  1.83it/s, loss=4.49]epoch 1/100: 310it [06:48,  1.83it/s, loss=4.54]epoch 1/100: 310it [06:48,  1.83it/s, loss=4.59]epoch 1/100: 311it [06:48,  2.01it/s, loss=4.54]epoch 1/100: 311it [06:48,  2.01it/s, loss=4.59]epoch 1/100: 311it [06:49,  2.01it/s, loss=4.53]epoch 1/100: 311it [06:49,  2.01it/s, loss=4.34]epoch 1/100: 312it [06:49,  2.16it/s, loss=4.34]epoch 1/100: 312it [06:49,  2.16it/s, loss=4.53]epoch 1/100: 312it [06:49,  2.16it/s, loss=4.55]epoch 1/100: 312it [06:49,  2.16it/s, loss=4.79]epoch 1/100: 313it [06:49,  2.27it/s, loss=4.55]epoch 1/100: 313it [06:49,  2.27it/s, loss=4.79]epoch 1/100: 313it [06:50,  2.27it/s, loss=4.63]epoch 1/100: 313it [06:50,  2.27it/s, loss=4.39]epoch 1/100: 314it [06:50,  2.36it/s, loss=4.63]epoch 1/100: 314it [06:50,  2.36it/s, loss=4.39]epoch 1/100: 314it [06:50,  2.36it/s, loss=4.59]epoch 1/100: 314it [06:50,  2.36it/s, loss=4.46]epoch 1/100: 315it [06:50,  2.43it/s, loss=4.59]epoch 1/100: 315it [06:50,  2.43it/s, loss=4.46]epoch 1/100: 315it [06:50,  2.43it/s, loss=4.57]epoch 1/100: 315it [06:50,  2.43it/s, loss=4.44]epoch 1/100: 316it [06:50,  2.48it/s, loss=4.57]epoch 1/100: 316it [06:50,  2.48it/s, loss=4.44]epoch 1/100: 316it [06:51,  2.48it/s, loss=4.37]epoch 1/100: 316it [06:51,  2.48it/s, loss=4.63]epoch 1/100: 317it [06:51,  2.51it/s, loss=4.63]epoch 1/100: 317it [06:51,  2.51it/s, loss=4.37]epoch 1/100: 317it [06:51,  2.51it/s, loss=4.66]epoch 1/100: 317it [06:51,  2.51it/s, loss=4.5] epoch 1/100: 318it [06:51,  2.54it/s, loss=4.66]epoch 1/100: 318it [06:51,  2.54it/s, loss=4.5]epoch 1/100: 318it [06:51,  2.54it/s, loss=4.67]epoch 1/100: 318it [06:51,  2.54it/s, loss=4.4] epoch 1/100: 319it [06:51,  2.55it/s, loss=4.67]epoch 1/100: 319it [06:51,  2.55it/s, loss=4.4]epoch 1/100: 319it [06:52,  2.55it/s, loss=4.59]epoch 1/100: 319it [06:52,  2.55it/s, loss=4.48]epoch 1/100: 320it [06:52,  2.56it/s, loss=4.59]epoch 1/100: 320it [06:52,  2.56it/s, loss=4.48]epoch 1/100: 320it [06:52,  2.56it/s, loss=4.51]epoch 1/100: 320it [06:52,  2.56it/s, loss=4.66]epoch 1/100: 321it [06:52,  2.57it/s, loss=4.51]epoch 1/100: 321it [06:52,  2.57it/s, loss=4.66]epoch 1/100: 321it [06:53,  2.57it/s, loss=4.45]epoch 1/100: 321it [06:53,  2.57it/s, loss=4.58]epoch 1/100: 322it [06:53,  2.58it/s, loss=4.45]epoch 1/100: 322it [06:53,  2.58it/s, loss=4.58]epoch 1/100: 322it [06:53,  2.58it/s, loss=4.57]epoch 1/100: 322it [06:53,  2.58it/s, loss=4.47]epoch 1/100: 323it [06:53,  2.59it/s, loss=4.47]epoch 1/100: 323it [06:53,  2.59it/s, loss=4.57]epoch 1/100: 323it [06:53,  2.59it/s, loss=4.5] epoch 1/100: 323it [06:53,  2.59it/s, loss=4.32]epoch 1/100: 324it [06:53,  2.59it/s, loss=4.5]epoch 1/100: 324it [06:53,  2.59it/s, loss=4.32]epoch 1/100: 324it [06:54,  2.59it/s, loss=4.54]epoch 1/100: 324it [06:54,  2.59it/s, loss=4.58]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:04,  4.63s/it][A
val: 1it [00:04,  4.63s/it][A
val: 2it [00:05,  2.18s/it][A
val: 2it [00:05,  2.30s/it][A
val: 3it [00:05,  1.30s/it][A
val: 3it [00:05,  1.36s/it][A
val: 4it [00:05,  1.13it/s][A
val: 4it [00:05,  1.08it/s][A
val: 5it [00:05,  1.52it/s][A
val: 5it [00:06,  1.46it/s][A
val: 6it [00:06,  1.92it/s][A
val: 6it [00:06,  1.86it/s][A
val: 7it [00:06,  2.31it/s][A
val: 7it [00:06,  2.26it/s][A
val: 8it [00:06,  2.66it/s][A
val: 8it [00:06,  2.62it/s][A
val: 9it [00:06,  2.96it/s][A
val: 9it [00:07,  2.93it/s][A
val: 10it [00:07,  3.21it/s][A
val: 10it [00:07,  3.19it/s][A
val: 11it [00:07,  3.41it/s][A
val: 11it [00:07,  3.40it/s][A
val: 12it [00:07,  3.56it/s][A
val: 12it [00:07,  3.55it/s][A
val: 13it [00:07,  3.68it/s][A
val: 13it [00:08,  3.67it/s][A
val: 14it [00:08,  3.75it/s][A
val: 14it [00:08,  3.76it/s][A
val: 15it [00:08,  3.81it/s][A
val: 15it [00:08,  3.82it/s][A
val: 16it [00:08,  3.86it/s][A
val: 16it [00:08,  3.86it/s][A
val: 17it [00:08,  3.89it/s][A
val: 17it [00:09,  3.90it/s][A
val: 18it [00:09,  3.92it/s][A
val: 18it [00:09,  3.92it/s][A
val: 19it [00:09,  3.93it/s][A
val: 19it [00:09,  3.93it/s][A
val: 20it [00:09,  3.95it/s][A
val: 20it [00:09,  3.94it/s][A
val: 21it [00:09,  3.95it/s][A
val: 21it [00:10,  3.96it/s][A
val: 22it [00:10,  3.96it/s][A
val: 22it [00:10,  3.96it/s][A
val: 23it [00:10,  3.96it/s][A
val: 23it [00:10,  3.97it/s][A
val: 24it [00:10,  3.97it/s][A
                            [A
val: 24it [00:10,  3.97it/s][A
                            [Aepoch 1/100: 324it [07:05,  2.59it/s, loss=4.58]epoch 1/100: 324it [07:14,  2.59it/s, best_val=4.47, loss=4.54, val=4.47]epoch 1/100: 325it [07:14,  6.45s/it, loss=4.58]epoch 1/100: 325it [07:14,  6.45s/it, best_val=4.47, loss=4.54, val=4.47]epoch 1/100: 325it [07:14,  6.45s/it, loss=4.66]epoch 1/100: 325it [07:14,  6.45s/it, loss=4.4]                          epoch 1/100: 326it [07:14,  4.64s/it, loss=4.66]epoch 1/100: 326it [07:14,  4.64s/it, loss=4.4]epoch 1/100: 326it [07:15,  4.64s/it, loss=4.64]epoch 1/100: 326it [07:15,  4.64s/it, loss=4.58]epoch 1/100: 327it [07:15,  3.36s/it, loss=4.64]epoch 1/100: 327it [07:15,  3.36s/it, loss=4.58]epoch 1/100: 327it [07:15,  3.36s/it, loss=4.5] epoch 1/100: 327it [07:15,  3.36s/it, loss=4.61]epoch 1/100: 328it [07:15,  2.47s/it, loss=4.5]epoch 1/100: 328it [07:15,  2.47s/it, loss=4.61]epoch 1/100: 328it [07:16,  2.47s/it, loss=4.47]epoch 1/100: 328it [07:16,  2.47s/it, loss=4.68]epoch 1/100: 329it [07:16,  1.84s/it, loss=4.47]epoch 1/100: 329it [07:16,  1.84s/it, loss=4.68]epoch 1/100: 329it [07:16,  1.84s/it, loss=4.54]epoch 1/100: 329it [07:16,  1.84s/it, loss=4.41]epoch 1/100: 330it [07:16,  1.41s/it, loss=4.54]epoch 1/100: 330it [07:16,  1.41s/it, loss=4.41]epoch 1/100: 330it [07:16,  1.41s/it, loss=4.58]epoch 1/100: 330it [07:16,  1.41s/it, loss=4.46]epoch 1/100: 331it [07:16,  1.10s/it, loss=4.58]epoch 1/100: 331it [07:16,  1.10s/it, loss=4.46]epoch 1/100: 331it [07:17,  1.10s/it, loss=4.5] epoch 1/100: 331it [07:17,  1.10s/it, loss=4.45]epoch 1/100: 332it [07:17,  1.13it/s, loss=4.5]epoch 1/100: 332it [07:17,  1.13it/s, loss=4.45]epoch 1/100: 332it [07:17,  1.13it/s, loss=4.52]epoch 1/100: 332it [07:17,  1.13it/s, loss=4.49]epoch 1/100: 333it [07:17,  1.36it/s, loss=4.52]epoch 1/100: 333it [07:17,  1.36it/s, loss=4.49]epoch 1/100: 333it [07:18,  1.36it/s, loss=4.53]epoch 1/100: 333it [07:17,  1.36it/s, loss=4.62]epoch 1/100: 334it [07:18,  1.59it/s, loss=4.53]epoch 1/100: 334it [07:17,  1.59it/s, loss=4.62]epoch 1/100: 334it [07:18,  1.59it/s, loss=4.61]epoch 1/100: 334it [07:18,  1.59it/s, loss=4.57]epoch 1/100: 335it [07:18,  1.80it/s, loss=4.57]epoch 1/100: 335it [07:18,  1.80it/s, loss=4.61]epoch 1/100: 335it [07:18,  1.80it/s, loss=4.34]epoch 1/100: 335it [07:18,  1.80it/s, loss=4.41]epoch 1/100: 336it [07:18,  1.98it/s, loss=4.34]epoch 1/100: 336it [07:18,  1.98it/s, loss=4.41]epoch 1/100: 336it [07:19,  1.98it/s, loss=4.53]epoch 1/100: 336it [07:19,  1.98it/s, loss=4.47]epoch 1/100: 337it [07:19,  2.13it/s, loss=4.53]epoch 1/100: 337it [07:19,  2.13it/s, loss=4.47]epoch 1/100: 337it [07:19,  2.13it/s, loss=4.43]epoch 1/100: 337it [07:19,  2.13it/s, loss=4.46]epoch 1/100: 338it [07:19,  2.25it/s, loss=4.43]epoch 1/100: 338it [07:19,  2.25it/s, loss=4.46]epoch 1/100: 338it [07:19,  2.25it/s, loss=4.34]epoch 1/100: 338it [07:19,  2.25it/s, loss=4.68]epoch 1/100: 339it [07:19,  2.34it/s, loss=4.34]epoch 1/100: 339it [07:19,  2.34it/s, loss=4.68]epoch 1/100: 339it [07:20,  2.34it/s, loss=4.58]epoch 1/100: 339it [07:20,  2.34it/s, loss=4.49]epoch 1/100: 340it [07:20,  2.41it/s, loss=4.58]epoch 1/100: 340it [07:20,  2.41it/s, loss=4.49]epoch 1/100: 340it [07:20,  2.41it/s, loss=4.53]epoch 1/100: 340it [07:20,  2.41it/s, loss=4.7] epoch 1/100: 341it [07:20,  2.46it/s, loss=4.53]epoch 1/100: 341it [07:20,  2.46it/s, loss=4.7]epoch 1/100: 341it [07:21,  2.46it/s, loss=4.41]epoch 1/100: 341it [07:21,  2.46it/s, loss=4.37]epoch 1/100: 342it [07:21,  2.50it/s, loss=4.41]epoch 1/100: 342it [07:21,  2.50it/s, loss=4.37]epoch 1/100: 342it [07:21,  2.50it/s, loss=4.22]epoch 1/100: 342it [07:21,  2.50it/s, loss=4.54]epoch 1/100: 343it [07:21,  2.53it/s, loss=4.22]epoch 1/100: 343it [07:21,  2.53it/s, loss=4.54]epoch 1/100: 343it [07:21,  2.53it/s, loss=4.42]epoch 1/100: 343it [07:21,  2.53it/s, loss=4.48]epoch 1/100: 344it [07:21,  2.55it/s, loss=4.42]epoch 1/100: 344it [07:21,  2.55it/s, loss=4.48]epoch 1/100: 344it [07:22,  2.55it/s, loss=4.59]epoch 1/100: 344it [07:22,  2.55it/s, loss=4.66]epoch 1/100: 345it [07:22,  2.56it/s, loss=4.59]epoch 1/100: 345it [07:22,  2.56it/s, loss=4.66]epoch 1/100: 345it [07:22,  2.56it/s, loss=4.4] epoch 1/100: 345it [07:22,  2.56it/s, loss=4.46]epoch 1/100: 346it [07:22,  2.57it/s, loss=4.46]epoch 1/100: 346it [07:22,  2.57it/s, loss=4.4]epoch 1/100: 346it [07:23,  2.57it/s, loss=4.33]epoch 1/100: 346it [07:23,  2.57it/s, loss=4.4] epoch 1/100: 347it [07:23,  2.58it/s, loss=4.33]epoch 1/100: 347it [07:23,  2.58it/s, loss=4.4]epoch 1/100: 347it [07:23,  2.58it/s, loss=4.73]epoch 1/100: 347it [07:23,  2.58it/s, loss=4.44]epoch 1/100: 348it [07:23,  2.58it/s, loss=4.44]epoch 1/100: 348it [07:23,  2.58it/s, loss=4.73]epoch 1/100: 348it [07:23,  2.58it/s, loss=4.47]epoch 1/100: 348it [07:23,  2.58it/s, loss=4.53]epoch 1/100: 349it [07:23,  2.59it/s, loss=4.53]epoch 1/100: 349it [07:23,  2.59it/s, loss=4.47]epoch 1/100: 349it [07:24,  2.59it/s, loss=4.48]epoch 1/100: 349it [07:24,  2.59it/s, loss=4.4] 
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.07s/it][A
val: 2it [00:03,  1.43s/it][A
val: 3it [00:03,  1.12it/s][A
val: 4it [00:03,  1.56it/s][A
val: 5it [00:04,  2.00it/s][A
val: 6it [00:04,  2.41it/s][A
val: 7it [00:04,  2.76it/s][A
val: 1it [00:04,  4.63s/it][A
val: 8it [00:04,  3.06it/s][A
val: 2it [00:04,  2.06s/it][A
val: 9it [00:05,  3.30it/s][A
val: 3it [00:05,  1.23s/it][A
val: 10it [00:05,  3.48it/s][A
val: 4it [00:05,  1.18it/s][A
val: 11it [00:05,  3.62it/s][A
val: 5it [00:05,  1.58it/s][A
val: 12it [00:05,  3.72it/s][A
val: 6it [00:05,  1.99it/s][A
val: 13it [00:06,  3.79it/s][A
val: 7it [00:06,  2.38it/s][A
val: 14it [00:06,  3.84it/s][A
val: 8it [00:06,  2.72it/s][A
val: 15it [00:06,  3.88it/s][A
val: 9it [00:06,  3.02it/s][A
val: 16it [00:06,  3.91it/s][A
val: 10it [00:06,  3.26it/s][A
val: 17it [00:07,  3.93it/s][A
val: 11it [00:07,  3.44it/s][A
val: 18it [00:07,  3.94it/s][A
val: 12it [00:07,  3.59it/s][A
val: 19it [00:07,  3.96it/s][A
val: 13it [00:07,  3.69it/s][A
val: 20it [00:07,  3.96it/s][A
val: 14it [00:07,  3.77it/s][A
val: 21it [00:08,  3.97it/s][A
val: 15it [00:08,  3.82it/s][A
val: 22it [00:08,  3.98it/s][A
val: 16it [00:08,  3.86it/s][A
val: 23it [00:08,  3.98it/s][A
val: 17it [00:08,  3.90it/s][A
val: 24it [00:08,  3.98it/s][A
val: 18it [00:08,  3.92it/s][A
                            [A
val: 19it [00:09,  3.94it/s][A
val: 20it [00:09,  3.95it/s][A
val: 21it [00:09,  3.96it/s][A
val: 22it [00:09,  3.97it/s][A
val: 23it [00:10,  3.97it/s][A
val: 24it [00:10,  3.97it/s][A
                            [Aepoch 1/100: 349it [07:34,  2.59it/s, loss=4.48]epoch 1/100: 349it [07:44,  2.59it/s, best_val=4.44, loss=4.4, val=4.44]epoch 1/100: 350it [07:44,  6.39s/it, loss=4.48]epoch 1/100: 350it [07:44,  6.39s/it, best_val=4.44, loss=4.4, val=4.44]epoch 1/100: 350it [07:44,  6.39s/it, loss=4.53]                        epoch 1/100: 350it [07:44,  6.39s/it, loss=4.5] epoch 1/100: 351it [07:44,  4.59s/it, loss=4.5]epoch 1/100: 351it [07:44,  4.59s/it, loss=4.53]epoch 1/100: 351it [07:44,  4.59s/it, loss=4.32]epoch 1/100: 351it [07:44,  4.59s/it, loss=4.32]epoch 1/100: 352it [07:44,  3.33s/it, loss=4.32]epoch 1/100: 352it [07:44,  3.33s/it, loss=4.32]epoch 1/100: 352it [07:45,  3.33s/it, loss=4.63]epoch 1/100: 352it [07:45,  3.33s/it, loss=4.37]epoch 1/100: 353it [07:45,  2.45s/it, loss=4.37]epoch 1/100: 353it [07:45,  2.45s/it, loss=4.63]epoch 1/100: 353it [07:45,  2.45s/it, loss=4.42]epoch 1/100: 353it [07:45,  2.45s/it, loss=4.49]epoch 1/100: 354it [07:45,  1.83s/it, loss=4.49]epoch 1/100: 354it [07:45,  1.83s/it, loss=4.42]epoch 1/100: 354it [07:46,  1.83s/it, loss=4.33]epoch 1/100: 354it [07:46,  1.83s/it, loss=4.57]epoch 1/100: 355it [07:46,  1.40s/it, loss=4.33]epoch 1/100: 355it [07:46,  1.40s/it, loss=4.57]epoch 1/100: 355it [07:46,  1.40s/it, loss=4.6] epoch 1/100: 355it [07:46,  1.40s/it, loss=4.43]epoch 1/100: 356it [07:46,  1.09s/it, loss=4.6]epoch 1/100: 356it [07:46,  1.09s/it, loss=4.43]epoch 1/100: 356it [07:46,  1.09s/it, loss=4.3]epoch 1/100: 356it [07:46,  1.09s/it, loss=4.33]epoch 1/100: 357it [07:46,  1.14it/s, loss=4.3]epoch 1/100: 357it [07:46,  1.14it/s, loss=4.33]epoch 1/100: 357it [07:47,  1.14it/s, loss=4.3]epoch 1/100: 357it [07:47,  1.14it/s, loss=4.53]epoch 1/100: 358it [07:47,  1.37it/s, loss=4.3]epoch 1/100: 358it [07:47,  1.37it/s, loss=4.53]epoch 1/100: 358it [07:47,  1.37it/s, loss=4.59]epoch 1/100: 358it [07:47,  1.37it/s, loss=4.66]epoch 1/100: 359it [07:47,  1.59it/s, loss=4.66]epoch 1/100: 359it [07:47,  1.59it/s, loss=4.59]epoch 1/100: 359it [07:48,  1.59it/s, loss=4.48]epoch 1/100: 359it [07:48,  1.59it/s, loss=4.59]epoch 1/100: 360it [07:48,  1.80it/s, loss=4.48]epoch 1/100: 360it [07:48,  1.80it/s, loss=4.59]epoch 1/100: 360it [07:48,  1.80it/s, loss=4.4] epoch 1/100: 360it [07:48,  1.80it/s, loss=4.41]epoch 1/100: 361it [07:48,  1.98it/s, loss=4.41]epoch 1/100: 361it [07:48,  1.98it/s, loss=4.4]epoch 1/100: 361it [07:48,  1.98it/s, loss=4.23]epoch 1/100: 361it [07:48,  1.98it/s, loss=4.54]epoch 1/100: 362it [07:48,  2.13it/s, loss=4.54]epoch 1/100: 362it [07:48,  2.13it/s, loss=4.23]epoch 1/100: 362it [07:49,  2.13it/s, loss=4.49]epoch 1/100: 362it [07:49,  2.13it/s, loss=4.5] epoch 1/100: 363it [07:49,  2.25it/s, loss=4.49]epoch 1/100: 363it [07:49,  2.25it/s, loss=4.5]epoch 1/100: 363it [07:49,  2.25it/s, loss=4.44]epoch 1/100: 363it [07:49,  2.25it/s, loss=4.48]epoch 1/100: 364it [07:49,  2.35it/s, loss=4.44]epoch 1/100: 364it [07:49,  2.35it/s, loss=4.48]epoch 1/100: 364it [07:49,  2.35it/s, loss=4.44]epoch 1/100: 364it [07:49,  2.35it/s, loss=4.53]epoch 1/100: 365it [07:49,  2.42it/s, loss=4.53]epoch 1/100: 365it [07:49,  2.42it/s, loss=4.44]epoch 1/100: 365it [07:50,  2.42it/s, loss=4.47]epoch 1/100: 365it [07:50,  2.42it/s, loss=4.48]epoch 1/100: 366it [07:50,  2.47it/s, loss=4.47]epoch 1/100: 366it [07:50,  2.47it/s, loss=4.48]epoch 1/100: 366it [07:50,  2.47it/s, loss=4.44]epoch 1/100: 366it [07:50,  2.47it/s, loss=4.3] epoch 1/100: 367it [07:50,  2.50it/s, loss=4.3]epoch 1/100: 367it [07:50,  2.50it/s, loss=4.44]epoch 1/100: 367it [07:51,  2.50it/s, loss=4.06]epoch 1/100: 367it [07:51,  2.50it/s, loss=4.5] epoch 1/100: 368it [07:51,  2.53it/s, loss=4.06]epoch 1/100: 368it [07:51,  2.53it/s, loss=4.5]epoch 1/100: 368it [07:51,  2.53it/s, loss=4.52]epoch 1/100: 368it [07:51,  2.53it/s, loss=4.55]epoch 1/100: 369it [07:51,  2.55it/s, loss=4.52]epoch 1/100: 369it [07:51,  2.55it/s, loss=4.55]epoch 1/100: 369it [07:51,  2.55it/s, loss=4.41]epoch 1/100: 369it [07:51,  2.55it/s, loss=4.57]epoch 1/100: 370it [07:51,  2.57it/s, loss=4.57]epoch 1/100: 370it [07:51,  2.57it/s, loss=4.41]epoch 1/100: 370it [07:52,  2.57it/s, loss=4.5] epoch 1/100: 370it [07:52,  2.57it/s, loss=4.58]epoch 1/100: 371it [07:52,  2.58it/s, loss=4.5]epoch 1/100: 371it [07:52,  2.58it/s, loss=4.58]epoch 1/100: 371it [07:52,  2.58it/s, loss=4.29]epoch 1/100: 371it [07:52,  2.58it/s, loss=4.46]epoch 1/100: 372it [07:52,  2.58it/s, loss=4.29]epoch 1/100: 372it [07:52,  2.58it/s, loss=4.46]epoch 1/100: 372it [07:53,  2.58it/s, loss=4.5] epoch 1/100: 372it [07:53,  2.58it/s, loss=4.53]epoch 1/100: 373it [07:53,  2.59it/s, loss=4.53]epoch 1/100: 373it [07:53,  2.59it/s, loss=4.5]epoch 1/100: 373it [07:53,  2.59it/s, loss=4.51]epoch 1/100: 373it [07:53,  2.59it/s, loss=4.27]epoch 1/100: 374it [07:53,  2.59it/s, loss=4.51]epoch 1/100: 374it [07:53,  2.59it/s, loss=4.27]epoch 1/100: 374it [07:53,  2.59it/s, loss=4.57]epoch 1/100: 374it [07:53,  2.59it/s, loss=4.42]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:02,  3.00s/it][A
val: 1it [00:05,  5.13s/it][A
val: 2it [00:05,  2.26s/it][A
val: 3it [00:05,  1.34s/it][A
val: 4it [00:05,  1.10it/s][A
val: 5it [00:06,  1.48it/s][A
val: 6it [00:06,  1.88it/s][A
val: 7it [00:06,  2.28it/s][A
val: 8it [00:06,  2.63it/s][A
val: 9it [00:07,  2.94it/s][A
val: 10it [00:07,  3.20it/s][A
val: 11it [00:07,  3.40it/s][A
val: 12it [00:07,  3.55it/s][A
val: 13it [00:08,  3.67it/s][A
val: 14it [00:08,  3.75it/s][A
val: 15it [00:08,  3.81it/s][A
val: 16it [00:08,  3.86it/s][A
val: 17it [00:09,  3.89it/s][A
val: 18it [00:09,  3.92it/s][A
val: 19it [00:09,  3.93it/s][A
val: 20it [00:09,  3.95it/s][A
val: 21it [00:10,  3.96it/s][A
val: 22it [00:10,  3.97it/s][A
val: 23it [00:10,  3.97it/s][A
val: 24it [00:10,  3.98it/s][A
                            [A
val: 2it [00:21, 11.94s/it][A
val: 3it [00:21,  6.60s/it][A
val: 4it [00:21,  4.09s/it][A
val: 5it [00:21,  2.71s/it][A
val: 6it [00:22,  1.87s/it][A
val: 7it [00:22,  1.34s/it][A
val: 8it [00:22,  1.01it/s][A
val: 9it [00:22,  1.31it/s][A
val: 10it [00:23,  1.66it/s][A
val: 11it [00:23,  2.02it/s][A
val: 12it [00:23,  2.37it/s][A
val: 13it [00:23,  2.71it/s][A
val: 14it [00:24,  3.00it/s][A
val: 15it [00:24,  3.24it/s][A
val: 16it [00:24,  3.43it/s][A
val: 17it [00:24,  3.58it/s][A
val: 18it [00:25,  3.69it/s][A
val: 19it [00:25,  3.78it/s][A
val: 20it [00:25,  3.84it/s][A
val: 21it [00:25,  3.89it/s][A
val: 22it [00:26,  3.92it/s][A
val: 23it [00:26,  3.94it/s][A
val: 24it [00:26,  3.96it/s][A
                            [Aepoch 1/100: 374it [08:20,  2.59it/s, loss=4.57]epoch 1/100: 374it [08:29,  2.59it/s, best_val=4.42, loss=4.42, val=4.42]epoch 1/100: 375it [08:29, 11.14s/it, loss=4.57]epoch 1/100: 375it [08:29, 11.14s/it, best_val=4.42, loss=4.42, val=4.42]epoch 1/100: 375it [08:30, 11.14s/it, loss=4.32]epoch 1/100: 375it [08:30, 11.14s/it, loss=4.44]                         epoch 1/100: 376it [08:30,  7.92s/it, loss=4.32]epoch 1/100: 376it [08:30,  7.92s/it, loss=4.44]epoch 1/100: 376it [08:30,  7.92s/it, loss=4.54]epoch 1/100: 376it [08:30,  7.92s/it, loss=4.37]epoch 1/100: 377it [08:30,  5.66s/it, loss=4.54]epoch 1/100: 377it [08:30,  5.66s/it, loss=4.37]epoch 1/100: 377it [08:30,  5.66s/it, loss=4.3] epoch 1/100: 377it [08:30,  5.66s/it, loss=4.34]epoch 1/100: 378it [08:30,  4.08s/it, loss=4.3]epoch 1/100: 378it [08:30,  4.08s/it, loss=4.34]epoch 1/100: 378it [08:31,  4.08s/it, loss=4.19]epoch 1/100: 378it [08:31,  4.08s/it, loss=4.68]epoch 1/100: 379it [08:31,  2.97s/it, loss=4.19]epoch 1/100: 379it [08:31,  2.97s/it, loss=4.68]epoch 1/100: 379it [08:31,  2.97s/it, loss=4.4] epoch 1/100: 379it [08:31,  2.97s/it, loss=4.43]epoch 1/100: 380it [08:31,  2.19s/it, loss=4.4]epoch 1/100: 380it [08:31,  2.19s/it, loss=4.43]epoch 1/100: 380it [08:32,  2.19s/it, loss=4.38]epoch 1/100: 380it [08:31,  2.19s/it, loss=4.35]epoch 1/100: 381it [08:32,  1.65s/it, loss=4.38]epoch 1/100: 381it [08:31,  1.65s/it, loss=4.35]epoch 1/100: 381it [08:32,  1.65s/it, loss=4.34]epoch 1/100: 381it [08:32,  1.65s/it, loss=4.59]epoch 1/100: 382it [08:32,  1.27s/it, loss=4.34]epoch 1/100: 382it [08:32,  1.27s/it, loss=4.59]epoch 1/100: 382it [08:32,  1.27s/it, loss=4.59]epoch 1/100: 382it [08:32,  1.27s/it, loss=4.29]epoch 1/100: 383it [08:32,  1.01s/it, loss=4.59]epoch 1/100: 383it [08:32,  1.01s/it, loss=4.29]epoch 1/100: 383it [08:33,  1.01s/it, loss=4.37]epoch 1/100: 383it [08:33,  1.01s/it, loss=4.68]epoch 1/100: 384it [08:33,  1.22it/s, loss=4.37]epoch 1/100: 384it [08:33,  1.22it/s, loss=4.68]epoch 1/100: 384it [08:33,  1.22it/s, loss=4.44]epoch 1/100: 384it [08:33,  1.22it/s, loss=4.59]epoch 1/100: 385it [08:33,  1.45it/s, loss=4.44]epoch 1/100: 385it [08:33,  1.45it/s, loss=4.59]epoch 1/100: 385it [08:33,  1.45it/s, loss=4.3] epoch 1/100: 385it [08:33,  1.45it/s, loss=4.43]epoch 1/100: 386it [08:33,  1.67it/s, loss=4.43]epoch 1/100: 386it [08:33,  1.67it/s, loss=4.3]epoch 1/100: 386it [08:34,  1.67it/s, loss=4.53]epoch 1/100: 386it [08:34,  1.67it/s, loss=4.42]epoch 1/100: 387it [08:34,  1.87it/s, loss=4.53]epoch 1/100: 387it [08:34,  1.87it/s, loss=4.42]epoch 1/100: 387it [08:34,  1.87it/s, loss=4.47]epoch 1/100: 387it [08:34,  1.87it/s, loss=4.36]epoch 1/100: 388it [08:34,  2.04it/s, loss=4.47]epoch 1/100: 388it [08:34,  2.04it/s, loss=4.36]epoch 1/100: 388it [08:35,  2.04it/s, loss=4.44]epoch 1/100: 388it [08:35,  2.04it/s, loss=4.62]epoch 1/100: 389it [08:35,  2.18it/s, loss=4.44]epoch 1/100: 389it [08:35,  2.18it/s, loss=4.62]epoch 1/100: 389it [08:35,  2.18it/s, loss=4.41]epoch 1/100: 389it [08:35,  2.18it/s, loss=4.38]epoch 1/100: 390it [08:35,  2.29it/s, loss=4.41]epoch 1/100: 390it [08:35,  2.29it/s, loss=4.38]epoch 1/100: 390it [08:35,  2.29it/s, loss=4.4] epoch 1/100: 390it [08:35,  2.29it/s, loss=4.49]epoch 1/100: 391it [08:35,  2.38it/s, loss=4.4]epoch 1/100: 391it [08:35,  2.38it/s, loss=4.49]epoch 1/100: 391it [08:36,  2.38it/s, loss=4.43]epoch 1/100: 391it [08:36,  2.38it/s, loss=4.32]epoch 1/100: 392it [08:36,  2.44it/s, loss=4.43]epoch 1/100: 392it [08:36,  2.44it/s, loss=4.32]epoch 1/100: 392it [08:36,  2.44it/s, loss=4.37]epoch 1/100: 392it [08:36,  2.44it/s, loss=4.19]epoch 1/100: 393it [08:36,  2.48it/s, loss=4.37]epoch 1/100: 393it [08:36,  2.48it/s, loss=4.19]epoch 1/100: 393it [08:36,  2.48it/s, loss=4.4] epoch 1/100: 393it [08:37,  2.48it/s, loss=4.46]epoch 1/100: 394it [08:36,  2.52it/s, loss=4.4]epoch 1/100: 394it [08:37,  2.52it/s, loss=4.46]epoch 1/100: 394it [08:37,  2.52it/s, loss=4.35]epoch 1/100: 394it [08:37,  2.52it/s, loss=4.38]epoch 1/100: 395it [08:37,  2.54it/s, loss=4.38]epoch 1/100: 395it [08:37,  2.54it/s, loss=4.35]epoch 1/100: 395it [08:37,  2.54it/s, loss=4.23]epoch 1/100: 395it [08:37,  2.54it/s, loss=4.31]epoch 1/100: 396it [08:37,  2.56it/s, loss=4.23]epoch 1/100: 396it [08:37,  2.56it/s, loss=4.31]epoch 1/100: 396it [08:38,  2.56it/s, loss=4.36]epoch 1/100: 396it [08:38,  2.56it/s, loss=4.07]epoch 1/100: 397it [08:38,  2.57it/s, loss=4.07]epoch 1/100: 397it [08:38,  2.57it/s, loss=4.36]epoch 1/100: 397it [08:38,  2.57it/s, loss=4.32]epoch 1/100: 397it [08:38,  2.57it/s, loss=4.28]epoch 1/100: 398it [08:38,  2.58it/s, loss=4.32]epoch 1/100: 398it [08:38,  2.58it/s, loss=4.28]epoch 1/100: 398it [08:38,  2.58it/s, loss=4.46]epoch 1/100: 398it [08:38,  2.58it/s, loss=4.38]epoch 1/100: 399it [08:38,  2.58it/s, loss=4.46]epoch 1/100: 399it [08:38,  2.58it/s, loss=4.38]epoch 1/100: 399it [08:39,  2.58it/s, loss=4.63]epoch 1/100: 399it [08:39,  2.58it/s, loss=4.23]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:30, 30.57s/it][A
val: 1it [00:30, 30.58s/it][A
val: 2it [00:30, 12.74s/it][A
val: 2it [00:30, 12.74s/it][A
val: 3it [00:31,  7.03s/it][A
val: 3it [00:31,  7.03s/it][A
val: 4it [00:31,  4.36s/it][A
val: 4it [00:31,  4.36s/it][A

val: 5it [00:31,  2.88s/it][Aval: 5it [00:31,  2.88s/it][A
val: 6it [00:31,  1.98s/it][A
val: 6it [00:31,  1.98s/it][A
val: 7it [00:32,  1.42s/it][A
val: 7it [00:32,  1.42s/it][A
val: 8it [00:32,  1.04s/it][A
val: 8it [00:32,  1.04s/it][A
val: 9it [00:32,  1.26it/s][A
val: 9it [00:32,  1.26it/s][A
val: 10it [00:32,  1.59it/s][A
val: 10it [00:32,  1.59it/s][A
val: 11it [00:33,  1.95it/s][A
val: 11it [00:33,  1.95it/s][A
val: 12it [00:33,  2.31it/s][A
val: 12it [00:33,  2.31it/s][A
val: 13it [00:33,  2.65it/s][A
val: 13it [00:33,  2.65it/s][A
val: 14it [00:33,  2.96it/s][A
val: 14it [00:33,  2.95it/s][A
val: 15it [00:34,  3.21it/s][A
val: 15it [00:34,  3.20it/s][A
val: 16it [00:34,  3.41it/s][A
val: 16it [00:34,  3.41it/s][A
val: 17it [00:34,  3.57it/s][A
val: 17it [00:34,  3.56it/s][A
val: 18it [00:34,  3.68it/s][A
val: 18it [00:34,  3.68it/s][A
val: 19it [00:35,  3.77it/s][A
val: 19it [00:35,  3.77it/s][A
val: 20it [00:35,  3.84it/s][A
val: 20it [00:35,  3.83it/s][A
val: 21it [00:35,  3.89it/s][A
val: 21it [00:35,  3.88it/s][A
val: 22it [00:35,  3.92it/s][A
val: 22it [00:35,  3.91it/s][A
val: 23it [00:36,  3.94it/s][A
val: 23it [00:36,  3.94it/s][A
val: 24it [00:36,  3.96it/s][A
val: 24it [00:36,  3.95it/s][A
                            [A
                            [Aepoch 1/100: 399it [09:15,  2.58it/s, loss=4.63]epoch 1/100: 399it [09:25,  2.58it/s, best_val=4.38, loss=4.23, val=4.38]epoch 1/100: 400it [09:25, 14.10s/it, loss=4.63]epoch 1/100: 400it [09:25, 14.10s/it, best_val=4.38, loss=4.23, val=4.38]epoch 1/100: 400it [09:25, 14.10s/it, loss=4.38]                         epoch 1/100: 400it [09:25, 14.10s/it, loss=4.43]epoch 1/100: 401it [09:25,  9.99s/it, loss=4.43]epoch 1/100: 401it [09:25,  9.99s/it, loss=4.38]epoch 1/100: 401it [09:25,  9.99s/it, loss=4.47]epoch 1/100: 401it [09:25,  9.99s/it, loss=4.46]epoch 1/100: 402it [09:25,  7.11s/it, loss=4.47]epoch 1/100: 402it [09:25,  7.11s/it, loss=4.46]epoch 1/100: 402it [09:26,  7.11s/it, loss=4.35]epoch 1/100: 402it [09:26,  7.11s/it, loss=4.56]epoch 1/100: 403it [09:26,  5.09s/it, loss=4.35]epoch 1/100: 403it [09:26,  5.09s/it, loss=4.56]epoch 1/100: 403it [09:26,  5.09s/it, loss=4.38]epoch 1/100: 403it [09:26,  5.09s/it, loss=4.36]epoch 1/100: 404it [09:26,  3.68s/it, loss=4.36]epoch 1/100: 404it [09:26,  3.68s/it, loss=4.38]epoch 1/100: 404it [09:26,  3.68s/it, loss=4.23]epoch 1/100: 404it [09:26,  3.68s/it, loss=4.21]epoch 1/100: 405it [09:26,  2.69s/it, loss=4.23]epoch 1/100: 405it [09:26,  2.69s/it, loss=4.21]epoch 1/100: 405it [09:27,  2.69s/it, loss=4.38]epoch 1/100: 405it [09:27,  2.69s/it, loss=4.37]epoch 1/100: 406it [09:27,  2.00s/it, loss=4.38]epoch 1/100: 406it [09:27,  2.00s/it, loss=4.37]epoch 1/100: 406it [09:27,  2.00s/it, loss=4.43]epoch 1/100: 406it [09:27,  2.00s/it, loss=4.29]epoch 1/100: 407it [09:27,  1.51s/it, loss=4.43]epoch 1/100: 407it [09:27,  1.51s/it, loss=4.29]epoch 1/100: 407it [09:28,  1.51s/it, loss=4.44]epoch 1/100: 407it [09:28,  1.51s/it, loss=4.36]epoch 1/100: 408it [09:28,  1.18s/it, loss=4.44]epoch 1/100: 408it [09:28,  1.18s/it, loss=4.36]epoch 1/100: 408it [09:28,  1.18s/it, loss=4.3] epoch 1/100: 408it [09:28,  1.18s/it, loss=4.55]epoch 1/100: 409it [09:28,  1.07it/s, loss=4.55]epoch 1/100: 409it [09:28,  1.07it/s, loss=4.3]epoch 1/100: 409it [09:28,  1.07it/s, loss=4.7] epoch 1/100: 409it [09:28,  1.07it/s, loss=4.32]epoch 1/100: 410it [09:28,  1.30it/s, loss=4.7]epoch 1/100: 410it [09:28,  1.30it/s, loss=4.32]epoch 1/100: 410it [09:29,  1.30it/s, loss=4.39]epoch 1/100: 410it [09:29,  1.30it/s, loss=4.26]epoch 1/100: 411it [09:29,  1.53it/s, loss=4.39]epoch 1/100: 411it [09:29,  1.53it/s, loss=4.26]epoch 1/100: 411it [09:29,  1.53it/s, loss=4.55]epoch 1/100: 411it [09:29,  1.53it/s, loss=4.36]epoch 1/100: 412it [09:29,  1.74it/s, loss=4.55]epoch 1/100: 412it [09:29,  1.74it/s, loss=4.36]epoch 1/100: 412it [09:30,  1.74it/s, loss=4.6] epoch 1/100: 412it [09:30,  1.74it/s, loss=4.45]epoch 1/100: 413it [09:30,  1.93it/s, loss=4.6]epoch 1/100: 413it [09:30,  1.93it/s, loss=4.45]epoch 1/100: 413it [09:30,  1.93it/s, loss=4.49]epoch 1/100: 413it [09:30,  1.93it/s, loss=4.41]epoch 1/100: 414it [09:30,  2.10it/s, loss=4.49]epoch 1/100: 414it [09:30,  2.10it/s, loss=4.41]epoch 1/100: 414it [09:30,  2.10it/s, loss=4.56]epoch 1/100: 414it [09:30,  2.10it/s, loss=4.47]epoch 1/100: 415it [09:30,  2.23it/s, loss=4.56]epoch 1/100: 415it [09:30,  2.23it/s, loss=4.47]epoch 1/100: 415it [09:31,  2.23it/s, loss=4.37]epoch 1/100: 415it [09:31,  2.23it/s, loss=4.36]epoch 1/100: 416it [09:31,  2.33it/s, loss=4.37]epoch 1/100: 416it [09:31,  2.33it/s, loss=4.36]epoch 1/100: 416it [09:31,  2.33it/s, loss=4.39]epoch 1/100: 416it [09:31,  2.33it/s, loss=4.5] epoch 1/100: 417it [09:31,  2.40it/s, loss=4.39]epoch 1/100: 417it [09:31,  2.40it/s, loss=4.5]epoch 1/100: 417it [09:31,  2.40it/s, loss=4.46]epoch 1/100: 417it [09:31,  2.40it/s, loss=4.43]epoch 1/100: 418it [09:31,  2.46it/s, loss=4.43]epoch 1/100: 418it [09:31,  2.46it/s, loss=4.46]epoch 1/100: 418it [09:32,  2.46it/s, loss=4.41]epoch 1/100: 418it [09:32,  2.46it/s, loss=4.38]epoch 1/100: 419it [09:32,  2.50it/s, loss=4.38]epoch 1/100: 419it [09:32,  2.50it/s, loss=4.41]epoch 1/100: 419it [09:32,  2.50it/s, loss=4.5] epoch 1/100: 419it [09:32,  2.50it/s, loss=4.24]epoch 1/100: 420it [09:32,  2.53it/s, loss=4.5]epoch 1/100: 420it [09:32,  2.53it/s, loss=4.24]epoch 1/100: 420it [09:33,  2.53it/s, loss=4.37]epoch 1/100: 420it [09:33,  2.53it/s, loss=4.46]epoch 1/100: 421it [09:33,  2.55it/s, loss=4.37]epoch 1/100: 421it [09:33,  2.55it/s, loss=4.46]epoch 1/100: 421it [09:33,  2.55it/s, loss=4.61]epoch 1/100: 421it [09:33,  2.55it/s, loss=4.42]epoch 1/100: 422it [09:33,  2.57it/s, loss=4.61]epoch 1/100: 422it [09:33,  2.57it/s, loss=4.42]epoch 1/100: 422it [09:33,  2.57it/s, loss=4.26]epoch 1/100: 422it [09:33,  2.57it/s, loss=4.4] epoch 1/100: 423it [09:33,  2.58it/s, loss=4.26]epoch 1/100: 423it [09:33,  2.58it/s, loss=4.4]epoch 1/100: 423it [09:34,  2.58it/s, loss=4.53]epoch 1/100: 423it [09:34,  2.58it/s, loss=4.5]epoch 1/100: 424it [09:34,  2.58it/s, loss=4.5]epoch 1/100: 424it [09:34,  2.58it/s, loss=4.53]epoch 1/100: 424it [09:34,  2.58it/s, loss=4.44]epoch 1/100: 424it [09:34,  2.58it/s, loss=4.39]

val: 0it [00:00, ?it/s][Aval: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.37s/it][A
val: 1it [00:03,  3.37s/it][A
val: 2it [00:03,  1.53s/it][A
val: 2it [00:03,  1.54s/it][A
val: 3it [00:03,  1.05it/s][A
val: 3it [00:03,  1.05it/s][A
val: 4it [00:04,  1.49it/s][A
val: 4it [00:04,  1.48it/s][A
val: 5it [00:04,  1.92it/s][A
val: 5it [00:04,  1.91it/s][A
val: 6it [00:04,  2.33it/s][A
val: 6it [00:04,  2.32it/s][A
val: 7it [00:04,  2.70it/s][A
val: 7it [00:04,  2.68it/s][A
val: 8it [00:05,  3.01it/s][A
val: 8it [00:05,  2.99it/s][A
val: 9it [00:05,  3.26it/s][A
val: 9it [00:05,  3.24it/s][A
val: 10it [00:05,  3.45it/s][A
val: 10it [00:05,  3.43it/s][A
val: 11it [00:05,  3.60it/s][A
val: 11it [00:05,  3.58it/s][A
val: 12it [00:06,  3.71it/s][A
val: 12it [00:06,  3.69it/s][A
val: 13it [00:06,  3.79it/s][A
val: 13it [00:06,  3.77it/s][A
val: 14it [00:06,  3.84it/s][A
val: 14it [00:06,  3.82it/s][A
val: 15it [00:06,  3.89it/s][A
val: 15it [00:06,  3.86it/s][A
val: 16it [00:07,  3.92it/s][A
val: 16it [00:07,  3.89it/s][A
val: 17it [00:07,  3.94it/s][A
val: 17it [00:07,  3.91it/s][A
val: 18it [00:07,  3.95it/s][A
val: 18it [00:07,  3.92it/s][A
val: 19it [00:07,  3.96it/s][A
val: 19it [00:07,  3.94it/s][A
val: 20it [00:08,  3.97it/s][A
val: 20it [00:08,  3.94it/s][A
val: 21it [00:08,  3.97it/s][A
val: 21it [00:08,  3.95it/s][A
val: 22it [00:08,  3.98it/s][A
val: 22it [00:08,  3.96it/s][A
val: 23it [00:08,  3.98it/s][A
val: 23it [00:08,  3.97it/s][A
val: 24it [00:09,  3.99it/s][A
val: 24it [00:09,  3.97it/s][A
                            [A
                            [Aepoch 1/100: 424it [09:43,  2.58it/s, loss=4.44]epoch 1/100: 424it [09:53,  2.58it/s, best_val=4.36, loss=4.39, val=4.36]epoch 1/100: 425it [09:53,  6.00s/it, loss=4.44]epoch 1/100: 425it [09:53,  6.00s/it, best_val=4.36, loss=4.39, val=4.36]epoch 1/100: 425it [09:53,  6.00s/it, loss=4.29]                         epoch 1/100: 425it [09:53,  6.00s/it, loss=4.34]epoch 1/100: 426it [09:53,  4.32s/it, loss=4.34]epoch 1/100: 426it [09:53,  4.32s/it, loss=4.29]epoch 1/100: 426it [09:54,  4.32s/it, loss=4.14]epoch 1/100: 426it [09:54,  4.32s/it, loss=4.33]epoch 1/100: 427it [09:54,  3.14s/it, loss=4.14]epoch 1/100: 427it [09:54,  3.14s/it, loss=4.33]epoch 1/100: 427it [09:54,  3.14s/it, loss=4.28]epoch 1/100: 427it [09:54,  3.14s/it, loss=4.24]epoch 1/100: 428it [09:54,  2.31s/it, loss=4.28]epoch 1/100: 428it [09:54,  2.31s/it, loss=4.24]epoch 1/100: 428it [09:54,  2.31s/it, loss=4.27]epoch 1/100: 428it [09:54,  2.31s/it, loss=4.37]epoch 1/100: 429it [09:54,  1.73s/it, loss=4.27]epoch 1/100: 429it [09:54,  1.73s/it, loss=4.37]epoch 1/100: 429it [09:55,  1.73s/it, loss=4.33]epoch 1/100: 429it [09:55,  1.73s/it, loss=4.47]epoch 1/100: 430it [09:55,  1.33s/it, loss=4.33]epoch 1/100: 430it [09:55,  1.33s/it, loss=4.47]epoch 1/100: 430it [09:55,  1.33s/it, loss=4.46]epoch 1/100: 430it [09:55,  1.33s/it, loss=4.34]epoch 1/100: 431it [09:55,  1.05s/it, loss=4.46]epoch 1/100: 431it [09:55,  1.05s/it, loss=4.34]epoch 1/100: 431it [09:56,  1.05s/it, loss=4.3] epoch 1/100: 431it [09:56,  1.05s/it, loss=4.52]epoch 1/100: 432it [09:56,  1.18it/s, loss=4.3]epoch 1/100: 432it [09:56,  1.18it/s, loss=4.52]epoch 1/100: 432it [09:56,  1.18it/s, loss=4.42]epoch 1/100: 432it [09:56,  1.18it/s, loss=4.34]epoch 1/100: 433it [09:56,  1.41it/s, loss=4.34]epoch 1/100: 433it [09:56,  1.41it/s, loss=4.42]epoch 1/100: 433it [09:56,  1.41it/s, loss=4.31]epoch 1/100: 433it [09:56,  1.41it/s, loss=4.48]epoch 1/100: 434it [09:56,  1.63it/s, loss=4.31]epoch 1/100: 434it [09:56,  1.63it/s, loss=4.48]epoch 1/100: 434it [09:57,  1.63it/s, loss=4.44]epoch 1/100: 434it [09:57,  1.63it/s, loss=4.38]epoch 1/100: 435it [09:57,  1.84it/s, loss=4.44]epoch 1/100: 435it [09:57,  1.84it/s, loss=4.38]epoch 1/100: 435it [09:57,  1.84it/s, loss=4.28]epoch 1/100: 435it [09:57,  1.84it/s, loss=4.16]epoch 1/100: 436it [09:57,  2.01it/s, loss=4.28]epoch 1/100: 436it [09:57,  2.01it/s, loss=4.16]epoch 1/100: 436it [09:58,  2.01it/s, loss=4.43]epoch 1/100: 436it [09:57,  2.01it/s, loss=4.26]epoch 1/100: 437it [09:58,  2.16it/s, loss=4.43]epoch 1/100: 437it [09:58,  2.16it/s, loss=4.26]epoch 1/100: 437it [09:58,  2.16it/s, loss=4.3] epoch 1/100: 437it [09:58,  2.16it/s, loss=4.4] epoch 1/100: 438it [09:58,  2.27it/s, loss=4.3]epoch 1/100: 438it [09:58,  2.27it/s, loss=4.4]epoch 1/100: 438it [09:58,  2.27it/s, loss=4.3]epoch 1/100: 438it [09:58,  2.27it/s, loss=4.37]epoch 1/100: 439it [09:58,  2.36it/s, loss=4.3]epoch 1/100: 439it [09:58,  2.36it/s, loss=4.37]epoch 1/100: 439it [09:59,  2.36it/s, loss=4.36]epoch 1/100: 439it [09:59,  2.36it/s, loss=4.46]epoch 1/100: 440it [09:59,  2.42it/s, loss=4.36]epoch 1/100: 440it [09:59,  2.42it/s, loss=4.46]epoch 1/100: 440it [09:59,  2.42it/s, loss=4.55]epoch 1/100: 440it [09:59,  2.42it/s, loss=4.2] epoch 1/100: 441it [09:59,  2.47it/s, loss=4.55]epoch 1/100: 441it [09:59,  2.47it/s, loss=4.2]epoch 1/100: 441it [09:59,  2.47it/s, loss=4.33]epoch 1/100: 441it [09:59,  2.47it/s, loss=4.41]epoch 1/100: 442it [09:59,  2.51it/s, loss=4.33]epoch 1/100: 442it [09:59,  2.51it/s, loss=4.41]epoch 1/100: 442it [10:00,  2.51it/s, loss=4.44]epoch 1/100: 442it [10:00,  2.51it/s, loss=4.4] epoch 1/100: 443it [10:00,  2.53it/s, loss=4.44]epoch 1/100: 443it [10:00,  2.53it/s, loss=4.4]epoch 1/100: 443it [10:00,  2.53it/s, loss=4.32]epoch 1/100: 443it [10:00,  2.53it/s, loss=4.41]epoch 1/100: 444it [10:00,  2.55it/s, loss=4.41]epoch 1/100: 444it [10:00,  2.55it/s, loss=4.32]epoch 1/100: 444it [10:01,  2.55it/s, loss=4.41]epoch 1/100: 444it [10:01,  2.55it/s, loss=4.49]epoch 1/100: 445it [10:01,  2.56it/s, loss=4.41]epoch 1/100: 445it [10:01,  2.56it/s, loss=4.49]epoch 1/100: 445it [10:01,  2.56it/s, loss=4.35]epoch 1/100: 445it [10:01,  2.56it/s, loss=4.45]epoch 1/100: 446it [10:01,  2.57it/s, loss=4.45]epoch 1/100: 446it [10:01,  2.57it/s, loss=4.35]epoch 1/100: 446it [10:01,  2.57it/s, loss=4.27]epoch 1/100: 446it [10:01,  2.57it/s, loss=4.5] epoch 1/100: 447it [10:01,  2.58it/s, loss=4.5]epoch 1/100: 447it [10:01,  2.58it/s, loss=4.27]epoch 1/100: 447it [10:02,  2.58it/s, loss=4.46]epoch 1/100: 447it [10:02,  2.58it/s, loss=4.07]epoch 1/100: 448it [10:02,  2.58it/s, loss=4.46]epoch 1/100: 448it [10:02,  2.58it/s, loss=4.07]epoch 1/100: 448it [10:02,  2.58it/s, loss=4.24]epoch 1/100: 448it [10:02,  2.58it/s, loss=4.22]epoch 1/100: 449it [10:02,  2.58it/s, loss=4.24]epoch 1/100: 449it [10:02,  2.58it/s, loss=4.22]epoch 1/100: 449it [10:03,  2.58it/s, loss=4.54]epoch 1/100: 449it [10:03,  2.58it/s, loss=4.12]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.51s/it][A
val: 2it [00:03,  1.59s/it][A
val: 1it [00:03,  3.88s/it][A
val: 3it [00:04,  1.02it/s][A
val: 2it [00:04,  1.75s/it][A
val: 4it [00:04,  1.44it/s][A
val: 3it [00:04,  1.06s/it][A
val: 5it [00:04,  1.88it/s][A
val: 4it [00:04,  1.35it/s][A
val: 6it [00:04,  2.28it/s][A
val: 5it [00:04,  1.77it/s][A
val: 7it [00:05,  2.66it/s][A
val: 6it [00:05,  2.18it/s][A
val: 8it [00:05,  2.97it/s][A
val: 7it [00:05,  2.55it/s][A
val: 9it [00:05,  3.22it/s][A
val: 8it [00:05,  2.88it/s][A
val: 10it [00:05,  3.42it/s][A
val: 9it [00:05,  3.15it/s][A
val: 11it [00:06,  3.58it/s][A
val: 10it [00:06,  3.36it/s][A
val: 12it [00:06,  3.69it/s][A
val: 11it [00:06,  3.52it/s][A
val: 13it [00:06,  3.77it/s][A
val: 12it [00:06,  3.64it/s][A
val: 14it [00:06,  3.84it/s][A
val: 13it [00:06,  3.73it/s][A
val: 15it [00:07,  3.87it/s][A
val: 14it [00:07,  3.80it/s][A
val: 16it [00:07,  3.90it/s][A
val: 15it [00:07,  3.84it/s][A
val: 17it [00:07,  3.91it/s][A
val: 16it [00:07,  3.87it/s][A
val: 18it [00:07,  3.93it/s][A
val: 17it [00:07,  3.90it/s][A
val: 19it [00:08,  3.94it/s][A
val: 18it [00:08,  3.92it/s][A
val: 20it [00:08,  3.95it/s][A
val: 19it [00:08,  3.93it/s][A
val: 21it [00:08,  3.96it/s][A
val: 20it [00:08,  3.94it/s][A
val: 22it [00:08,  3.96it/s][A
val: 21it [00:08,  3.95it/s][A
val: 23it [00:09,  3.97it/s][A
val: 22it [00:09,  3.96it/s][A
val: 24it [00:09,  3.98it/s][A
val: 23it [00:09,  3.96it/s][A
                            [A
val: 24it [00:09,  3.96it/s][A
                            [Aepoch 1/100: 449it [10:12,  2.58it/s, loss=4.12]epoch 1/100: 449it [10:21,  2.58it/s, best_val=4.33, loss=4.54, val=4.33]epoch 1/100: 450it [10:21,  6.04s/it, loss=4.12]epoch 1/100: 450it [10:21,  6.04s/it, best_val=4.33, loss=4.54, val=4.33]epoch 1/100: 450it [10:22,  6.04s/it, loss=4.49]                         epoch 1/100: 450it [10:22,  6.04s/it, loss=4.56]epoch 1/100: 451it [10:22,  4.35s/it, loss=4.56]epoch 1/100: 451it [10:22,  4.35s/it, loss=4.49]epoch 1/100: 451it [10:22,  4.35s/it, loss=4.27]epoch 1/100: 451it [10:22,  4.35s/it, loss=4.33]epoch 1/100: 452it [10:22,  3.16s/it, loss=4.27]epoch 1/100: 452it [10:22,  3.16s/it, loss=4.33]epoch 1/100: 452it [10:23,  3.16s/it, loss=4.33]epoch 1/100: 452it [10:23,  3.16s/it, loss=4.37]epoch 1/100: 453it [10:23,  2.33s/it, loss=4.33]epoch 1/100: 453it [10:23,  2.33s/it, loss=4.37]epoch 1/100: 453it [10:23,  2.33s/it, loss=4.28]epoch 1/100: 453it [10:23,  2.33s/it, loss=4.26]epoch 1/100: 454it [10:23,  1.74s/it, loss=4.28]epoch 1/100: 454it [10:23,  1.74s/it, loss=4.26]epoch 1/100: 454it [10:23,  1.74s/it, loss=4.18]epoch 1/100: 454it [10:23,  1.74s/it, loss=4.18]epoch 1/100: 455it [10:23,  1.34s/it, loss=4.18]epoch 1/100: 455it [10:23,  1.34s/it, loss=4.18]epoch 1/100: 455it [10:24,  1.34s/it, loss=4.29]epoch 1/100: 455it [10:24,  1.34s/it, loss=4.51]epoch 1/100: 456it [10:24,  1.05s/it, loss=4.29]epoch 1/100: 456it [10:24,  1.05s/it, loss=4.51]epoch 1/100: 456it [10:24,  1.05s/it, loss=4.33]epoch 1/100: 456it [10:24,  1.05s/it, loss=4.29]epoch 1/100: 457it [10:24,  1.17it/s, loss=4.33]epoch 1/100: 457it [10:24,  1.17it/s, loss=4.29]epoch 1/100: 457it [10:24,  1.17it/s, loss=4.55]epoch 1/100: 457it [10:24,  1.17it/s, loss=4.42]epoch 1/100: 458it [10:24,  1.41it/s, loss=4.55]epoch 1/100: 458it [10:24,  1.41it/s, loss=4.42]epoch 1/100: 458it [10:25,  1.41it/s, loss=4.33]epoch 1/100: 458it [10:25,  1.41it/s, loss=4.33]epoch 1/100: 459it [10:25,  1.63it/s, loss=4.33]epoch 1/100: 459it [10:25,  1.63it/s, loss=4.33]epoch 1/100: 459it [10:25,  1.63it/s, loss=4.18]epoch 1/100: 459it [10:25,  1.63it/s, loss=4.27]epoch 1/100: 460it [10:25,  1.83it/s, loss=4.27]epoch 1/100: 460it [10:25,  1.83it/s, loss=4.18]epoch 1/100: 460it [10:26,  1.83it/s, loss=4.28]epoch 1/100: 460it [10:26,  1.83it/s, loss=4.59]epoch 1/100: 461it [10:26,  2.01it/s, loss=4.28]epoch 1/100: 461it [10:26,  2.01it/s, loss=4.59]epoch 1/100: 461it [10:26,  2.01it/s, loss=4.45]epoch 1/100: 461it [10:26,  2.01it/s, loss=4.47]epoch 1/100: 462it [10:26,  2.16it/s, loss=4.45]epoch 1/100: 462it [10:26,  2.16it/s, loss=4.47]epoch 1/100: 462it [10:26,  2.16it/s, loss=4.29]epoch 1/100: 462it [10:26,  2.16it/s, loss=4.41]epoch 1/100: 463it [10:26,  2.27it/s, loss=4.29]epoch 1/100: 463it [10:26,  2.27it/s, loss=4.41]epoch 1/100: 463it [10:27,  2.27it/s, loss=4.25]epoch 1/100: 463it [10:27,  2.27it/s, loss=4.56]epoch 1/100: 464it [10:27,  2.36it/s, loss=4.25]epoch 1/100: 464it [10:27,  2.36it/s, loss=4.56]epoch 1/100: 464it [10:27,  2.36it/s, loss=4.22]epoch 1/100: 464it [10:27,  2.36it/s, loss=4.38]epoch 1/100: 465it [10:27,  2.43it/s, loss=4.22]epoch 1/100: 465it [10:27,  2.43it/s, loss=4.38]epoch 1/100: 465it [10:28,  2.43it/s, loss=4.32]epoch 1/100: 465it [10:28,  2.43it/s, loss=4.34]epoch 1/100: 466it [10:28,  2.47it/s, loss=4.32]epoch 1/100: 466it [10:28,  2.47it/s, loss=4.34]epoch 1/100: 466it [10:28,  2.47it/s, loss=4.33]epoch 1/100: 466it [10:28,  2.47it/s, loss=4.19]epoch 1/100: 467it [10:28,  2.51it/s, loss=4.33]epoch 1/100: 467it [10:28,  2.51it/s, loss=4.19]epoch 1/100: 467it [10:28,  2.51it/s, loss=4.1] epoch 1/100: 467it [10:28,  2.51it/s, loss=4.45]epoch 1/100: 468it [10:28,  2.53it/s, loss=4.1]epoch 1/100: 468it [10:28,  2.53it/s, loss=4.45]epoch 1/100: 468it [10:29,  2.53it/s, loss=4.41]epoch 1/100: 468it [10:29,  2.53it/s, loss=4.56]epoch 1/100: 469it [10:29,  2.55it/s, loss=4.41]epoch 1/100: 469it [10:29,  2.55it/s, loss=4.56]epoch 1/100: 469it [10:29,  2.55it/s, loss=4.34]epoch 1/100: 469it [10:29,  2.55it/s, loss=4.37]epoch 1/100: 470it [10:29,  2.56it/s, loss=4.34]epoch 1/100: 470it [10:29,  2.56it/s, loss=4.37]epoch 1/100: 470it [10:29,  2.56it/s, loss=4.37]epoch 1/100: 470it [10:29,  2.56it/s, loss=4.25]epoch 1/100: 471it [10:29,  2.57it/s, loss=4.37]epoch 1/100: 471it [10:29,  2.57it/s, loss=4.25]epoch 1/100: 471it [10:30,  2.57it/s, loss=4.55]epoch 1/100: 471it [10:30,  2.57it/s, loss=4.16]epoch 1/100: 472it [10:30,  2.57it/s, loss=4.16]epoch 1/100: 472it [10:30,  2.57it/s, loss=4.55]epoch 1/100: 472it [10:30,  2.57it/s, loss=4.3] epoch 1/100: 472it [10:30,  2.57it/s, loss=4.19]epoch 1/100: 473it [10:30,  2.58it/s, loss=4.3]epoch 1/100: 473it [10:30,  2.58it/s, loss=4.19]epoch 1/100: 473it [10:31,  2.58it/s, loss=4.28]epoch 1/100: 473it [10:31,  2.58it/s, loss=4.26]epoch 1/100: 474it [10:31,  2.58it/s, loss=4.28]epoch 1/100: 474it [10:31,  2.58it/s, loss=4.26]epoch 1/100: 474it [10:31,  2.58it/s, loss=4.38]epoch 1/100: 474it [10:31,  2.58it/s, loss=4.47]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.46s/it][A
val: 1it [00:03,  3.46s/it][A
val: 2it [00:03,  1.57s/it][A
val: 2it [00:03,  1.57s/it][A
val: 3it [00:03,  1.03it/s][A
val: 3it [00:03,  1.03it/s][A
val: 4it [00:04,  1.46it/s][A
val: 4it [00:04,  1.46it/s][A
val: 5it [00:04,  1.89it/s][A
val: 5it [00:04,  1.89it/s][A
val: 6it [00:04,  2.30it/s][A
val: 6it [00:04,  2.29it/s][A
val: 7it [00:04,  2.67it/s][A
val: 7it [00:04,  2.66it/s][A
val: 8it [00:05,  2.98it/s][A
val: 8it [00:05,  2.97it/s][A
val: 9it [00:05,  3.23it/s][A
val: 9it [00:05,  3.23it/s][A
val: 10it [00:05,  3.43it/s][A
val: 10it [00:05,  3.42it/s][A
val: 11it [00:05,  3.58it/s][A
val: 11it [00:05,  3.57it/s][A
val: 12it [00:06,  3.70it/s][A
val: 12it [00:06,  3.68it/s][A
val: 13it [00:06,  3.78it/s][A
val: 13it [00:06,  3.76it/s][A
val: 14it [00:06,  3.84it/s][A
val: 14it [00:06,  3.82it/s][A
val: 15it [00:06,  3.88it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:07,  3.92it/s][A
val: 16it [00:07,  3.90it/s][A
val: 17it [00:07,  3.94it/s][A
val: 17it [00:07,  3.93it/s][A
val: 18it [00:07,  3.95it/s][A
val: 18it [00:07,  3.94it/s][A
val: 19it [00:07,  3.96it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:08,  3.97it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.98it/s][A
val: 21it [00:08,  3.97it/s][A
val: 22it [00:08,  3.98it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.98it/s][A
val: 23it [00:09,  3.97it/s][A
val: 24it [00:09,  3.99it/s][A
val: 24it [00:09,  3.98it/s][A
                            [A
                            [Aepoch 1/100: 474it [10:40,  2.58it/s, loss=4.47]epoch 1/100: 474it [10:50,  2.58it/s, best_val=4.31, loss=4.38, val=4.31]epoch 1/100: 475it [10:50,  6.01s/it, loss=4.47]epoch 1/100: 475it [10:50,  6.01s/it, best_val=4.31, loss=4.38, val=4.31]epoch 1/100: 475it [10:50,  6.01s/it, loss=4.17]epoch 1/100: 475it [10:50,  6.01s/it, loss=4.27]                         epoch 1/100: 476it [10:50,  4.33s/it, loss=4.17]epoch 1/100: 476it [10:50,  4.33s/it, loss=4.27]epoch 1/100: 476it [10:51,  4.33s/it, loss=4.43]epoch 1/100: 476it [10:51,  4.33s/it, loss=4.24]epoch 1/100: 477it [10:51,  3.14s/it, loss=4.43]epoch 1/100: 477it [10:51,  3.14s/it, loss=4.24]epoch 1/100: 477it [10:51,  3.14s/it, loss=4.25]epoch 1/100: 477it [10:51,  3.14s/it, loss=4.33]epoch 1/100: 478it [10:51,  2.32s/it, loss=4.25]epoch 1/100: 478it [10:51,  2.32s/it, loss=4.33]epoch 1/100: 478it [10:51,  2.32s/it, loss=4.39]epoch 1/100: 478it [10:51,  2.32s/it, loss=4.36]epoch 1/100: 479it [10:51,  1.74s/it, loss=4.36]epoch 1/100: 479it [10:51,  1.74s/it, loss=4.39]epoch 1/100: 479it [10:52,  1.74s/it, loss=4.14]epoch 1/100: 479it [10:52,  1.74s/it, loss=4.54]epoch 1/100: 480it [10:52,  1.33s/it, loss=4.14]epoch 1/100: 480it [10:52,  1.33s/it, loss=4.54]epoch 1/100: 480it [10:52,  1.33s/it, loss=4.6] epoch 1/100: 480it [10:52,  1.33s/it, loss=4.32]epoch 1/100: 481it [10:52,  1.05s/it, loss=4.6]epoch 1/100: 481it [10:52,  1.05s/it, loss=4.32]epoch 1/100: 481it [10:52,  1.05s/it, loss=4.13]epoch 1/100: 481it [10:52,  1.05s/it, loss=4.26]epoch 1/100: 482it [10:52,  1.18it/s, loss=4.26]epoch 1/100: 482it [10:52,  1.18it/s, loss=4.13]epoch 1/100: 482it [10:53,  1.18it/s, loss=4.24]epoch 1/100: 482it [10:53,  1.18it/s, loss=4.4] epoch 1/100: 483it [10:53,  1.41it/s, loss=4.4]epoch 1/100: 483it [10:53,  1.41it/s, loss=4.24]epoch 1/100: 483it [10:53,  1.41it/s, loss=4.23]epoch 1/100: 483it [10:53,  1.41it/s, loss=4.42]epoch 1/100: 484it [10:53,  1.63it/s, loss=4.23]epoch 1/100: 484it [10:53,  1.63it/s, loss=4.42]epoch 1/100: 484it [10:54,  1.63it/s, loss=4.36]epoch 1/100: 484it [10:54,  1.63it/s, loss=4.34]epoch 1/100: 485it [10:54,  1.84it/s, loss=4.36]epoch 1/100: 485it [10:54,  1.84it/s, loss=4.34]epoch 1/100: 485it [10:54,  1.84it/s, loss=4.23]epoch 1/100: 485it [10:54,  1.84it/s, loss=4.43]epoch 1/100: 486it [10:54,  2.01it/s, loss=4.23]epoch 1/100: 486it [10:54,  2.01it/s, loss=4.43]epoch 1/100: 486it [10:54,  2.01it/s, loss=4.51]epoch 1/100: 486it [10:54,  2.01it/s, loss=4.58]epoch 1/100: 487it [10:54,  2.16it/s, loss=4.51]epoch 1/100: 487it [10:54,  2.16it/s, loss=4.58]epoch 1/100: 487it [10:55,  2.16it/s, loss=4.22]epoch 1/100: 487it [10:55,  2.16it/s, loss=4.23]epoch 1/100: 488it [10:55,  2.27it/s, loss=4.22]epoch 1/100: 488it [10:55,  2.27it/s, loss=4.23]epoch 1/100: 488it [10:55,  1.34s/it, loss=4.23]
epoch 1/100: 488it [10:55,  1.34s/it, loss=4.22]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 2/100: 0it [00:00, ?it/s]Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 2/100: 0it [00:00, ?it/s]Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
epoch 2/100: 0it [00:03, ?it/s, loss=4.27]epoch 2/100: 0it [00:03, ?it/s, loss=4.53]epoch 2/100: 1it [00:03,  3.20s/it, loss=4.53]epoch 2/100: 1it [00:03,  3.21s/it, loss=4.27]epoch 2/100: 1it [00:05,  3.20s/it, loss=4.26]epoch 2/100: 1it [00:05,  3.21s/it, loss=4.32]epoch 2/100: 2it [00:05,  2.49s/it, loss=4.32]epoch 2/100: 2it [00:05,  2.49s/it, loss=4.26]epoch 2/100: 2it [00:05,  2.49s/it, loss=4.54]epoch 2/100: 2it [00:05,  2.49s/it, loss=4.23]epoch 2/100: 3it [00:05,  1.53s/it, loss=4.54]epoch 2/100: 3it [00:05,  1.53s/it, loss=4.23]epoch 2/100: 3it [00:05,  1.53s/it, loss=4.27]epoch 2/100: 3it [00:05,  1.53s/it, loss=4.32]epoch 2/100: 4it [00:05,  1.08s/it, loss=4.32]epoch 2/100: 4it [00:05,  1.08s/it, loss=4.27]epoch 2/100: 4it [00:06,  1.08s/it, loss=4.18]epoch 2/100: 4it [00:06,  1.08s/it, loss=4.4] epoch 2/100: 5it [00:06,  1.21it/s, loss=4.18]epoch 2/100: 5it [00:06,  1.21it/s, loss=4.4]epoch 2/100: 5it [00:06,  1.21it/s, loss=4.3] epoch 2/100: 5it [00:06,  1.21it/s, loss=4.32]epoch 2/100: 6it [00:06,  1.48it/s, loss=4.3]epoch 2/100: 6it [00:06,  1.47it/s, loss=4.32]epoch 2/100: 6it [00:07,  1.48it/s, loss=4.43]epoch 2/100: 6it [00:07,  1.47it/s, loss=4.54]epoch 2/100: 7it [00:07,  1.72it/s, loss=4.43]epoch 2/100: 7it [00:07,  1.72it/s, loss=4.54]epoch 2/100: 7it [00:07,  1.72it/s, loss=4.33]epoch 2/100: 7it [00:07,  1.72it/s, loss=4.26]epoch 2/100: 8it [00:07,  1.92it/s, loss=4.33]epoch 2/100: 8it [00:07,  1.92it/s, loss=4.26]epoch 2/100: 8it [00:07,  1.92it/s, loss=4.34]epoch 2/100: 8it [00:07,  1.92it/s, loss=4.42]epoch 2/100: 9it [00:07,  2.09it/s, loss=4.34]epoch 2/100: 9it [00:07,  2.09it/s, loss=4.42]epoch 2/100: 9it [00:08,  2.09it/s, loss=4.32]epoch 2/100: 9it [00:08,  2.09it/s, loss=4.12]epoch 2/100: 10it [00:08,  2.23it/s, loss=4.32]epoch 2/100: 10it [00:08,  2.22it/s, loss=4.12]epoch 2/100: 10it [00:08,  2.23it/s, loss=4.28]epoch 2/100: 10it [00:08,  2.22it/s, loss=4.12]epoch 2/100: 11it [00:08,  2.32it/s, loss=4.28]epoch 2/100: 11it [00:08,  2.32it/s, loss=4.12]epoch 2/100: 11it [00:09,  2.32it/s, loss=4.37]epoch 2/100: 11it [00:09,  2.32it/s, loss=4.23]epoch 2/100: 12it [00:09,  2.39it/s, loss=4.23]epoch 2/100: 12it [00:09,  2.39it/s, loss=4.37]epoch 2/100: 12it [00:09,  2.39it/s, loss=4.55]epoch 2/100: 12it [00:09,  2.39it/s, loss=4.4] epoch 2/100: 13it [00:09,  2.45it/s, loss=4.55]epoch 2/100: 13it [00:09,  2.45it/s, loss=4.4]epoch 2/100: 13it [00:09,  2.45it/s, loss=4.44]epoch 2/100: 13it [00:09,  2.45it/s, loss=4.15]epoch 2/100: 14it [00:09,  2.49it/s, loss=4.44]epoch 2/100: 14it [00:09,  2.49it/s, loss=4.15]epoch 2/100: 14it [00:10,  2.49it/s, loss=4.12]epoch 2/100: 14it [00:10,  2.49it/s, loss=4.18]epoch 2/100: 15it [00:10,  2.52it/s, loss=4.12]epoch 2/100: 15it [00:10,  2.52it/s, loss=4.18]epoch 2/100: 15it [00:10,  2.52it/s, loss=4.38]epoch 2/100: 15it [00:10,  2.52it/s, loss=4.29]epoch 2/100: 16it [00:10,  2.54it/s, loss=4.38]epoch 2/100: 16it [00:10,  2.54it/s, loss=4.29]epoch 2/100: 16it [00:10,  2.54it/s, loss=4.29]epoch 2/100: 16it [00:10,  2.54it/s, loss=4.24]epoch 2/100: 17it [00:10,  2.56it/s, loss=4.24]epoch 2/100: 17it [00:10,  2.56it/s, loss=4.29]epoch 2/100: 17it [00:11,  2.56it/s, loss=4.28]epoch 2/100: 17it [00:11,  2.56it/s, loss=4.34]epoch 2/100: 18it [00:11,  2.57it/s, loss=4.28]epoch 2/100: 18it [00:11,  2.57it/s, loss=4.34]epoch 2/100: 18it [00:11,  2.57it/s, loss=4.28]epoch 2/100: 18it [00:11,  2.57it/s, loss=4.48]epoch 2/100: 19it [00:11,  2.58it/s, loss=4.48]epoch 2/100: 19it [00:11,  2.58it/s, loss=4.28]epoch 2/100: 19it [00:12,  2.58it/s, loss=4.32]epoch 2/100: 19it [00:12,  2.58it/s, loss=4.18]epoch 2/100: 20it [00:12,  2.58it/s, loss=4.18]epoch 2/100: 20it [00:12,  2.58it/s, loss=4.32]epoch 2/100: 20it [00:12,  2.58it/s, loss=4.27]epoch 2/100: 20it [00:12,  2.58it/s, loss=4.36]epoch 2/100: 21it [00:12,  2.59it/s, loss=4.36]epoch 2/100: 21it [00:12,  2.59it/s, loss=4.27]epoch 2/100: 21it [00:12,  2.59it/s, loss=4.04]epoch 2/100: 21it [00:12,  2.59it/s, loss=4.25]epoch 2/100: 22it [00:12,  2.59it/s, loss=4.25]epoch 2/100: 22it [00:12,  2.59it/s, loss=4.04]epoch 2/100: 22it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 22it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 23it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 23it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 23it [00:13,  2.59it/s, loss=4.39]epoch 2/100: 23it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 24it [00:13,  2.59it/s, loss=4.33]epoch 2/100: 24it [00:13,  2.59it/s, loss=4.39]epoch 2/100: 24it [00:14,  2.59it/s, loss=4.44]epoch 2/100: 24it [00:14,  2.59it/s, loss=4.3] 
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.36s/it][A
val: 1it [00:03,  3.63s/it][A
val: 2it [00:03,  1.64s/it][A
val: 3it [00:04,  1.01s/it][A
val: 4it [00:04,  1.41it/s][A
val: 5it [00:04,  1.83it/s][A
val: 6it [00:04,  2.24it/s][A
val: 7it [00:05,  2.62it/s][A
val: 8it [00:05,  2.93it/s][A
val: 9it [00:05,  3.19it/s][A
val: 10it [00:05,  3.40it/s][A
val: 11it [00:06,  3.55it/s][A
val: 12it [00:06,  3.67it/s][A
val: 13it [00:06,  3.75it/s][A
val: 14it [00:06,  3.81it/s][A
val: 15it [00:07,  3.86it/s][A
val: 16it [00:07,  3.89it/s][A
val: 17it [00:07,  3.91it/s][A
val: 18it [00:07,  3.93it/s][A
val: 19it [00:08,  3.94it/s][A
val: 20it [00:08,  3.95it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:09,  3.97it/s][A
val: 24it [00:09,  3.97it/s][A
                            [AFailed to connect to remote data host. Retrying in 5sec [1/20]

val: 2it [00:18, 10.15s/it][A
val: 3it [00:18,  5.63s/it][A
val: 4it [00:18,  3.51s/it][A
val: 5it [00:19,  2.33s/it][A
val: 6it [00:19,  1.62s/it][A
val: 7it [00:19,  1.18s/it][A
val: 8it [00:19,  1.14it/s][A
val: 9it [00:20,  1.46it/s][A
val: 10it [00:20,  1.82it/s][A
val: 11it [00:20,  1.92it/s][A
val: 12it [00:20,  2.28it/s][A
val: 13it [00:21,  2.62it/s][A
val: 14it [00:21,  2.92it/s][A
val: 15it [00:21,  3.18it/s][A
val: 16it [00:21,  3.39it/s][A
val: 17it [00:22,  3.55it/s][A
val: 18it [00:22,  3.67it/s][A
val: 19it [00:22,  3.76it/s][A
val: 20it [00:22,  3.83it/s][A
val: 21it [00:23,  3.88it/s][A
val: 22it [00:23,  3.91it/s][A
val: 23it [00:23,  3.94it/s][A
val: 24it [00:23,  3.95it/s][A
                            [Aepoch 2/100: 24it [00:38,  2.59it/s, loss=4.3]epoch 2/100: 24it [00:47,  2.59it/s, best_val=4.27, loss=4.44, val=4.27]epoch 2/100: 25it [00:47, 10.42s/it, loss=4.3]epoch 2/100: 25it [00:47, 10.42s/it, best_val=4.27, loss=4.44, val=4.27]epoch 2/100: 25it [00:47, 10.42s/it, loss=4.16]epoch 2/100: 25it [00:47, 10.42s/it, loss=4.19]                         epoch 2/100: 26it [00:47,  7.41s/it, loss=4.16]epoch 2/100: 26it [00:47,  7.41s/it, loss=4.19]epoch 2/100: 26it [00:48,  7.41s/it, loss=4.21]epoch 2/100: 26it [00:48,  7.41s/it, loss=4.3] epoch 2/100: 27it [00:48,  5.30s/it, loss=4.3]epoch 2/100: 27it [00:48,  5.30s/it, loss=4.21]epoch 2/100: 27it [00:48,  5.30s/it, loss=4.25]epoch 2/100: 27it [00:48,  5.30s/it, loss=4.26]epoch 2/100: 28it [00:48,  3.83s/it, loss=4.25]epoch 2/100: 28it [00:48,  3.83s/it, loss=4.26]epoch 2/100: 28it [00:49,  3.83s/it, loss=4.35]epoch 2/100: 28it [00:49,  3.83s/it, loss=4.26]epoch 2/100: 29it [00:49,  2.80s/it, loss=4.35]epoch 2/100: 29it [00:49,  2.80s/it, loss=4.26]epoch 2/100: 29it [00:49,  2.80s/it, loss=4.32]epoch 2/100: 29it [00:49,  2.80s/it, loss=4.23]epoch 2/100: 30it [00:49,  2.07s/it, loss=4.23]epoch 2/100: 30it [00:49,  2.07s/it, loss=4.32]epoch 2/100: 30it [00:49,  2.07s/it, loss=4.2] epoch 2/100: 30it [00:49,  2.07s/it, loss=4.31]epoch 2/100: 31it [00:49,  1.57s/it, loss=4.2]epoch 2/100: 31it [00:49,  1.57s/it, loss=4.31]epoch 2/100: 31it [00:50,  1.57s/it, loss=4.26]epoch 2/100: 31it [00:50,  1.57s/it, loss=4.17]epoch 2/100: 32it [00:50,  1.21s/it, loss=4.26]epoch 2/100: 32it [00:50,  1.21s/it, loss=4.17]epoch 2/100: 32it [00:50,  1.21s/it, loss=4.56]epoch 2/100: 32it [00:50,  1.21s/it, loss=4.3] epoch 2/100: 33it [00:50,  1.04it/s, loss=4.56]epoch 2/100: 33it [00:50,  1.04it/s, loss=4.3]epoch 2/100: 33it [00:50,  1.04it/s, loss=4.25]epoch 2/100: 33it [00:50,  1.04it/s, loss=4.37]epoch 2/100: 34it [00:50,  1.27it/s, loss=4.37]epoch 2/100: 34it [00:50,  1.27it/s, loss=4.25]epoch 2/100: 34it [00:51,  1.27it/s, loss=4.23]epoch 2/100: 34it [00:51,  1.27it/s, loss=4.36]epoch 2/100: 35it [00:51,  1.50it/s, loss=4.36]epoch 2/100: 35it [00:51,  1.50it/s, loss=4.23]epoch 2/100: 35it [00:51,  1.50it/s, loss=4.34]epoch 2/100: 35it [00:51,  1.50it/s, loss=4.43]epoch 2/100: 36it [00:51,  1.71it/s, loss=4.34]epoch 2/100: 36it [00:51,  1.71it/s, loss=4.43]epoch 2/100: 36it [00:52,  1.71it/s, loss=4.36]epoch 2/100: 36it [00:52,  1.71it/s, loss=4.18]epoch 2/100: 37it [00:52,  1.91it/s, loss=4.36]epoch 2/100: 37it [00:52,  1.91it/s, loss=4.18]epoch 2/100: 37it [00:52,  1.91it/s, loss=4.22]epoch 2/100: 37it [00:52,  1.91it/s, loss=4.57]epoch 2/100: 38it [00:52,  2.07it/s, loss=4.57]epoch 2/100: 38it [00:52,  2.07it/s, loss=4.22]epoch 2/100: 38it [00:52,  2.07it/s, loss=4.08]epoch 2/100: 38it [00:52,  2.07it/s, loss=4.5] epoch 2/100: 39it [00:52,  2.21it/s, loss=4.08]epoch 2/100: 39it [00:52,  2.21it/s, loss=4.5]epoch 2/100: 39it [00:53,  2.21it/s, loss=4.15]epoch 2/100: 39it [00:53,  2.21it/s, loss=4.28]epoch 2/100: 40it [00:53,  2.31it/s, loss=4.15]epoch 2/100: 40it [00:53,  2.31it/s, loss=4.28]epoch 2/100: 40it [00:53,  2.31it/s, loss=4.45]epoch 2/100: 40it [00:53,  2.31it/s, loss=4.26]epoch 2/100: 41it [00:53,  2.39it/s, loss=4.45]epoch 2/100: 41it [00:53,  2.39it/s, loss=4.26]epoch 2/100: 41it [00:54,  2.39it/s, loss=4.23]epoch 2/100: 41it [00:54,  2.39it/s, loss=4.27]epoch 2/100: 42it [00:54,  2.45it/s, loss=4.27]epoch 2/100: 42it [00:54,  2.45it/s, loss=4.23]epoch 2/100: 42it [00:54,  2.45it/s, loss=4.33]epoch 2/100: 42it [00:54,  2.45it/s, loss=4.39]epoch 2/100: 43it [00:54,  2.49it/s, loss=4.33]epoch 2/100: 43it [00:54,  2.49it/s, loss=4.39]epoch 2/100: 43it [00:54,  2.49it/s, loss=4.05]epoch 2/100: 43it [00:54,  2.49it/s, loss=4.21]epoch 2/100: 44it [00:54,  2.52it/s, loss=4.21]epoch 2/100: 44it [00:54,  2.52it/s, loss=4.05]epoch 2/100: 44it [00:55,  2.52it/s, loss=4.31]epoch 2/100: 44it [00:55,  2.52it/s, loss=4.49]epoch 2/100: 45it [00:55,  2.54it/s, loss=4.31]epoch 2/100: 45it [00:55,  2.54it/s, loss=4.49]epoch 2/100: 45it [00:55,  2.54it/s, loss=4.04]epoch 2/100: 45it [00:55,  2.54it/s, loss=4.41]epoch 2/100: 46it [00:55,  2.56it/s, loss=4.04]epoch 2/100: 46it [00:55,  2.56it/s, loss=4.41]epoch 2/100: 46it [00:55,  2.56it/s, loss=4.36]epoch 2/100: 46it [00:55,  2.56it/s, loss=4.25]epoch 2/100: 47it [00:56,  2.57it/s, loss=4.25]epoch 2/100: 47it [00:55,  2.57it/s, loss=4.36]epoch 2/100: 47it [00:56,  2.57it/s, loss=4.23]epoch 2/100: 47it [00:56,  2.57it/s, loss=4.09]epoch 2/100: 48it [00:56,  2.57it/s, loss=4.23]epoch 2/100: 48it [00:56,  2.57it/s, loss=4.09]epoch 2/100: 48it [00:56,  2.57it/s, loss=4.37]epoch 2/100: 48it [00:56,  2.57it/s, loss=4.26]epoch 2/100: 49it [00:56,  2.58it/s, loss=4.26]epoch 2/100: 49it [00:56,  2.58it/s, loss=4.37]epoch 2/100: 49it [00:57,  2.58it/s, loss=4.41]epoch 2/100: 49it [00:57,  2.58it/s, loss=4.39]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:02,  2.96s/it][A
val: 1it [00:03,  3.19s/it][A
val: 2it [00:03,  1.46s/it][A
val: 3it [00:03,  1.10it/s][A
val: 4it [00:03,  1.54it/s][A
val: 5it [00:04,  1.97it/s][A
val: 6it [00:04,  2.38it/s][A
val: 7it [00:04,  2.73it/s][A
val: 8it [00:04,  3.03it/s][A
val: 9it [00:05,  3.27it/s][A
val: 10it [00:05,  3.46it/s][A
val: 11it [00:05,  3.60it/s][A
val: 12it [00:05,  3.71it/s][A
val: 13it [00:06,  3.78it/s][A
val: 14it [00:06,  3.85it/s][A
val: 15it [00:06,  3.88it/s][A
val: 16it [00:06,  3.91it/s][A
val: 2it [00:07,  3.65s/it][A
val: 17it [00:07,  3.93it/s][A
val: 3it [00:07,  2.10s/it][A
val: 18it [00:07,  3.95it/s][A
val: 4it [00:07,  1.37s/it][A
val: 19it [00:07,  3.95it/s][A
val: 5it [00:07,  1.04it/s][A
val: 20it [00:07,  3.96it/s][A
val: 6it [00:08,  1.38it/s][A
val: 21it [00:08,  3.97it/s][A
val: 7it [00:08,  1.76it/s][A
val: 22it [00:08,  3.97it/s][A
val: 8it [00:08,  2.14it/s][A
val: 23it [00:08,  3.97it/s][A
val: 9it [00:08,  2.51it/s][A
val: 24it [00:08,  3.97it/s][A
val: 10it [00:09,  2.83it/s][A
                            [A
val: 11it [00:09,  3.11it/s][A
val: 12it [00:09,  3.34it/s][A
val: 13it [00:09,  3.51it/s][A
val: 14it [00:10,  3.65it/s][A
val: 15it [00:10,  3.74it/s][A
val: 16it [00:10,  3.82it/s][A
val: 17it [00:10,  3.86it/s][A
val: 18it [00:11,  3.90it/s][A
val: 19it [00:11,  3.93it/s][A
val: 20it [00:11,  3.95it/s][A
val: 21it [00:11,  3.96it/s][A
val: 22it [00:12,  3.97it/s][A
val: 23it [00:12,  3.98it/s][A
val: 24it [00:12,  3.98it/s][A
                            [Aepoch 2/100: 49it [01:09,  2.58it/s, loss=4.39]epoch 2/100: 49it [01:18,  2.58it/s, best_val=4.25, loss=4.41, val=4.25]epoch 2/100: 50it [01:18,  6.67s/it, loss=4.39]epoch 2/100: 50it [01:18,  6.67s/it, best_val=4.25, loss=4.41, val=4.25]epoch 2/100: 50it [01:18,  6.67s/it, loss=4.21]                         epoch 2/100: 50it [01:18,  6.67s/it, loss=4.41]epoch 2/100: 51it [01:18,  4.79s/it, loss=4.41]epoch 2/100: 51it [01:18,  4.79s/it, loss=4.21]epoch 2/100: 51it [01:18,  4.79s/it, loss=4.27]epoch 2/100: 51it [01:18,  4.79s/it, loss=4.4] epoch 2/100: 52it [01:18,  3.47s/it, loss=4.27]epoch 2/100: 52it [01:18,  3.47s/it, loss=4.4]epoch 2/100: 52it [01:19,  3.47s/it, loss=4.25]epoch 2/100: 52it [01:19,  3.47s/it, loss=4.46]epoch 2/100: 53it [01:19,  2.54s/it, loss=4.25]epoch 2/100: 53it [01:19,  2.54s/it, loss=4.46]epoch 2/100: 53it [01:19,  2.54s/it, loss=4.24]epoch 2/100: 53it [01:19,  2.54s/it, loss=4.14]epoch 2/100: 54it [01:19,  1.90s/it, loss=4.24]epoch 2/100: 54it [01:19,  1.90s/it, loss=4.14]epoch 2/100: 54it [01:20,  1.90s/it, loss=4.33]epoch 2/100: 54it [01:20,  1.90s/it, loss=4.32]epoch 2/100: 55it [01:20,  1.44s/it, loss=4.33]epoch 2/100: 55it [01:20,  1.44s/it, loss=4.32]epoch 2/100: 55it [01:20,  1.44s/it, loss=4.31]epoch 2/100: 55it [01:20,  1.44s/it, loss=4.28]epoch 2/100: 56it [01:20,  1.12s/it, loss=4.31]epoch 2/100: 56it [01:20,  1.12s/it, loss=4.28]epoch 2/100: 56it [01:20,  1.12s/it, loss=4.1] epoch 2/100: 56it [01:20,  1.12s/it, loss=4.03]epoch 2/100: 57it [01:20,  1.11it/s, loss=4.1]epoch 2/100: 57it [01:20,  1.11it/s, loss=4.03]epoch 2/100: 57it [01:21,  1.11it/s, loss=4.26]epoch 2/100: 57it [01:21,  1.11it/s, loss=4.41]epoch 2/100: 58it [01:21,  1.34it/s, loss=4.26]epoch 2/100: 58it [01:21,  1.34it/s, loss=4.41]epoch 2/100: 58it [01:21,  1.34it/s, loss=4.31]epoch 2/100: 58it [01:21,  1.34it/s, loss=4.2] epoch 2/100: 59it [01:21,  1.56it/s, loss=4.2]epoch 2/100: 59it [01:21,  1.56it/s, loss=4.31]epoch 2/100: 59it [01:21,  1.56it/s, loss=4.48]epoch 2/100: 59it [01:21,  1.56it/s, loss=4.32]epoch 2/100: 60it [01:21,  1.77it/s, loss=4.32]epoch 2/100: 60it [01:21,  1.77it/s, loss=4.48]epoch 2/100: 60it [01:22,  1.77it/s, loss=4.21]epoch 2/100: 60it [01:22,  1.77it/s, loss=4.2] epoch 2/100: 61it [01:22,  1.96it/s, loss=4.2]epoch 2/100: 61it [01:22,  1.96it/s, loss=4.21]epoch 2/100: 61it [01:22,  1.96it/s, loss=4.45]epoch 2/100: 61it [01:22,  1.96it/s, loss=4.44]epoch 2/100: 62it [01:22,  2.11it/s, loss=4.45]epoch 2/100: 62it [01:22,  2.11it/s, loss=4.44]epoch 2/100: 62it [01:23,  2.11it/s, loss=4.23]epoch 2/100: 62it [01:23,  2.11it/s, loss=4.37]epoch 2/100: 63it [01:23,  2.24it/s, loss=4.23]epoch 2/100: 63it [01:23,  2.24it/s, loss=4.37]epoch 2/100: 63it [01:23,  2.24it/s, loss=4.39]epoch 2/100: 63it [01:23,  2.24it/s, loss=4.37]epoch 2/100: 64it [01:23,  2.33it/s, loss=4.39]epoch 2/100: 64it [01:23,  2.33it/s, loss=4.37]epoch 2/100: 64it [01:23,  2.33it/s, loss=4.31]epoch 2/100: 64it [01:23,  2.33it/s, loss=4.27]epoch 2/100: 65it [01:23,  2.40it/s, loss=4.31]epoch 2/100: 65it [01:23,  2.40it/s, loss=4.27]epoch 2/100: 65it [01:24,  2.40it/s, loss=4.12]epoch 2/100: 65it [01:24,  2.40it/s, loss=4.47]epoch 2/100: 66it [01:24,  2.46it/s, loss=4.12]epoch 2/100: 66it [01:24,  2.46it/s, loss=4.47]epoch 2/100: 66it [01:24,  2.46it/s, loss=4.45]epoch 2/100: 66it [01:24,  2.46it/s, loss=4.37]epoch 2/100: 67it [01:24,  2.50it/s, loss=4.45]epoch 2/100: 67it [01:24,  2.50it/s, loss=4.37]epoch 2/100: 67it [01:25,  2.50it/s, loss=4.25]epoch 2/100: 67it [01:25,  2.50it/s, loss=4.16]epoch 2/100: 68it [01:25,  2.52it/s, loss=4.16]epoch 2/100: 68it [01:25,  2.52it/s, loss=4.25]epoch 2/100: 68it [01:25,  2.52it/s, loss=4.17]epoch 2/100: 68it [01:25,  2.52it/s, loss=4.42]epoch 2/100: 69it [01:25,  2.54it/s, loss=4.17]epoch 2/100: 69it [01:25,  2.54it/s, loss=4.42]epoch 2/100: 69it [01:25,  2.54it/s, loss=4.26]epoch 2/100: 69it [01:25,  2.54it/s, loss=4.32]epoch 2/100: 70it [01:25,  2.56it/s, loss=4.26]epoch 2/100: 70it [01:25,  2.56it/s, loss=4.32]epoch 2/100: 70it [01:26,  2.56it/s, loss=4.42]epoch 2/100: 70it [01:26,  2.56it/s, loss=4.33]epoch 2/100: 71it [01:26,  2.56it/s, loss=4.42]epoch 2/100: 71it [01:26,  2.56it/s, loss=4.33]epoch 2/100: 71it [01:26,  2.56it/s, loss=4.23]epoch 2/100: 71it [01:26,  2.56it/s, loss=4.12]epoch 2/100: 72it [01:26,  2.57it/s, loss=4.23]epoch 2/100: 72it [01:26,  2.57it/s, loss=4.12]epoch 2/100: 72it [01:26,  2.57it/s, loss=4.46]epoch 2/100: 72it [01:26,  2.57it/s, loss=4.18]epoch 2/100: 73it [01:26,  2.58it/s, loss=4.46]epoch 2/100: 73it [01:26,  2.58it/s, loss=4.18]epoch 2/100: 73it [01:27,  2.58it/s, loss=4.41]epoch 2/100: 73it [01:27,  2.58it/s, loss=4.16]epoch 2/100: 74it [01:27,  2.58it/s, loss=4.41]epoch 2/100: 74it [01:27,  2.58it/s, loss=4.16]epoch 2/100: 74it [01:27,  2.58it/s, loss=4.24]epoch 2/100: 74it [01:27,  2.58it/s, loss=4.62]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.07s/it][A
val: 2it [00:03,  1.41s/it][A
val: 3it [00:03,  1.13it/s][A
val: 1it [00:03,  3.59s/it][A
val: 4it [00:03,  1.58it/s][A
val: 2it [00:03,  1.63s/it][A
val: 5it [00:04,  2.02it/s][A
val: 3it [00:04,  1.00it/s][A
val: 6it [00:04,  2.43it/s][A
val: 4it [00:04,  1.42it/s][A
val: 7it [00:04,  2.78it/s][A
val: 5it [00:04,  1.62it/s][A
val: 8it [00:04,  3.07it/s][A
val: 6it [00:05,  2.03it/s][A
val: 9it [00:05,  3.31it/s][A
val: 7it [00:05,  2.41it/s][A
val: 10it [00:05,  3.49it/s][A
val: 8it [00:05,  2.76it/s][A
val: 11it [00:05,  3.63it/s][A
val: 9it [00:05,  3.05it/s][A
val: 12it [00:05,  3.73it/s][A
val: 10it [00:06,  3.28it/s][A
val: 13it [00:06,  3.80it/s][A
val: 11it [00:06,  3.46it/s][A
val: 14it [00:06,  3.85it/s][A
val: 12it [00:06,  3.61it/s][A
val: 15it [00:06,  3.89it/s][A
val: 13it [00:06,  3.71it/s][A
val: 16it [00:06,  3.91it/s][A
val: 14it [00:07,  3.78it/s][A
val: 17it [00:07,  3.93it/s][A
val: 15it [00:07,  3.84it/s][A
val: 18it [00:07,  3.95it/s][A
val: 16it [00:07,  3.88it/s][A
val: 19it [00:07,  3.96it/s][A
val: 17it [00:07,  3.90it/s][A
val: 20it [00:07,  3.97it/s][A
val: 18it [00:08,  3.92it/s][A
val: 21it [00:08,  3.97it/s][A
val: 19it [00:08,  3.93it/s][A
val: 22it [00:08,  3.98it/s][A
val: 20it [00:08,  3.95it/s][A
val: 23it [00:08,  3.98it/s][A
val: 21it [00:08,  3.95it/s][A
val: 24it [00:08,  3.96it/s][A
                            [A
val: 22it [00:09,  3.96it/s][A
val: 23it [00:09,  3.96it/s][A
val: 24it [00:09,  3.97it/s][A
                            [Aepoch 2/100: 74it [01:37,  2.58it/s, loss=4.24]epoch 2/100: 74it [01:45,  2.58it/s, best_val=4.23, loss=4.62, val=4.23]epoch 2/100: 75it [01:45,  5.82s/it, loss=4.24]epoch 2/100: 75it [01:45,  5.82s/it, best_val=4.23, loss=4.62, val=4.23]epoch 2/100: 75it [01:46,  5.82s/it, loss=4.27]                         epoch 2/100: 75it [01:46,  5.82s/it, loss=3.88]epoch 2/100: 76it [01:46,  4.19s/it, loss=3.88]epoch 2/100: 76it [01:46,  4.19s/it, loss=4.27]epoch 2/100: 76it [01:46,  4.19s/it, loss=4.08]epoch 2/100: 76it [01:46,  4.19s/it, loss=4.23]epoch 2/100: 77it [01:46,  3.05s/it, loss=4.08]epoch 2/100: 77it [01:46,  3.05s/it, loss=4.23]epoch 2/100: 77it [01:47,  3.05s/it, loss=4.37]epoch 2/100: 77it [01:47,  3.05s/it, loss=4.36]epoch 2/100: 78it [01:47,  2.25s/it, loss=4.37]epoch 2/100: 78it [01:47,  2.25s/it, loss=4.36]epoch 2/100: 78it [01:47,  2.25s/it, loss=4.15]epoch 2/100: 78it [01:47,  2.25s/it, loss=4.35]epoch 2/100: 79it [01:47,  1.69s/it, loss=4.15]epoch 2/100: 79it [01:47,  1.69s/it, loss=4.35]epoch 2/100: 79it [01:47,  1.69s/it, loss=4.4] epoch 2/100: 79it [01:47,  1.69s/it, loss=4.43]epoch 2/100: 80it [01:47,  1.30s/it, loss=4.4]epoch 2/100: 80it [01:47,  1.30s/it, loss=4.43]epoch 2/100: 80it [01:48,  1.30s/it, loss=4.09]epoch 2/100: 80it [01:48,  1.30s/it, loss=4.28]epoch 2/100: 81it [01:48,  1.02s/it, loss=4.09]epoch 2/100: 81it [01:48,  1.02s/it, loss=4.28]epoch 2/100: 81it [01:48,  1.02s/it, loss=4.03]epoch 2/100: 81it [01:48,  1.02s/it, loss=3.99]epoch 2/100: 82it [01:48,  1.20it/s, loss=4.03]epoch 2/100: 82it [01:48,  1.20it/s, loss=3.99]epoch 2/100: 82it [01:48,  1.20it/s, loss=4.35]epoch 2/100: 82it [01:48,  1.20it/s, loss=4.3] epoch 2/100: 83it [01:48,  1.43it/s, loss=4.35]epoch 2/100: 83it [01:48,  1.43it/s, loss=4.3]epoch 2/100: 83it [01:49,  1.43it/s, loss=4.35]epoch 2/100: 83it [01:49,  1.43it/s, loss=4.14]epoch 2/100: 84it [01:49,  1.65it/s, loss=4.35]epoch 2/100: 84it [01:49,  1.65it/s, loss=4.14]epoch 2/100: 84it [01:49,  1.65it/s, loss=4.16]epoch 2/100: 84it [01:49,  1.65it/s, loss=4.19]epoch 2/100: 85it [01:49,  1.85it/s, loss=4.16]epoch 2/100: 85it [01:49,  1.85it/s, loss=4.19]epoch 2/100: 85it [01:50,  1.85it/s, loss=4.41]epoch 2/100: 85it [01:50,  1.85it/s, loss=4.07]epoch 2/100: 86it [01:50,  2.03it/s, loss=4.41]epoch 2/100: 86it [01:50,  2.03it/s, loss=4.07]epoch 2/100: 86it [01:50,  2.03it/s, loss=4.24]epoch 2/100: 86it [01:50,  2.03it/s, loss=4.37]epoch 2/100: 87it [01:50,  2.17it/s, loss=4.24]epoch 2/100: 87it [01:50,  2.17it/s, loss=4.37]epoch 2/100: 87it [01:50,  2.17it/s, loss=4.3] epoch 2/100: 87it [01:50,  2.17it/s, loss=4.15]epoch 2/100: 88it [01:50,  2.28it/s, loss=4.3]epoch 2/100: 88it [01:50,  2.28it/s, loss=4.15]epoch 2/100: 88it [01:51,  2.28it/s, loss=4.23]epoch 2/100: 88it [01:51,  2.28it/s, loss=4.12]epoch 2/100: 89it [01:51,  2.36it/s, loss=4.12]epoch 2/100: 89it [01:51,  2.36it/s, loss=4.23]epoch 2/100: 89it [01:51,  2.36it/s, loss=4.17]epoch 2/100: 89it [01:51,  2.36it/s, loss=4.32]epoch 2/100: 90it [01:51,  2.43it/s, loss=4.17]epoch 2/100: 90it [01:51,  2.43it/s, loss=4.32]epoch 2/100: 90it [01:52,  2.43it/s, loss=4.36]epoch 2/100: 90it [01:52,  2.43it/s, loss=4.4] epoch 2/100: 91it [01:52,  2.47it/s, loss=4.4]epoch 2/100: 91it [01:52,  2.47it/s, loss=4.36]epoch 2/100: 91it [01:52,  2.47it/s, loss=4.24]epoch 2/100: 91it [01:52,  2.47it/s, loss=4.18]epoch 2/100: 92it [01:52,  2.51it/s, loss=4.18]epoch 2/100: 92it [01:52,  2.51it/s, loss=4.24]epoch 2/100: 92it [01:52,  2.51it/s, loss=4.35]epoch 2/100: 92it [01:52,  2.51it/s, loss=4.21]epoch 2/100: 93it [01:52,  2.53it/s, loss=4.35]epoch 2/100: 93it [01:52,  2.53it/s, loss=4.21]epoch 2/100: 93it [01:53,  2.53it/s, loss=3.98]epoch 2/100: 93it [01:53,  2.53it/s, loss=4.24]epoch 2/100: 94it [01:53,  2.55it/s, loss=4.24]epoch 2/100: 94it [01:53,  2.55it/s, loss=3.98]epoch 2/100: 94it [01:53,  2.55it/s, loss=4.17]epoch 2/100: 94it [01:53,  2.55it/s, loss=4.07]epoch 2/100: 95it [01:53,  2.57it/s, loss=4.07]epoch 2/100: 95it [01:53,  2.57it/s, loss=4.17]epoch 2/100: 95it [01:53,  2.57it/s, loss=4.28]epoch 2/100: 95it [01:53,  2.57it/s, loss=4.14]epoch 2/100: 96it [01:53,  2.58it/s, loss=4.28]epoch 2/100: 96it [01:53,  2.58it/s, loss=4.14]epoch 2/100: 96it [01:54,  2.58it/s, loss=4.3] epoch 2/100: 96it [01:54,  2.58it/s, loss=4.17]epoch 2/100: 97it [01:54,  2.58it/s, loss=4.17]epoch 2/100: 97it [01:54,  2.58it/s, loss=4.3]epoch 2/100: 97it [01:54,  2.58it/s, loss=4.34]epoch 2/100: 97it [01:54,  2.58it/s, loss=4.18]epoch 2/100: 98it [01:54,  2.59it/s, loss=4.18]epoch 2/100: 98it [01:54,  2.59it/s, loss=4.34]epoch 2/100: 98it [01:55,  2.59it/s, loss=4.23]epoch 2/100: 98it [01:55,  2.59it/s, loss=4.51]epoch 2/100: 99it [01:55,  2.59it/s, loss=4.23]epoch 2/100: 99it [01:55,  2.59it/s, loss=4.51]epoch 2/100: 99it [01:55,  2.59it/s, loss=4.18]epoch 2/100: 99it [01:55,  2.59it/s, loss=4.27]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.44s/it][A
val: 1it [00:03,  3.45s/it][A
val: 2it [00:03,  1.57s/it][A
val: 3it [00:03,  1.04it/s][A
val: 4it [00:04,  1.46it/s][A
val: 5it [00:04,  1.89it/s][A
val: 6it [00:04,  2.31it/s][A
val: 7it [00:04,  2.67it/s][A
val: 8it [00:05,  2.98it/s][A
val: 9it [00:05,  3.23it/s][A
val: 10it [00:05,  3.43it/s][A
val: 11it [00:05,  3.58it/s][A
val: 12it [00:06,  3.69it/s][A
val: 13it [00:06,  3.77it/s][A
val: 14it [00:06,  3.83it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:07,  3.90it/s][A
val: 17it [00:07,  3.92it/s][A
val: 18it [00:07,  3.94it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.97it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.96it/s][A
val: 24it [00:09,  3.97it/s][A
                            [AFailed to connect to remote data host. Retrying in 5sec [1/20]

val: 2it [00:18, 10.12s/it][A
val: 3it [00:18,  5.61s/it][A
val: 4it [00:18,  3.50s/it][A
val: 5it [00:18,  2.33s/it][A
val: 6it [00:19,  1.62s/it][A
val: 7it [00:19,  1.17s/it][A
val: 8it [00:19,  1.14it/s][A
val: 9it [00:19,  1.46it/s][A
val: 10it [00:20,  1.82it/s][A
val: 11it [00:20,  2.18it/s][A
val: 12it [00:20,  2.52it/s][A
val: 13it [00:21,  2.84it/s][A
val: 14it [00:21,  3.11it/s][A
val: 15it [00:21,  3.33it/s][A
val: 16it [00:21,  3.50it/s][A
val: 17it [00:22,  3.63it/s][A
val: 18it [00:22,  3.73it/s][A
val: 19it [00:22,  3.80it/s][A
val: 20it [00:22,  3.85it/s][A
val: 21it [00:23,  3.89it/s][A
val: 22it [00:23,  3.91it/s][A
val: 23it [00:23,  3.94it/s][A
val: 24it [00:23,  3.95it/s][A
                            [Aepoch 2/100: 99it [02:19,  2.59it/s, loss=4.18]epoch 2/100: 99it [02:28,  2.59it/s, best_val=4.22, loss=4.27, val=4.22]epoch 2/100: 100it [02:28, 10.34s/it, loss=4.18]epoch 2/100: 100it [02:28, 10.34s/it, best_val=4.22, loss=4.27, val=4.22]epoch 2/100: 100it [02:29, 10.34s/it, loss=4.47]epoch 2/100: 100it [02:29, 10.34s/it, loss=4.23]                         epoch 2/100: 101it [02:29,  7.36s/it, loss=4.47]epoch 2/100: 101it [02:29,  7.36s/it, loss=4.23]epoch 2/100: 101it [02:29,  7.36s/it, loss=4.12]epoch 2/100: 101it [02:29,  7.36s/it, loss=4.44]epoch 2/100: 102it [02:29,  5.27s/it, loss=4.12]epoch 2/100: 102it [02:29,  5.27s/it, loss=4.44]epoch 2/100: 102it [02:29,  5.27s/it, loss=4.16]epoch 2/100: 102it [02:29,  5.27s/it, loss=4.34]epoch 2/100: 103it [02:29,  3.80s/it, loss=4.16]epoch 2/100: 103it [02:29,  3.80s/it, loss=4.34]epoch 2/100: 103it [02:30,  3.80s/it, loss=4.05]epoch 2/100: 103it [02:30,  3.80s/it, loss=4.38]epoch 2/100: 104it [02:30,  2.78s/it, loss=4.05]epoch 2/100: 104it [02:30,  2.78s/it, loss=4.38]epoch 2/100: 104it [02:30,  2.78s/it, loss=4.26]epoch 2/100: 104it [02:30,  2.78s/it, loss=4.26]epoch 2/100: 105it [02:30,  2.06s/it, loss=4.26]epoch 2/100: 105it [02:30,  2.06s/it, loss=4.26]epoch 2/100: 105it [02:30,  2.06s/it, loss=4.37]epoch 2/100: 105it [02:31,  2.06s/it, loss=4.29]epoch 2/100: 106it [02:30,  1.56s/it, loss=4.37]epoch 2/100: 106it [02:31,  1.56s/it, loss=4.29]epoch 2/100: 106it [02:31,  1.56s/it, loss=4.27]epoch 2/100: 106it [02:31,  1.56s/it, loss=4.16]epoch 2/100: 107it [02:31,  1.21s/it, loss=4.27]epoch 2/100: 107it [02:31,  1.21s/it, loss=4.16]epoch 2/100: 107it [02:31,  1.21s/it, loss=3.97]epoch 2/100: 107it [02:31,  1.21s/it, loss=4.18]epoch 2/100: 108it [02:31,  1.04it/s, loss=3.97]epoch 2/100: 108it [02:31,  1.04it/s, loss=4.18]epoch 2/100: 108it [02:32,  1.04it/s, loss=4.21]epoch 2/100: 108it [02:32,  1.04it/s, loss=4.31]epoch 2/100: 109it [02:32,  1.27it/s, loss=4.21]epoch 2/100: 109it [02:32,  1.27it/s, loss=4.31]epoch 2/100: 109it [02:32,  1.27it/s, loss=4.28]epoch 2/100: 109it [02:32,  1.27it/s, loss=4.4] epoch 2/100: 110it [02:32,  1.50it/s, loss=4.28]epoch 2/100: 110it [02:32,  1.50it/s, loss=4.4]epoch 2/100: 110it [02:32,  1.50it/s, loss=4.21]epoch 2/100: 110it [02:32,  1.50it/s, loss=4.32]epoch 2/100: 111it [02:32,  1.72it/s, loss=4.32]epoch 2/100: 111it [02:32,  1.72it/s, loss=4.21]epoch 2/100: 111it [02:33,  1.72it/s, loss=4.23]epoch 2/100: 111it [02:33,  1.72it/s, loss=4.25]epoch 2/100: 112it [02:33,  1.91it/s, loss=4.25]epoch 2/100: 112it [02:33,  1.91it/s, loss=4.23]epoch 2/100: 112it [02:33,  1.91it/s, loss=4.03]epoch 2/100: 112it [02:33,  1.91it/s, loss=4.03]epoch 2/100: 113it [02:33,  2.08it/s, loss=4.03]epoch 2/100: 113it [02:33,  2.08it/s, loss=4.03]epoch 2/100: 113it [02:34,  2.08it/s, loss=4.47]epoch 2/100: 113it [02:34,  2.08it/s, loss=4.33]epoch 2/100: 114it [02:34,  2.21it/s, loss=4.47]epoch 2/100: 114it [02:34,  2.21it/s, loss=4.33]epoch 2/100: 114it [02:34,  2.21it/s, loss=4.2] epoch 2/100: 114it [02:34,  2.21it/s, loss=4.3] epoch 2/100: 115it [02:34,  2.31it/s, loss=4.3]epoch 2/100: 115it [02:34,  2.31it/s, loss=4.2]epoch 2/100: 115it [02:34,  2.31it/s, loss=4.07]epoch 2/100: 115it [02:34,  2.31it/s, loss=4.25]epoch 2/100: 116it [02:34,  2.39it/s, loss=4.25]epoch 2/100: 116it [02:34,  2.39it/s, loss=4.07]epoch 2/100: 116it [02:35,  2.39it/s, loss=4.45]epoch 2/100: 116it [02:35,  2.39it/s, loss=4.24]epoch 2/100: 117it [02:35,  2.45it/s, loss=4.45]epoch 2/100: 117it [02:35,  2.45it/s, loss=4.24]epoch 2/100: 117it [02:35,  2.45it/s, loss=4.09]epoch 2/100: 117it [02:35,  2.45it/s, loss=4.21]epoch 2/100: 118it [02:35,  2.49it/s, loss=4.21]epoch 2/100: 118it [02:35,  2.49it/s, loss=4.09]epoch 2/100: 118it [02:36,  2.49it/s, loss=4.32]epoch 2/100: 118it [02:36,  2.49it/s, loss=4.13]epoch 2/100: 119it [02:36,  2.52it/s, loss=4.32]epoch 2/100: 119it [02:36,  2.52it/s, loss=4.13]epoch 2/100: 119it [02:36,  2.52it/s, loss=4.38]epoch 2/100: 119it [02:36,  2.52it/s, loss=4.39]epoch 2/100: 120it [02:36,  2.54it/s, loss=4.38]epoch 2/100: 120it [02:36,  2.54it/s, loss=4.39]epoch 2/100: 120it [02:36,  2.54it/s, loss=4.17]epoch 2/100: 120it [02:36,  2.54it/s, loss=4.16]epoch 2/100: 121it [02:36,  2.55it/s, loss=4.17]epoch 2/100: 121it [02:36,  2.55it/s, loss=4.16]epoch 2/100: 121it [02:37,  2.55it/s, loss=4.27]epoch 2/100: 121it [02:37,  2.55it/s, loss=4.11]epoch 2/100: 122it [02:37,  2.57it/s, loss=4.11]epoch 2/100: 122it [02:37,  2.57it/s, loss=4.27]epoch 2/100: 122it [02:37,  2.57it/s, loss=4.37]epoch 2/100: 122it [02:37,  2.57it/s, loss=4.38]epoch 2/100: 123it [02:37,  2.57it/s, loss=4.38]epoch 2/100: 123it [02:37,  2.57it/s, loss=4.37]epoch 2/100: 123it [02:37,  2.57it/s, loss=4.34]epoch 2/100: 123it [02:37,  2.57it/s, loss=4.29]epoch 2/100: 124it [02:37,  2.58it/s, loss=4.29]epoch 2/100: 124it [02:37,  2.58it/s, loss=4.34]epoch 2/100: 124it [02:38,  2.58it/s, loss=4.3] epoch 2/100: 124it [02:38,  2.58it/s, loss=4.06]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.75s/it][A
val: 1it [00:04,  4.44s/it][A
val: 2it [00:04,  1.98s/it][A
val: 3it [00:04,  1.19s/it][A
val: 2it [00:05,  2.36s/it][A
val: 4it [00:05,  1.22it/s][A
val: 3it [00:05,  1.40s/it][A
val: 5it [00:05,  1.63it/s][A
val: 4it [00:05,  1.06it/s][A
val: 6it [00:05,  2.03it/s][A
val: 5it [00:05,  1.44it/s][A
val: 7it [00:05,  2.42it/s][A
val: 6it [00:06,  1.84it/s][A
val: 8it [00:06,  2.76it/s][A
val: 7it [00:06,  2.24it/s][A
val: 9it [00:06,  3.05it/s][A
val: 8it [00:06,  2.60it/s][A
val: 10it [00:06,  3.28it/s][A
val: 9it [00:06,  2.92it/s][A
val: 11it [00:06,  3.46it/s][A
val: 10it [00:07,  3.18it/s][A
val: 12it [00:07,  3.61it/s][A
val: 11it [00:07,  3.39it/s][A
val: 13it [00:07,  3.71it/s][A
val: 12it [00:07,  3.56it/s][A
val: 14it [00:07,  3.79it/s][A
val: 13it [00:07,  3.68it/s][A
val: 15it [00:07,  3.84it/s][A
val: 14it [00:08,  3.77it/s][A
val: 16it [00:08,  3.87it/s][A
val: 15it [00:08,  3.83it/s][A
val: 17it [00:08,  3.91it/s][A
val: 16it [00:08,  3.88it/s][A
val: 18it [00:08,  3.93it/s][A
val: 17it [00:08,  3.91it/s][A
val: 19it [00:08,  3.94it/s][A
val: 18it [00:09,  3.94it/s][A
val: 20it [00:09,  3.95it/s][A
val: 19it [00:09,  3.96it/s][A
val: 21it [00:09,  3.95it/s][A
val: 20it [00:09,  3.97it/s][A
val: 22it [00:09,  3.96it/s][A
val: 21it [00:09,  3.98it/s][A
val: 23it [00:09,  3.97it/s][A
val: 22it [00:10,  3.98it/s][A
val: 24it [00:10,  3.98it/s][A
val: 23it [00:10,  3.99it/s][A
                            [A
val: 24it [00:10,  3.99it/s][A
                            [Aepoch 2/100: 124it [02:49,  2.58it/s, loss=4.3]epoch 2/100: 124it [02:57,  2.58it/s, best_val=4.2, loss=4.06, val=4.2]epoch 2/100: 125it [02:57,  6.21s/it, loss=4.3]epoch 2/100: 125it [02:57,  6.21s/it, best_val=4.2, loss=4.06, val=4.2]epoch 2/100: 125it [02:58,  6.21s/it, loss=4.22]epoch 2/100: 125it [02:58,  6.21s/it, loss=4.36]                       epoch 2/100: 126it [02:58,  4.47s/it, loss=4.22]epoch 2/100: 126it [02:58,  4.47s/it, loss=4.36]epoch 2/100: 126it [02:58,  4.47s/it, loss=4.07]epoch 2/100: 126it [02:58,  4.47s/it, loss=4.34]epoch 2/100: 127it [02:58,  3.24s/it, loss=4.34]epoch 2/100: 127it [02:58,  3.24s/it, loss=4.07]epoch 2/100: 127it [02:58,  3.24s/it, loss=3.96]epoch 2/100: 127it [02:58,  3.24s/it, loss=4.18]epoch 2/100: 128it [02:58,  2.38s/it, loss=3.96]epoch 2/100: 128it [02:58,  2.38s/it, loss=4.18]epoch 2/100: 128it [02:59,  2.38s/it, loss=3.91]epoch 2/100: 128it [02:59,  2.38s/it, loss=4.32]epoch 2/100: 129it [02:59,  1.78s/it, loss=4.32]epoch 2/100: 129it [02:59,  1.78s/it, loss=3.91]epoch 2/100: 129it [02:59,  1.78s/it, loss=4.25]epoch 2/100: 129it [02:59,  1.78s/it, loss=4.29]epoch 2/100: 130it [02:59,  1.36s/it, loss=4.29]epoch 2/100: 130it [02:59,  1.36s/it, loss=4.25]epoch 2/100: 130it [03:00,  1.36s/it, loss=4.24]epoch 2/100: 130it [03:00,  1.36s/it, loss=4.05]epoch 2/100: 131it [03:00,  1.07s/it, loss=4.24]epoch 2/100: 131it [03:00,  1.07s/it, loss=4.05]epoch 2/100: 131it [03:00,  1.07s/it, loss=4.25]epoch 2/100: 131it [03:00,  1.07s/it, loss=4.35]epoch 2/100: 132it [03:00,  1.16it/s, loss=4.25]epoch 2/100: 132it [03:00,  1.16it/s, loss=4.35]epoch 2/100: 132it [03:00,  1.16it/s, loss=4.33]epoch 2/100: 132it [03:00,  1.16it/s, loss=4.28]epoch 2/100: 133it [03:00,  1.39it/s, loss=4.33]epoch 2/100: 133it [03:00,  1.39it/s, loss=4.28]epoch 2/100: 133it [03:01,  1.39it/s, loss=4.23]epoch 2/100: 133it [03:01,  1.39it/s, loss=4.16]epoch 2/100: 134it [03:01,  1.61it/s, loss=4.16]epoch 2/100: 134it [03:01,  1.61it/s, loss=4.23]epoch 2/100: 134it [03:01,  1.61it/s, loss=4.46]epoch 2/100: 134it [03:01,  1.61it/s, loss=4.24]epoch 2/100: 135it [03:01,  1.82it/s, loss=4.46]epoch 2/100: 135it [03:01,  1.82it/s, loss=4.24]epoch 2/100: 135it [03:01,  1.82it/s, loss=4.24]epoch 2/100: 135it [03:01,  1.82it/s, loss=4.38]epoch 2/100: 136it [03:01,  2.00it/s, loss=4.38]epoch 2/100: 136it [03:01,  2.00it/s, loss=4.24]epoch 2/100: 136it [03:02,  2.00it/s, loss=4.16]epoch 2/100: 136it [03:02,  2.00it/s, loss=4.31]epoch 2/100: 137it [03:02,  2.15it/s, loss=4.31]epoch 2/100: 137it [03:02,  2.15it/s, loss=4.16]epoch 2/100: 137it [03:02,  2.15it/s, loss=4.02]epoch 2/100: 137it [03:02,  2.15it/s, loss=4.09]epoch 2/100: 138it [03:02,  2.27it/s, loss=4.02]epoch 2/100: 138it [03:02,  2.27it/s, loss=4.09]epoch 2/100: 138it [03:03,  2.27it/s, loss=4.25]epoch 2/100: 138it [03:03,  2.27it/s, loss=4.15]epoch 2/100: 139it [03:03,  2.36it/s, loss=4.25]epoch 2/100: 139it [03:03,  2.36it/s, loss=4.15]epoch 2/100: 139it [03:03,  2.36it/s, loss=4.29]epoch 2/100: 139it [03:03,  2.36it/s, loss=4.31]epoch 2/100: 140it [03:03,  2.43it/s, loss=4.29]epoch 2/100: 140it [03:03,  2.43it/s, loss=4.31]epoch 2/100: 140it [03:03,  2.43it/s, loss=4.21]epoch 2/100: 140it [03:03,  2.43it/s, loss=4.2] epoch 2/100: 141it [03:03,  2.48it/s, loss=4.21]epoch 2/100: 141it [03:03,  2.48it/s, loss=4.2]epoch 2/100: 141it [03:04,  2.48it/s, loss=4.23]epoch 2/100: 141it [03:04,  2.48it/s, loss=4.35]epoch 2/100: 142it [03:04,  2.52it/s, loss=4.35]epoch 2/100: 142it [03:04,  2.52it/s, loss=4.23]epoch 2/100: 142it [03:04,  2.52it/s, loss=4.43]epoch 2/100: 142it [03:04,  2.52it/s, loss=4.17]epoch 2/100: 143it [03:04,  2.54it/s, loss=4.43]epoch 2/100: 143it [03:04,  2.54it/s, loss=4.17]epoch 2/100: 143it [03:05,  2.54it/s, loss=4]   epoch 2/100: 143it [03:05,  2.54it/s, loss=4.19]epoch 2/100: 144it [03:05,  2.56it/s, loss=4]epoch 2/100: 144it [03:05,  2.56it/s, loss=4.19]epoch 2/100: 144it [03:05,  2.56it/s, loss=4.35]epoch 2/100: 144it [03:05,  2.56it/s, loss=4.2] epoch 2/100: 145it [03:05,  2.57it/s, loss=4.35]epoch 2/100: 145it [03:05,  2.57it/s, loss=4.2]epoch 2/100: 145it [03:05,  2.57it/s, loss=4.28]epoch 2/100: 145it [03:05,  2.57it/s, loss=4.18]epoch 2/100: 146it [03:05,  2.58it/s, loss=4.28]epoch 2/100: 146it [03:05,  2.58it/s, loss=4.18]epoch 2/100: 146it [03:06,  2.58it/s, loss=4.31]epoch 2/100: 146it [03:06,  2.58it/s, loss=4.11]epoch 2/100: 147it [03:06,  2.59it/s, loss=4.11]epoch 2/100: 147it [03:06,  2.59it/s, loss=4.31]epoch 2/100: 147it [03:06,  2.59it/s, loss=4.3] epoch 2/100: 147it [03:06,  2.59it/s, loss=4.17]epoch 2/100: 148it [03:06,  2.59it/s, loss=4.3]epoch 2/100: 148it [03:06,  2.59it/s, loss=4.17]epoch 2/100: 148it [03:06,  2.59it/s, loss=4.05]epoch 2/100: 148it [03:06,  2.59it/s, loss=4.42]epoch 2/100: 149it [03:06,  2.59it/s, loss=4.05]epoch 2/100: 149it [03:06,  2.59it/s, loss=4.42]epoch 2/100: 149it [03:07,  2.59it/s, loss=4.05]epoch 2/100: 149it [03:07,  2.59it/s, loss=4.18]

val: 0it [00:00, ?it/s][Aval: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:04,  4.52s/it][A
val: 1it [00:04,  4.52s/it][A
val: 2it [00:04,  2.01s/it][A
val: 2it [00:04,  2.01s/it][A
val: 3it [00:05,  1.21s/it][A
val: 3it [00:05,  1.21s/it][A
val: 4it [00:05,  1.21it/s][A
val: 4it [00:05,  1.21it/s][A
val: 5it [00:05,  1.61it/s][A
val: 5it [00:05,  1.61it/s][A
val: 6it [00:05,  2.02it/s][A
val: 6it [00:05,  2.02it/s][A
val: 7it [00:06,  2.41it/s][A
val: 7it [00:06,  2.41it/s][A
val: 8it [00:06,  2.76it/s][A
val: 8it [00:06,  2.76it/s][A
val: 9it [00:06,  3.05it/s][A
val: 9it [00:06,  3.05it/s][A
val: 10it [00:06,  3.29it/s][A
val: 10it [00:06,  3.29it/s][A
val: 11it [00:07,  3.47it/s][A
val: 11it [00:07,  3.46it/s][A
val: 12it [00:07,  3.62it/s][A
val: 12it [00:07,  3.60it/s][A
val: 13it [00:07,  3.71it/s][A
val: 13it [00:07,  3.71it/s][A
val: 14it [00:07,  3.79it/s][A
val: 14it [00:07,  3.79it/s][A
val: 15it [00:08,  3.85it/s][A
val: 15it [00:08,  3.84it/s][A
val: 16it [00:08,  3.88it/s][A
val: 16it [00:08,  3.89it/s][A
val: 17it [00:08,  3.91it/s][A
val: 17it [00:08,  3.91it/s][A
val: 18it [00:08,  3.93it/s][A
val: 18it [00:08,  3.94it/s][A
val: 19it [00:09,  3.95it/s][A
val: 19it [00:09,  3.95it/s][A
val: 20it [00:09,  3.95it/s][A
val: 20it [00:09,  3.96it/s][A
val: 21it [00:09,  3.96it/s][A
val: 21it [00:09,  3.97it/s][A
val: 22it [00:09,  3.96it/s][A
val: 22it [00:09,  3.97it/s][A
val: 23it [00:10,  3.98it/s][A
val: 23it [00:10,  3.97it/s][A
val: 24it [00:10,  3.98it/s][A
val: 24it [00:10,  3.97it/s][A
                            [A
                            [Aepoch 2/100: 149it [03:17,  2.59it/s, loss=4.18]epoch 2/100: 149it [03:26,  2.59it/s, best_val=4.19, loss=4.05, val=4.19]epoch 2/100: 150it [03:26,  6.03s/it, best_val=4.19, loss=4.05, val=4.19]epoch 2/100: 150it [03:26,  6.03s/it, loss=4.18]epoch 2/100: 150it [03:26,  6.03s/it, loss=4.11]epoch 2/100: 150it [03:26,  6.03s/it, loss=4.4]                          epoch 2/100: 151it [03:26,  4.33s/it, loss=4.11]epoch 2/100: 151it [03:26,  4.33s/it, loss=4.4]epoch 2/100: 151it [03:26,  4.33s/it, loss=4.33]epoch 2/100: 151it [03:26,  4.33s/it, loss=4.27]epoch 2/100: 152it [03:26,  3.15s/it, loss=4.33]epoch 2/100: 152it [03:26,  3.15s/it, loss=4.27]epoch 2/100: 152it [03:27,  3.15s/it, loss=4.02]epoch 2/100: 152it [03:27,  3.15s/it, loss=4.11]epoch 2/100: 153it [03:27,  2.32s/it, loss=4.11]epoch 2/100: 153it [03:27,  2.32s/it, loss=4.02]epoch 2/100: 153it [03:27,  2.32s/it, loss=4.06]epoch 2/100: 153it [03:27,  2.32s/it, loss=4.3] epoch 2/100: 154it [03:27,  1.74s/it, loss=4.06]epoch 2/100: 154it [03:27,  1.74s/it, loss=4.3]epoch 2/100: 154it [03:28,  1.74s/it, loss=4.2] epoch 2/100: 154it [03:28,  1.74s/it, loss=4]  epoch 2/100: 155it [03:28,  1.33s/it, loss=4.2]epoch 2/100: 155it [03:28,  1.33s/it, loss=4]epoch 2/100: 155it [03:28,  1.33s/it, loss=4.44]epoch 2/100: 155it [03:28,  1.33s/it, loss=4.1]epoch 2/100: 156it [03:28,  1.05s/it, loss=4.1]epoch 2/100: 156it [03:28,  1.05s/it, loss=4.44]epoch 2/100: 156it [03:28,  1.05s/it, loss=4.32]epoch 2/100: 156it [03:28,  1.05s/it, loss=4.29]epoch 2/100: 157it [03:28,  1.18it/s, loss=4.32]epoch 2/100: 157it [03:28,  1.18it/s, loss=4.29]epoch 2/100: 157it [03:29,  1.18it/s, loss=4.12]epoch 2/100: 157it [03:29,  1.18it/s, loss=4.04]epoch 2/100: 158it [03:29,  1.40it/s, loss=4.12]epoch 2/100: 158it [03:29,  1.40it/s, loss=4.04]epoch 2/100: 158it [03:29,  1.40it/s, loss=4.22]epoch 2/100: 158it [03:29,  1.40it/s, loss=4.1] epoch 2/100: 159it [03:29,  1.63it/s, loss=4.22]epoch 2/100: 159it [03:29,  1.63it/s, loss=4.1]epoch 2/100: 159it [03:30,  1.63it/s, loss=4.28]epoch 2/100: 159it [03:30,  1.63it/s, loss=4.17]epoch 2/100: 160it [03:30,  1.83it/s, loss=4.28]epoch 2/100: 160it [03:30,  1.83it/s, loss=4.17]epoch 2/100: 160it [03:30,  1.83it/s, loss=4.11]epoch 2/100: 160it [03:30,  1.83it/s, loss=4.05]epoch 2/100: 161it [03:30,  2.01it/s, loss=4.05]epoch 2/100: 161it [03:30,  2.01it/s, loss=4.11]epoch 2/100: 161it [03:30,  2.01it/s, loss=4.04]epoch 2/100: 161it [03:30,  2.01it/s, loss=4.3] epoch 2/100: 162it [03:30,  2.15it/s, loss=4.04]epoch 2/100: 162it [03:30,  2.15it/s, loss=4.3]epoch 2/100: 162it [03:31,  2.15it/s, loss=4.32]epoch 2/100: 162it [03:31,  2.15it/s, loss=4.33]epoch 2/100: 163it [03:31,  2.27it/s, loss=4.33]epoch 2/100: 163it [03:31,  2.27it/s, loss=4.32]epoch 2/100: 163it [03:31,  2.27it/s, loss=4.1] epoch 2/100: 163it [03:31,  2.27it/s, loss=4.14]epoch 2/100: 164it [03:31,  2.36it/s, loss=4.1]epoch 2/100: 164it [03:31,  2.36it/s, loss=4.14]epoch 2/100: 164it [03:31,  2.36it/s, loss=4.3] epoch 2/100: 164it [03:31,  2.36it/s, loss=4.32]epoch 2/100: 165it [03:31,  2.42it/s, loss=4.32]epoch 2/100: 165it [03:31,  2.42it/s, loss=4.3]epoch 2/100: 165it [03:32,  2.42it/s, loss=4.25]epoch 2/100: 165it [03:32,  2.42it/s, loss=4.24]epoch 2/100: 166it [03:32,  2.47it/s, loss=4.24]epoch 2/100: 166it [03:32,  2.47it/s, loss=4.25]epoch 2/100: 166it [03:32,  2.47it/s, loss=4.06]epoch 2/100: 166it [03:32,  2.47it/s, loss=4.01]epoch 2/100: 167it [03:32,  2.51it/s, loss=4.06]epoch 2/100: 167it [03:32,  2.51it/s, loss=4.01]epoch 2/100: 167it [03:33,  2.51it/s, loss=4.39]epoch 2/100: 167it [03:33,  2.51it/s, loss=4.15]epoch 2/100: 168it [03:33,  2.54it/s, loss=4.15]epoch 2/100: 168it [03:33,  2.54it/s, loss=4.39]epoch 2/100: 168it [03:33,  2.54it/s, loss=4.15]epoch 2/100: 168it [03:33,  2.54it/s, loss=4.16]epoch 2/100: 169it [03:33,  2.55it/s, loss=4.15]epoch 2/100: 169it [03:33,  2.55it/s, loss=4.16]epoch 2/100: 169it [03:33,  2.55it/s, loss=4.23]epoch 2/100: 169it [03:33,  2.55it/s, loss=4.65]epoch 2/100: 170it [03:33,  2.57it/s, loss=4.23]epoch 2/100: 170it [03:33,  2.57it/s, loss=4.65]epoch 2/100: 170it [03:34,  2.57it/s, loss=4.28]epoch 2/100: 170it [03:34,  2.57it/s, loss=4.1] epoch 2/100: 171it [03:34,  2.57it/s, loss=4.28]epoch 2/100: 171it [03:34,  2.57it/s, loss=4.1]epoch 2/100: 171it [03:34,  2.57it/s, loss=4.03]epoch 2/100: 171it [03:34,  2.57it/s, loss=4.14]epoch 2/100: 172it [03:34,  2.58it/s, loss=4.14]epoch 2/100: 172it [03:34,  2.58it/s, loss=4.03]epoch 2/100: 172it [03:35,  2.58it/s, loss=4.51]epoch 2/100: 172it [03:35,  2.58it/s, loss=4.15]epoch 2/100: 173it [03:35,  2.59it/s, loss=4.51]epoch 2/100: 173it [03:35,  2.59it/s, loss=4.15]epoch 2/100: 173it [03:35,  2.59it/s, loss=4.31]epoch 2/100: 173it [03:35,  2.59it/s, loss=4.24]epoch 2/100: 174it [03:35,  2.59it/s, loss=4.24]epoch 2/100: 174it [03:35,  2.59it/s, loss=4.31]epoch 2/100: 174it [03:35,  2.59it/s, loss=4.36]epoch 2/100: 174it [03:35,  2.59it/s, loss=4.27]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.59s/it][A
val: 1it [00:03,  3.59s/it][A
val: 2it [00:03,  1.62s/it][A
val: 3it [00:04,  1.00it/s][A
val: 4it [00:04,  1.42it/s][A
val: 5it [00:04,  1.85it/s][A
val: 6it [00:04,  2.27it/s][A
val: 7it [00:05,  2.64it/s][A
val: 8it [00:05,  2.95it/s][A
val: 9it [00:05,  3.21it/s][A
val: 10it [00:05,  3.41it/s][A
val: 2it [00:05,  2.84s/it][A
val: 11it [00:06,  3.56it/s][A
val: 3it [00:06,  1.66s/it][A
val: 12it [00:06,  3.68it/s][A
val: 4it [00:06,  1.10s/it][A
val: 13it [00:06,  3.76it/s][A
val: 5it [00:06,  1.26it/s][A
val: 14it [00:06,  3.82it/s][A
val: 6it [00:06,  1.64it/s][A
val: 15it [00:07,  3.86it/s][A
val: 7it [00:07,  2.03it/s][A
val: 16it [00:07,  3.89it/s][A
val: 8it [00:07,  2.41it/s][A
val: 17it [00:07,  3.91it/s][A
val: 9it [00:07,  2.75it/s][A
val: 18it [00:07,  3.93it/s][A
val: 10it [00:07,  3.04it/s][A
val: 19it [00:08,  3.94it/s][A
val: 11it [00:08,  3.28it/s][A
val: 20it [00:08,  3.95it/s][A
val: 12it [00:08,  3.47it/s][A
val: 21it [00:08,  3.96it/s][A
val: 13it [00:08,  3.61it/s][A
val: 22it [00:08,  3.96it/s][A
val: 14it [00:08,  3.72it/s][A
val: 23it [00:09,  3.97it/s][A
val: 15it [00:09,  3.80it/s][A
val: 24it [00:09,  3.97it/s][A
val: 16it [00:09,  3.86it/s][A
                            [A
val: 17it [00:09,  3.90it/s][A
val: 18it [00:09,  3.92it/s][A
val: 19it [00:10,  3.95it/s][A
val: 20it [00:10,  3.96it/s][A
val: 21it [00:10,  3.97it/s][A
val: 22it [00:10,  3.98it/s][A
val: 23it [00:11,  3.99it/s][A
val: 24it [00:11,  4.00it/s][A
                            [Aepoch 2/100: 174it [03:47,  2.59it/s, loss=4.36]epoch 2/100: 174it [03:56,  2.59it/s, best_val=4.17, loss=4.27, val=4.17]epoch 2/100: 175it [03:56,  6.71s/it, loss=4.36]epoch 2/100: 175it [03:56,  6.71s/it, best_val=4.17, loss=4.27, val=4.17]epoch 2/100: 175it [03:57,  6.71s/it, loss=4.35]epoch 2/100: 175it [03:57,  6.71s/it, loss=4.33]                         epoch 2/100: 176it [03:57,  4.81s/it, loss=4.35]epoch 2/100: 176it [03:57,  4.81s/it, loss=4.33]epoch 2/100: 176it [03:57,  4.81s/it, loss=4.21]epoch 2/100: 176it [03:57,  4.81s/it, loss=4.37]epoch 2/100: 177it [03:57,  3.49s/it, loss=4.21]epoch 2/100: 177it [03:57,  3.49s/it, loss=4.37]epoch 2/100: 177it [03:58,  3.49s/it, loss=4.16]epoch 2/100: 177it [03:58,  3.49s/it, loss=4.23]epoch 2/100: 178it [03:58,  2.56s/it, loss=4.16]epoch 2/100: 178it [03:58,  2.56s/it, loss=4.23]epoch 2/100: 178it [03:58,  2.56s/it, loss=4.33]epoch 2/100: 178it [03:58,  2.56s/it, loss=4.02]epoch 2/100: 179it [03:58,  1.90s/it, loss=4.33]epoch 2/100: 179it [03:58,  1.90s/it, loss=4.02]epoch 2/100: 179it [03:58,  1.90s/it, loss=4.15]epoch 2/100: 179it [03:58,  1.90s/it, loss=4.33]epoch 2/100: 180it [03:58,  1.45s/it, loss=4.15]epoch 2/100: 180it [03:58,  1.45s/it, loss=4.33]epoch 2/100: 180it [03:59,  1.45s/it, loss=4.07]epoch 2/100: 180it [03:59,  1.45s/it, loss=4.21]epoch 2/100: 181it [03:59,  1.13s/it, loss=4.21]epoch 2/100: 181it [03:59,  1.13s/it, loss=4.07]epoch 2/100: 181it [03:59,  1.13s/it, loss=4.12]epoch 2/100: 181it [03:59,  1.13s/it, loss=3.97]epoch 2/100: 182it [03:59,  1.10it/s, loss=4.12]epoch 2/100: 182it [03:59,  1.10it/s, loss=3.97]epoch 2/100: 182it [03:59,  1.10it/s, loss=4.19]epoch 2/100: 182it [03:59,  1.10it/s, loss=4.31]epoch 2/100: 183it [03:59,  1.33it/s, loss=4.19]epoch 2/100: 183it [03:59,  1.33it/s, loss=4.31]epoch 2/100: 183it [04:00,  1.33it/s, loss=4.25]epoch 2/100: 183it [04:00,  1.33it/s, loss=4.38]epoch 2/100: 184it [04:00,  1.56it/s, loss=4.25]epoch 2/100: 184it [04:00,  1.56it/s, loss=4.38]epoch 2/100: 184it [04:00,  1.56it/s, loss=4.38]epoch 2/100: 184it [04:00,  1.56it/s, loss=4.31]epoch 2/100: 185it [04:00,  1.78it/s, loss=4.38]epoch 2/100: 185it [04:00,  1.78it/s, loss=4.31]epoch 2/100: 185it [04:01,  1.78it/s, loss=4.16]epoch 2/100: 185it [04:01,  1.78it/s, loss=4.12]epoch 2/100: 186it [04:01,  1.96it/s, loss=4.16]epoch 2/100: 186it [04:01,  1.96it/s, loss=4.12]epoch 2/100: 186it [04:01,  1.96it/s, loss=4.34]epoch 2/100: 186it [04:01,  1.96it/s, loss=4.54]epoch 2/100: 187it [04:01,  2.12it/s, loss=4.34]epoch 2/100: 187it [04:01,  2.12it/s, loss=4.54]epoch 2/100: 187it [04:01,  2.12it/s, loss=4.1] epoch 2/100: 187it [04:01,  2.12it/s, loss=4.16]epoch 2/100: 188it [04:01,  2.24it/s, loss=4.16]epoch 2/100: 188it [04:01,  2.24it/s, loss=4.1]epoch 2/100: 188it [04:02,  2.24it/s, loss=4.27]epoch 2/100: 188it [04:02,  2.24it/s, loss=4.16]epoch 2/100: 189it [04:02,  2.34it/s, loss=4.16]epoch 2/100: 189it [04:02,  2.34it/s, loss=4.27]epoch 2/100: 189it [04:02,  2.34it/s, loss=4.13]epoch 2/100: 189it [04:02,  2.34it/s, loss=4.18]epoch 2/100: 190it [04:02,  2.41it/s, loss=4.13]epoch 2/100: 190it [04:02,  2.41it/s, loss=4.18]epoch 2/100: 190it [04:03,  2.41it/s, loss=4.03]epoch 2/100: 190it [04:03,  2.41it/s, loss=4.18]epoch 2/100: 191it [04:03,  2.47it/s, loss=4.03]epoch 2/100: 191it [04:03,  2.47it/s, loss=4.18]epoch 2/100: 191it [04:03,  2.47it/s, loss=4.22]epoch 2/100: 191it [04:03,  2.47it/s, loss=4.12]epoch 2/100: 192it [04:03,  2.51it/s, loss=4.22]epoch 2/100: 192it [04:03,  2.51it/s, loss=4.12]epoch 2/100: 192it [04:03,  2.51it/s, loss=4.17]epoch 2/100: 192it [04:03,  2.51it/s, loss=4.45]epoch 2/100: 193it [04:03,  2.53it/s, loss=4.45]epoch 2/100: 193it [04:03,  2.53it/s, loss=4.17]epoch 2/100: 193it [04:04,  2.53it/s, loss=4.25]epoch 2/100: 193it [04:04,  2.53it/s, loss=4.25]epoch 2/100: 194it [04:04,  2.55it/s, loss=4.25]epoch 2/100: 194it [04:04,  2.56it/s, loss=4.25]epoch 2/100: 194it [04:04,  2.55it/s, loss=4.2] epoch 2/100: 194it [04:04,  2.56it/s, loss=4.38]epoch 2/100: 195it [04:04,  2.57it/s, loss=4.2]epoch 2/100: 195it [04:04,  2.57it/s, loss=4.38]epoch 2/100: 195it [04:04,  2.57it/s, loss=4.07]epoch 2/100: 195it [04:04,  2.57it/s, loss=4.14]epoch 2/100: 196it [04:04,  2.58it/s, loss=4.14]epoch 2/100: 196it [04:04,  2.58it/s, loss=4.07]epoch 2/100: 196it [04:05,  2.58it/s, loss=4.3] epoch 2/100: 196it [04:05,  2.58it/s, loss=4.31]epoch 2/100: 197it [04:05,  2.58it/s, loss=4.3]epoch 2/100: 197it [04:05,  2.58it/s, loss=4.31]epoch 2/100: 197it [04:05,  2.58it/s, loss=4.11]epoch 2/100: 197it [04:05,  2.58it/s, loss=4.42]epoch 2/100: 198it [04:05,  2.59it/s, loss=4.11]epoch 2/100: 198it [04:05,  2.59it/s, loss=4.42]epoch 2/100: 198it [04:06,  2.59it/s, loss=4.19]epoch 2/100: 198it [04:06,  2.59it/s, loss=4.15]epoch 2/100: 199it [04:06,  2.59it/s, loss=4.15]epoch 2/100: 199it [04:06,  2.59it/s, loss=4.19]epoch 2/100: 199it [04:06,  2.59it/s, loss=4.24]epoch 2/100: 199it [04:06,  2.59it/s, loss=4]   
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.50s/it][A
val: 2it [00:04,  1.77s/it][A
val: 3it [00:04,  1.08s/it][A
val: 4it [00:04,  1.33it/s][A
val: 5it [00:04,  1.75it/s][A
val: 1it [00:05,  5.06s/it][A
val: 6it [00:05,  2.16it/s][A
val: 2it [00:05,  2.23s/it][A
val: 7it [00:05,  2.55it/s][A
val: 3it [00:05,  1.33s/it][A
val: 8it [00:05,  2.88it/s][A
val: 4it [00:05,  1.11it/s][A
val: 9it [00:05,  3.15it/s][A
val: 5it [00:06,  1.50it/s][A
val: 10it [00:06,  3.37it/s][A
val: 11it [00:06,  3.53it/s][A
val: 6it [00:06,  1.90it/s][A
val: 12it [00:06,  3.66it/s][A
val: 7it [00:06,  2.29it/s][A
val: 13it [00:06,  3.75it/s][A
val: 8it [00:06,  2.65it/s][A
val: 14it [00:07,  3.82it/s][A
val: 9it [00:07,  2.96it/s][A
val: 15it [00:07,  3.87it/s][A
val: 10it [00:07,  3.20it/s][A
val: 16it [00:07,  3.91it/s][A
val: 11it [00:07,  3.40it/s][A
val: 17it [00:07,  3.93it/s][A
val: 12it [00:07,  3.56it/s][A
val: 18it [00:08,  3.95it/s][A
val: 13it [00:08,  3.67it/s][A
val: 19it [00:08,  3.96it/s][A
val: 14it [00:08,  3.76it/s][A
val: 20it [00:08,  3.97it/s][A
val: 15it [00:08,  3.82it/s][A
val: 21it [00:08,  3.98it/s][A
val: 16it [00:08,  3.86it/s][A
val: 22it [00:09,  3.98it/s][A
val: 17it [00:09,  3.89it/s][A
val: 23it [00:09,  3.99it/s][A
val: 18it [00:09,  3.91it/s][A
val: 24it [00:09,  3.99it/s][A
val: 19it [00:09,  3.93it/s][A
                            [A
val: 20it [00:09,  3.94it/s][A
val: 21it [00:10,  3.95it/s][A
val: 22it [00:10,  3.96it/s][A
val: 23it [00:10,  3.97it/s][A
val: 24it [00:10,  3.97it/s][A
                            [Aepoch 2/100: 199it [04:17,  2.59it/s, loss=4.24]epoch 2/100: 199it [04:26,  2.59it/s, best_val=4.15, loss=4, val=4.15]epoch 2/100: 200it [04:26,  6.42s/it, loss=4.24]epoch 2/100: 200it [04:26,  6.42s/it, best_val=4.15, loss=4, val=4.15]epoch 2/100: 200it [04:26,  6.42s/it, loss=4.33]epoch 2/100: 200it [04:27,  6.42s/it, loss=3.94]                      epoch 2/100: 201it [04:26,  4.61s/it, loss=4.33]epoch 2/100: 201it [04:27,  4.61s/it, loss=3.94]epoch 2/100: 201it [04:27,  4.61s/it, loss=4.06]epoch 2/100: 201it [04:27,  4.61s/it, loss=4.22]epoch 2/100: 202it [04:27,  3.35s/it, loss=4.06]epoch 2/100: 202it [04:27,  3.35s/it, loss=4.22]epoch 2/100: 202it [04:27,  3.35s/it, loss=4.28]epoch 2/100: 202it [04:27,  3.35s/it, loss=4.1] epoch 2/100: 203it [04:27,  2.46s/it, loss=4.28]epoch 2/100: 203it [04:27,  2.46s/it, loss=4.1]epoch 2/100: 203it [04:28,  2.46s/it, loss=4.21]epoch 2/100: 203it [04:28,  2.46s/it, loss=4.21]epoch 2/100: 204it [04:28,  1.84s/it, loss=4.21]epoch 2/100: 204it [04:28,  1.84s/it, loss=4.21]epoch 2/100: 204it [04:28,  1.84s/it, loss=4.13]epoch 2/100: 204it [04:28,  1.84s/it, loss=4.24]epoch 2/100: 205it [04:28,  1.40s/it, loss=4.24]epoch 2/100: 205it [04:28,  1.40s/it, loss=4.13]epoch 2/100: 205it [04:28,  1.40s/it, loss=3.94]epoch 2/100: 205it [04:28,  1.40s/it, loss=4.03]epoch 2/100: 206it [04:28,  1.10s/it, loss=3.94]epoch 2/100: 206it [04:28,  1.10s/it, loss=4.03]epoch 2/100: 206it [04:29,  1.10s/it, loss=4.16]epoch 2/100: 206it [04:29,  1.10s/it, loss=3.94]epoch 2/100: 207it [04:29,  1.13it/s, loss=4.16]epoch 2/100: 207it [04:29,  1.13it/s, loss=3.94]epoch 2/100: 207it [04:29,  1.13it/s, loss=4.02]epoch 2/100: 207it [04:29,  1.13it/s, loss=4.11]epoch 2/100: 208it [04:29,  1.36it/s, loss=4.02]epoch 2/100: 208it [04:29,  1.36it/s, loss=4.11]epoch 2/100: 208it [04:30,  1.36it/s, loss=4.33]epoch 2/100: 208it [04:30,  1.36it/s, loss=4.06]epoch 2/100: 209it [04:30,  1.59it/s, loss=4.33]epoch 2/100: 209it [04:30,  1.59it/s, loss=4.06]epoch 2/100: 209it [04:30,  1.59it/s, loss=4.13]epoch 2/100: 209it [04:30,  1.59it/s, loss=4.27]epoch 2/100: 210it [04:30,  1.80it/s, loss=4.13]epoch 2/100: 210it [04:30,  1.80it/s, loss=4.27]epoch 2/100: 210it [04:30,  1.80it/s, loss=4.19]epoch 2/100: 210it [04:30,  1.80it/s, loss=4.23]epoch 2/100: 211it [04:30,  1.98it/s, loss=4.19]epoch 2/100: 211it [04:30,  1.98it/s, loss=4.23]epoch 2/100: 211it [04:31,  1.98it/s, loss=4.25]epoch 2/100: 211it [04:31,  1.98it/s, loss=3.99]epoch 2/100: 212it [04:31,  2.13it/s, loss=4.25]epoch 2/100: 212it [04:31,  2.13it/s, loss=3.99]epoch 2/100: 212it [04:31,  2.13it/s, loss=4.22]epoch 2/100: 212it [04:31,  2.13it/s, loss=4.23]epoch 2/100: 213it [04:31,  2.26it/s, loss=4.23]epoch 2/100: 213it [04:31,  2.26it/s, loss=4.22]epoch 2/100: 213it [04:32,  2.26it/s, loss=4.19]epoch 2/100: 213it [04:32,  2.26it/s, loss=4.16]epoch 2/100: 214it [04:32,  2.35it/s, loss=4.19]epoch 2/100: 214it [04:32,  2.35it/s, loss=4.16]epoch 2/100: 214it [04:32,  2.35it/s, loss=4.17]epoch 2/100: 214it [04:32,  2.35it/s, loss=4.12]epoch 2/100: 215it [04:32,  2.42it/s, loss=4.17]epoch 2/100: 215it [04:32,  2.42it/s, loss=4.12]epoch 2/100: 215it [04:32,  2.42it/s, loss=4.07]epoch 2/100: 215it [04:32,  2.42it/s, loss=4.15]epoch 2/100: 216it [04:32,  2.47it/s, loss=4.07]epoch 2/100: 216it [04:32,  2.47it/s, loss=4.15]epoch 2/100: 216it [04:33,  2.47it/s, loss=4.18]epoch 2/100: 216it [04:33,  2.47it/s, loss=4.14]epoch 2/100: 217it [04:33,  2.51it/s, loss=4.18]epoch 2/100: 217it [04:33,  2.51it/s, loss=4.14]epoch 2/100: 217it [04:33,  2.51it/s, loss=4.2] epoch 2/100: 217it [04:33,  2.51it/s, loss=4.08]epoch 2/100: 218it [04:33,  2.53it/s, loss=4.2]epoch 2/100: 218it [04:33,  2.53it/s, loss=4.08]epoch 2/100: 218it [04:33,  2.53it/s, loss=4.18]epoch 2/100: 218it [04:33,  2.53it/s, loss=4.16]epoch 2/100: 219it [04:33,  2.55it/s, loss=4.16]epoch 2/100: 219it [04:33,  2.55it/s, loss=4.18]epoch 2/100: 219it [04:34,  2.55it/s, loss=4.17]epoch 2/100: 219it [04:34,  2.55it/s, loss=4.21]epoch 2/100: 220it [04:34,  2.57it/s, loss=4.21]epoch 2/100: 220it [04:34,  2.57it/s, loss=4.17]epoch 2/100: 220it [04:34,  2.57it/s, loss=4.26]epoch 2/100: 220it [04:34,  2.57it/s, loss=4.09]epoch 2/100: 221it [04:34,  2.58it/s, loss=4.09]epoch 2/100: 221it [04:34,  2.58it/s, loss=4.26]epoch 2/100: 221it [04:35,  2.58it/s, loss=4.17]epoch 2/100: 221it [04:35,  2.58it/s, loss=4.21]epoch 2/100: 222it [04:35,  2.58it/s, loss=4.17]epoch 2/100: 222it [04:35,  2.58it/s, loss=4.21]epoch 2/100: 222it [04:35,  2.58it/s, loss=4.12]epoch 2/100: 222it [04:35,  2.58it/s, loss=3.92]epoch 2/100: 223it [04:35,  2.59it/s, loss=4.12]epoch 2/100: 223it [04:35,  2.59it/s, loss=3.92]epoch 2/100: 223it [04:35,  2.59it/s, loss=4.27]epoch 2/100: 223it [04:35,  2.59it/s, loss=4.21]epoch 2/100: 224it [04:35,  2.59it/s, loss=4.27]epoch 2/100: 224it [04:35,  2.59it/s, loss=4.21]epoch 2/100: 224it [04:36,  2.59it/s, loss=4.12]epoch 2/100: 224it [04:36,  2.59it/s, loss=4.23]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.32s/it][A
val: 1it [00:03,  3.32s/it][A
val: 2it [00:03,  1.52s/it][A
val: 2it [00:03,  1.52s/it][A
val: 3it [00:03,  1.07it/s][A
val: 3it [00:03,  1.06it/s][A
val: 4it [00:04,  1.50it/s][A
val: 4it [00:04,  1.50it/s][A
val: 5it [00:04,  1.93it/s][A
val: 5it [00:04,  1.93it/s][A
val: 6it [00:04,  2.35it/s][A
val: 6it [00:04,  2.34it/s][A
val: 7it [00:04,  2.71it/s][A
val: 7it [00:04,  2.70it/s][A
val: 8it [00:05,  3.02it/s][A
val: 8it [00:05,  3.00it/s][A
val: 9it [00:05,  3.27it/s][A
val: 9it [00:05,  3.24it/s][A
val: 10it [00:05,  3.46it/s][A
val: 10it [00:05,  3.44it/s][A
val: 11it [00:05,  3.60it/s][A
val: 11it [00:05,  3.58it/s][A
val: 12it [00:06,  3.71it/s][A
val: 12it [00:06,  3.68it/s][A
val: 13it [00:06,  3.79it/s][A
val: 13it [00:06,  3.76it/s][A
val: 14it [00:06,  3.85it/s][A
val: 14it [00:06,  3.82it/s][A
val: 15it [00:06,  3.89it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:07,  3.92it/s][A
val: 16it [00:07,  3.89it/s][A
val: 17it [00:07,  3.95it/s][A
val: 17it [00:07,  3.91it/s][A
val: 18it [00:07,  3.96it/s][A
val: 18it [00:07,  3.93it/s][A
val: 19it [00:07,  3.97it/s][A
val: 19it [00:07,  3.94it/s][A
val: 20it [00:08,  3.98it/s][A
val: 20it [00:08,  3.94it/s][A
val: 21it [00:08,  3.98it/s][A
val: 21it [00:08,  3.95it/s][A
val: 22it [00:08,  3.98it/s][A
val: 22it [00:08,  3.96it/s][A
val: 23it [00:08,  3.99it/s][A
val: 23it [00:08,  3.96it/s][A
val: 24it [00:09,  3.99it/s][A
val: 24it [00:09,  3.96it/s][A
                            [A
                            [Aepoch 2/100: 224it [04:45,  2.59it/s, loss=4.23]epoch 2/100: 224it [04:54,  2.59it/s, best_val=4.14, loss=4.12, val=4.14]epoch 2/100: 225it [04:54,  5.98s/it, loss=4.23]epoch 2/100: 225it [04:54,  5.98s/it, best_val=4.14, loss=4.12, val=4.14]epoch 2/100: 225it [04:55,  5.98s/it, loss=4.1] epoch 2/100: 225it [04:55,  5.98s/it, loss=4.28]                         epoch 2/100: 226it [04:55,  4.31s/it, loss=4.1]epoch 2/100: 226it [04:55,  4.31s/it, loss=4.28]epoch 2/100: 226it [04:55,  4.31s/it, loss=4.11]epoch 2/100: 226it [04:55,  4.31s/it, loss=4.16]epoch 2/100: 227it [04:55,  3.13s/it, loss=4.16]epoch 2/100: 227it [04:55,  3.13s/it, loss=4.11]epoch 2/100: 227it [04:56,  3.13s/it, loss=4.18]epoch 2/100: 227it [04:56,  3.13s/it, loss=4.22]epoch 2/100: 228it [04:56,  2.31s/it, loss=4.22]epoch 2/100: 228it [04:56,  2.31s/it, loss=4.18]epoch 2/100: 228it [04:56,  2.31s/it, loss=4.17]epoch 2/100: 228it [04:56,  2.31s/it, loss=4.07]epoch 2/100: 229it [04:56,  1.73s/it, loss=4.17]epoch 2/100: 229it [04:56,  1.73s/it, loss=4.07]epoch 2/100: 229it [04:56,  1.73s/it, loss=4.11]epoch 2/100: 229it [04:56,  1.73s/it, loss=4.21]epoch 2/100: 230it [04:56,  1.33s/it, loss=4.11]epoch 2/100: 230it [04:56,  1.33s/it, loss=4.21]epoch 2/100: 230it [04:57,  1.33s/it, loss=4.33]epoch 2/100: 230it [04:57,  1.33s/it, loss=4.09]epoch 2/100: 231it [04:57,  1.04s/it, loss=4.33]epoch 2/100: 231it [04:57,  1.04s/it, loss=4.09]epoch 2/100: 231it [04:57,  1.04s/it, loss=4.04]epoch 2/100: 231it [04:57,  1.04s/it, loss=4.11]epoch 2/100: 232it [04:57,  1.18it/s, loss=4.04]epoch 2/100: 232it [04:57,  1.18it/s, loss=4.11]epoch 2/100: 232it [04:57,  1.18it/s, loss=4.39]epoch 2/100: 232it [04:58,  1.18it/s, loss=4.23]epoch 2/100: 233it [04:57,  1.41it/s, loss=4.39]epoch 2/100: 233it [04:58,  1.41it/s, loss=4.23]epoch 2/100: 233it [04:58,  1.41it/s, loss=3.97]epoch 2/100: 233it [04:58,  1.41it/s, loss=3.94]epoch 2/100: 234it [04:58,  1.64it/s, loss=3.97]epoch 2/100: 234it [04:58,  1.64it/s, loss=3.94]epoch 2/100: 234it [04:58,  1.64it/s, loss=4.22]epoch 2/100: 234it [04:58,  1.64it/s, loss=4.04]epoch 2/100: 235it [04:58,  1.84it/s, loss=4.04]epoch 2/100: 235it [04:58,  1.84it/s, loss=4.22]epoch 2/100: 235it [04:59,  1.84it/s, loss=4.36]epoch 2/100: 235it [04:59,  1.84it/s, loss=4.03]epoch 2/100: 236it [04:59,  2.02it/s, loss=4.36]epoch 2/100: 236it [04:59,  2.02it/s, loss=4.03]epoch 2/100: 236it [04:59,  2.02it/s, loss=4.08]epoch 2/100: 236it [04:59,  2.02it/s, loss=4.26]epoch 2/100: 237it [04:59,  2.16it/s, loss=4.08]epoch 2/100: 237it [04:59,  2.16it/s, loss=4.26]epoch 2/100: 237it [04:59,  2.16it/s, loss=4.15]epoch 2/100: 237it [04:59,  2.16it/s, loss=4.22]epoch 2/100: 238it [04:59,  2.27it/s, loss=4.15]epoch 2/100: 238it [04:59,  2.27it/s, loss=4.22]epoch 2/100: 238it [05:00,  2.27it/s, loss=4.14]epoch 2/100: 238it [05:00,  2.27it/s, loss=4.26]epoch 2/100: 239it [05:00,  2.36it/s, loss=4.14]epoch 2/100: 239it [05:00,  2.36it/s, loss=4.26]epoch 2/100: 239it [05:00,  2.36it/s, loss=4.16]epoch 2/100: 239it [05:00,  2.36it/s, loss=4.28]epoch 2/100: 240it [05:00,  2.42it/s, loss=4.28]epoch 2/100: 240it [05:00,  2.42it/s, loss=4.16]epoch 2/100: 240it [05:01,  2.42it/s, loss=4.15]epoch 2/100: 240it [05:01,  2.42it/s, loss=4.27]epoch 2/100: 241it [05:01,  2.48it/s, loss=4.27]epoch 2/100: 241it [05:01,  2.48it/s, loss=4.15]epoch 2/100: 241it [05:01,  2.48it/s, loss=4.34]epoch 2/100: 241it [05:01,  2.48it/s, loss=4.14]epoch 2/100: 242it [05:01,  2.51it/s, loss=4.34]epoch 2/100: 242it [05:01,  2.51it/s, loss=4.14]epoch 2/100: 242it [05:01,  2.51it/s, loss=4.18]epoch 2/100: 242it [05:01,  2.51it/s, loss=4.18]epoch 2/100: 243it [05:01,  2.54it/s, loss=4.18]epoch 2/100: 243it [05:01,  2.54it/s, loss=4.18]epoch 2/100: 243it [05:02,  2.54it/s, loss=4.18]epoch 2/100: 243it [05:02,  2.54it/s, loss=3.98]epoch 2/100: 244it [05:02,  2.56it/s, loss=4.18]epoch 2/100: 244it [05:02,  2.56it/s, loss=3.98]epoch 2/100: 244it [05:02,  2.56it/s, loss=4.08]epoch 2/100: 244it [05:02,  2.56it/s, loss=4.21]epoch 2/100: 245it [05:02,  2.57it/s, loss=4.21]epoch 2/100: 245it [05:02,  2.57it/s, loss=4.08]epoch 2/100: 245it [05:02,  2.57it/s, loss=4.28]epoch 2/100: 245it [05:03,  2.57it/s, loss=4.19]epoch 2/100: 246it [05:03,  2.58it/s, loss=4.19]epoch 2/100: 246it [05:03,  2.58it/s, loss=4.28]epoch 2/100: 246it [05:03,  2.58it/s, loss=4.15]epoch 2/100: 246it [05:03,  2.58it/s, loss=4.45]epoch 2/100: 247it [05:03,  2.58it/s, loss=4.15]epoch 2/100: 247it [05:03,  2.58it/s, loss=4.45]epoch 2/100: 247it [05:03,  2.58it/s, loss=3.87]epoch 2/100: 247it [05:03,  2.58it/s, loss=3.89]epoch 2/100: 248it [05:03,  2.58it/s, loss=3.87]epoch 2/100: 248it [05:03,  2.58it/s, loss=3.89]epoch 2/100: 248it [05:04,  2.58it/s, loss=4.07]epoch 2/100: 248it [05:04,  2.58it/s, loss=4.13]epoch 2/100: 249it [05:04,  2.58it/s, loss=4.07]epoch 2/100: 249it [05:04,  2.58it/s, loss=4.13]epoch 2/100: 249it [05:04,  2.58it/s, loss=4.07]epoch 2/100: 249it [05:04,  2.58it/s, loss=4]   
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:04,  4.85s/it][A
val: 1it [00:04,  4.86s/it][A
val: 2it [00:05,  2.15s/it][A
val: 2it [00:05,  2.15s/it][A
val: 3it [00:05,  1.28s/it][A
val: 3it [00:05,  1.28s/it][A
val: 4it [00:05,  1.14it/s][A
val: 4it [00:05,  1.14it/s][A
val: 5it [00:05,  1.54it/s][A
val: 5it [00:05,  1.54it/s][A
val: 6it [00:06,  1.95it/s][A
val: 6it [00:06,  1.94it/s][A
val: 7it [00:06,  2.34it/s][A
val: 7it [00:06,  2.33it/s][A
val: 8it [00:06,  2.69it/s][A
val: 8it [00:06,  2.68it/s][A
val: 9it [00:06,  3.00it/s][A
val: 9it [00:06,  2.98it/s][A
val: 10it [00:07,  3.25it/s][A
val: 10it [00:07,  3.22it/s][A
val: 11it [00:07,  3.45it/s][A
val: 11it [00:07,  3.42it/s][A
val: 12it [00:07,  3.60it/s][A
val: 12it [00:07,  3.57it/s][A
val: 13it [00:07,  3.72it/s][A
val: 13it [00:07,  3.68it/s][A
val: 14it [00:08,  3.80it/s][A
val: 14it [00:08,  3.76it/s][A
val: 15it [00:08,  3.86it/s][A
val: 15it [00:08,  3.82it/s][A
val: 16it [00:08,  3.90it/s][A
val: 16it [00:08,  3.86it/s][A
val: 17it [00:08,  3.92it/s][A
val: 17it [00:08,  3.89it/s][A
val: 18it [00:09,  3.94it/s][A
val: 18it [00:09,  3.91it/s][A
val: 19it [00:09,  3.96it/s][A
val: 19it [00:09,  3.93it/s][A
val: 20it [00:09,  3.95it/s][A
val: 20it [00:09,  3.93it/s][A
val: 21it [00:09,  3.96it/s][A
val: 21it [00:09,  3.94it/s][A
val: 22it [00:10,  3.97it/s][A
val: 22it [00:10,  3.95it/s][A
val: 23it [00:10,  3.98it/s][A
val: 23it [00:10,  3.96it/s][A
val: 24it [00:10,  3.99it/s][A
val: 24it [00:10,  3.96it/s][A
                            [A
                            [Aepoch 2/100: 249it [05:15,  2.58it/s, loss=4]epoch 2/100: 249it [05:23,  2.58it/s, best_val=4.12, loss=4.07, val=4.12]epoch 2/100: 250it [05:23,  6.12s/it, loss=4]epoch 2/100: 250it [05:23,  6.12s/it, best_val=4.12, loss=4.07, val=4.12]epoch 2/100: 250it [05:24,  6.12s/it, loss=4.31]epoch 2/100: 250it [05:24,  6.12s/it, loss=4.3]                          epoch 2/100: 251it [05:24,  4.41s/it, loss=4.31]epoch 2/100: 251it [05:24,  4.41s/it, loss=4.3]epoch 2/100: 251it [05:24,  4.41s/it, loss=4.16]epoch 2/100: 251it [05:24,  4.41s/it, loss=4.12]epoch 2/100: 252it [05:24,  3.21s/it, loss=4.16]epoch 2/100: 252it [05:24,  3.21s/it, loss=4.12]epoch 2/100: 252it [05:24,  3.21s/it, loss=4.21]epoch 2/100: 252it [05:24,  3.21s/it, loss=4.02]epoch 2/100: 253it [05:24,  2.36s/it, loss=4.21]epoch 2/100: 253it [05:24,  2.36s/it, loss=4.02]epoch 2/100: 253it [05:25,  2.36s/it, loss=4.27]epoch 2/100: 253it [05:25,  2.36s/it, loss=4.21]epoch 2/100: 254it [05:25,  1.77s/it, loss=4.27]epoch 2/100: 254it [05:25,  1.77s/it, loss=4.21]epoch 2/100: 254it [05:25,  1.77s/it, loss=4.3] epoch 2/100: 254it [05:25,  1.77s/it, loss=4.12]epoch 2/100: 255it [05:25,  1.35s/it, loss=4.12]epoch 2/100: 255it [05:25,  1.35s/it, loss=4.3]epoch 2/100: 255it [05:26,  1.35s/it, loss=4.15]epoch 2/100: 255it [05:26,  1.35s/it, loss=4.25]epoch 2/100: 256it [05:26,  1.06s/it, loss=4.15]epoch 2/100: 256it [05:26,  1.06s/it, loss=4.25]epoch 2/100: 256it [05:26,  1.06s/it, loss=4.21]epoch 2/100: 256it [05:26,  1.06s/it, loss=4.18]epoch 2/100: 257it [05:26,  1.16it/s, loss=4.21]epoch 2/100: 257it [05:26,  1.16it/s, loss=4.18]epoch 2/100: 257it [05:26,  1.16it/s, loss=4.19]epoch 2/100: 257it [05:26,  1.16it/s, loss=4.39]epoch 2/100: 258it [05:26,  1.39it/s, loss=4.39]epoch 2/100: 258it [05:26,  1.39it/s, loss=4.19]epoch 2/100: 258it [05:27,  1.39it/s, loss=4.35]epoch 2/100: 258it [05:27,  1.39it/s, loss=4.23]epoch 2/100: 259it [05:27,  1.62it/s, loss=4.23]epoch 2/100: 259it [05:27,  1.62it/s, loss=4.35]epoch 2/100: 259it [05:27,  1.62it/s, loss=4.17]epoch 2/100: 259it [05:27,  1.62it/s, loss=4.06]epoch 2/100: 260it [05:27,  1.82it/s, loss=4.06]epoch 2/100: 260it [05:27,  1.82it/s, loss=4.17]epoch 2/100: 260it [05:27,  1.82it/s, loss=4.44]epoch 2/100: 260it [05:27,  1.82it/s, loss=4.1] epoch 2/100: 261it [05:27,  2.00it/s, loss=4.44]epoch 2/100: 261it [05:27,  2.00it/s, loss=4.1]epoch 2/100: 261it [05:28,  2.00it/s, loss=4.2]epoch 2/100: 261it [05:28,  2.00it/s, loss=4]   epoch 2/100: 262it [05:28,  2.14it/s, loss=4.2]epoch 2/100: 262it [05:28,  2.14it/s, loss=4]epoch 2/100: 262it [05:28,  2.14it/s, loss=4.04]epoch 2/100: 262it [05:28,  2.14it/s, loss=4.11]epoch 2/100: 263it [05:28,  2.26it/s, loss=4.04]epoch 2/100: 263it [05:28,  2.26it/s, loss=4.11]epoch 2/100: 263it [05:29,  2.26it/s, loss=4.18]epoch 2/100: 263it [05:29,  2.26it/s, loss=4.27]epoch 2/100: 264it [05:29,  2.35it/s, loss=4.27]epoch 2/100: 264it [05:29,  2.35it/s, loss=4.18]epoch 2/100: 264it [05:29,  2.35it/s, loss=4.15]epoch 2/100: 264it [05:29,  2.35it/s, loss=4.03]epoch 2/100: 265it [05:29,  2.42it/s, loss=4.15]epoch 2/100: 265it [05:29,  2.42it/s, loss=4.03]epoch 2/100: 265it [05:29,  2.42it/s, loss=4.26]epoch 2/100: 265it [05:29,  2.42it/s, loss=4.11]epoch 2/100: 266it [05:29,  2.46it/s, loss=4.26]epoch 2/100: 266it [05:29,  2.46it/s, loss=4.11]epoch 2/100: 266it [05:30,  2.46it/s, loss=4.02]epoch 2/100: 266it [05:30,  2.46it/s, loss=4.16]epoch 2/100: 267it [05:30,  2.50it/s, loss=4.02]epoch 2/100: 267it [05:30,  2.50it/s, loss=4.16]epoch 2/100: 267it [05:30,  2.50it/s, loss=4.27]epoch 2/100: 267it [05:30,  2.50it/s, loss=4.21]epoch 2/100: 268it [05:30,  2.53it/s, loss=4.27]epoch 2/100: 268it [05:30,  2.53it/s, loss=4.21]epoch 2/100: 268it [05:31,  2.53it/s, loss=4.08]epoch 2/100: 268it [05:31,  2.53it/s, loss=4.11]epoch 2/100: 269it [05:31,  2.55it/s, loss=4.08]epoch 2/100: 269it [05:31,  2.55it/s, loss=4.11]epoch 2/100: 269it [05:31,  2.55it/s, loss=4]   epoch 2/100: 269it [05:31,  2.55it/s, loss=4.13]epoch 2/100: 270it [05:31,  2.56it/s, loss=4]epoch 2/100: 270it [05:31,  2.56it/s, loss=4.13]epoch 2/100: 270it [05:31,  2.56it/s, loss=4.29]epoch 2/100: 270it [05:31,  2.56it/s, loss=4.19]epoch 2/100: 271it [05:31,  2.57it/s, loss=4.29]epoch 2/100: 271it [05:31,  2.57it/s, loss=4.19]epoch 2/100: 271it [05:32,  2.57it/s, loss=4.23]epoch 2/100: 271it [05:32,  2.57it/s, loss=3.89]epoch 2/100: 272it [05:32,  2.57it/s, loss=4.23]epoch 2/100: 272it [05:32,  2.57it/s, loss=3.89]epoch 2/100: 272it [05:32,  2.57it/s, loss=4.28]epoch 2/100: 272it [05:32,  2.57it/s, loss=4.19]epoch 2/100: 273it [05:32,  2.58it/s, loss=4.19]epoch 2/100: 273it [05:32,  2.58it/s, loss=4.28]epoch 2/100: 273it [05:32,  2.58it/s, loss=4.04]epoch 2/100: 273it [05:32,  2.58it/s, loss=4.25]epoch 2/100: 274it [05:32,  2.58it/s, loss=4.04]epoch 2/100: 274it [05:32,  2.58it/s, loss=4.25]epoch 2/100: 274it [05:33,  2.58it/s, loss=4.09]epoch 2/100: 274it [05:33,  2.58it/s, loss=4.39]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.43s/it][A
val: 2it [00:03,  1.56s/it][A
val: 3it [00:03,  1.04it/s][A
val: 4it [00:04,  1.47it/s][A
val: 5it [00:04,  1.90it/s][A
val: 6it [00:04,  2.31it/s][A
val: 7it [00:04,  2.68it/s][A
val: 8it [00:05,  2.99it/s][A
val: 9it [00:05,  3.24it/s][A
val: 10it [00:05,  3.44it/s][A
val: 11it [00:05,  3.59it/s][A
val: 12it [00:06,  3.70it/s][A
val: 13it [00:06,  3.78it/s][A
val: 14it [00:06,  3.84it/s][A
val: 15it [00:06,  3.88it/s][A
val: 16it [00:07,  3.91it/s][A
val: 17it [00:07,  3.93it/s][A
val: 18it [00:07,  3.94it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:08,  3.96it/s][A
val: 23it [00:08,  3.96it/s][A
val: 24it [00:09,  3.96it/s][A
                            [A
val: 1it [00:34, 34.06s/it][A
val: 2it [00:34, 14.17s/it][A
val: 3it [00:34,  7.81s/it][A
val: 4it [00:34,  4.83s/it][A
val: 5it [00:35,  3.18s/it][A
val: 6it [00:35,  2.18s/it][A
val: 7it [00:35,  1.55s/it][A
val: 8it [00:35,  1.14s/it][A
val: 9it [00:36,  1.16it/s][A
val: 10it [00:36,  1.49it/s][A
val: 11it [00:36,  1.84it/s][A
val: 12it [00:36,  2.20it/s][A
val: 13it [00:37,  2.54it/s][A
val: 14it [00:37,  2.85it/s][A
val: 15it [00:37,  3.12it/s][A
val: 16it [00:37,  3.33it/s][A
val: 17it [00:38,  3.50it/s][A
val: 18it [00:38,  3.63it/s][A
val: 19it [00:38,  3.73it/s][A
val: 20it [00:38,  3.80it/s][A
val: 21it [00:39,  3.85it/s][A
val: 22it [00:39,  3.89it/s][A
val: 23it [00:39,  3.91it/s][A
val: 24it [00:39,  3.93it/s][A
                            [Aepoch 2/100: 274it [06:13,  2.58it/s, loss=4.09]epoch 2/100: 274it [06:21,  2.58it/s, best_val=4.11, loss=4.39, val=4.11]epoch 2/100: 275it [06:21, 14.88s/it, loss=4.09]epoch 2/100: 275it [06:21, 14.88s/it, best_val=4.11, loss=4.39, val=4.11]epoch 2/100: 275it [06:22, 14.88s/it, loss=3.83]epoch 2/100: 275it [06:22, 14.88s/it, loss=4.13]                         epoch 2/100: 276it [06:22, 10.53s/it, loss=3.83]epoch 2/100: 276it [06:22, 10.53s/it, loss=4.13]epoch 2/100: 276it [06:22, 10.53s/it, loss=4.18]epoch 2/100: 276it [06:22, 10.53s/it, loss=4.08]epoch 2/100: 277it [06:22,  7.49s/it, loss=4.18]epoch 2/100: 277it [06:22,  7.49s/it, loss=4.08]epoch 2/100: 277it [06:22,  7.49s/it, loss=4.16]epoch 2/100: 277it [06:22,  7.49s/it, loss=4.34]epoch 2/100: 278it [06:22,  5.36s/it, loss=4.16]epoch 2/100: 278it [06:22,  5.36s/it, loss=4.34]epoch 2/100: 278it [06:23,  5.36s/it, loss=4.2] epoch 2/100: 278it [06:23,  5.36s/it, loss=4.28]epoch 2/100: 279it [06:23,  3.87s/it, loss=4.2]epoch 2/100: 279it [06:23,  3.87s/it, loss=4.28]epoch 2/100: 279it [06:23,  3.87s/it, loss=4.21]epoch 2/100: 279it [06:23,  3.87s/it, loss=4.29]epoch 2/100: 280it [06:23,  2.82s/it, loss=4.21]epoch 2/100: 280it [06:23,  2.82s/it, loss=4.29]epoch 2/100: 280it [06:23,  2.82s/it, loss=4.1] epoch 2/100: 280it [06:23,  2.82s/it, loss=3.92]epoch 2/100: 281it [06:23,  2.09s/it, loss=4.1]epoch 2/100: 281it [06:23,  2.09s/it, loss=3.92]epoch 2/100: 281it [06:24,  2.09s/it, loss=4.2]epoch 2/100: 281it [06:24,  2.09s/it, loss=4.02]epoch 2/100: 282it [06:24,  1.58s/it, loss=4.2]epoch 2/100: 282it [06:24,  1.58s/it, loss=4.02]epoch 2/100: 282it [06:24,  1.58s/it, loss=4.07]epoch 2/100: 282it [06:24,  1.58s/it, loss=3.92]epoch 2/100: 283it [06:24,  1.22s/it, loss=4.07]epoch 2/100: 283it [06:24,  1.22s/it, loss=3.92]epoch 2/100: 283it [06:25,  1.22s/it, loss=4.12]epoch 2/100: 283it [06:25,  1.22s/it, loss=3.92]epoch 2/100: 284it [06:25,  1.03it/s, loss=4.12]epoch 2/100: 284it [06:25,  1.03it/s, loss=3.92]epoch 2/100: 284it [06:25,  1.03it/s, loss=4.1] epoch 2/100: 284it [06:25,  1.03it/s, loss=4.17]epoch 2/100: 285it [06:25,  1.26it/s, loss=4.17]epoch 2/100: 285it [06:25,  1.26it/s, loss=4.1]epoch 2/100: 285it [06:25,  1.26it/s, loss=4.06]epoch 2/100: 285it [06:25,  1.26it/s, loss=4.15]epoch 2/100: 286it [06:25,  1.49it/s, loss=4.06]epoch 2/100: 286it [06:25,  1.49it/s, loss=4.15]epoch 2/100: 286it [06:26,  1.49it/s, loss=4.09]epoch 2/100: 286it [06:26,  1.49it/s, loss=3.97]epoch 2/100: 287it [06:26,  1.71it/s, loss=3.97]epoch 2/100: 287it [06:26,  1.71it/s, loss=4.09]epoch 2/100: 287it [06:26,  1.71it/s, loss=3.86]epoch 2/100: 287it [06:26,  1.71it/s, loss=4.04]epoch 2/100: 288it [06:26,  1.90it/s, loss=3.86]epoch 2/100: 288it [06:26,  1.90it/s, loss=4.04]epoch 2/100: 288it [06:27,  1.90it/s, loss=4.22]epoch 2/100: 288it [06:27,  1.90it/s, loss=4.01]epoch 2/100: 289it [06:27,  2.07it/s, loss=4.22]epoch 2/100: 289it [06:27,  2.07it/s, loss=4.01]epoch 2/100: 289it [06:27,  2.07it/s, loss=4.15]epoch 2/100: 289it [06:27,  2.07it/s, loss=4.21]epoch 2/100: 290it [06:27,  2.20it/s, loss=4.15]epoch 2/100: 290it [06:27,  2.20it/s, loss=4.21]epoch 2/100: 290it [06:27,  2.20it/s, loss=4.09]epoch 2/100: 290it [06:27,  2.20it/s, loss=4.28]epoch 2/100: 291it [06:27,  2.30it/s, loss=4.09]epoch 2/100: 291it [06:27,  2.30it/s, loss=4.28]epoch 2/100: 291it [06:28,  2.30it/s, loss=3.97]epoch 2/100: 291it [06:28,  2.30it/s, loss=4.17]epoch 2/100: 292it [06:28,  2.38it/s, loss=3.97]epoch 2/100: 292it [06:28,  2.38it/s, loss=4.17]epoch 2/100: 292it [06:28,  2.38it/s, loss=4.21]epoch 2/100: 292it [06:28,  2.38it/s, loss=4.15]epoch 2/100: 293it [06:28,  2.44it/s, loss=4.21]epoch 2/100: 293it [06:28,  2.44it/s, loss=4.15]epoch 2/100: 293it [06:28,  2.44it/s, loss=4.29]epoch 2/100: 293it [06:29,  2.44it/s, loss=4.03]epoch 2/100: 294it [06:28,  2.49it/s, loss=4.29]epoch 2/100: 294it [06:29,  2.49it/s, loss=4.03]epoch 2/100: 294it [06:29,  2.49it/s, loss=4.29]epoch 2/100: 294it [06:29,  2.49it/s, loss=4.11]epoch 2/100: 295it [06:29,  2.52it/s, loss=4.29]epoch 2/100: 295it [06:29,  2.52it/s, loss=4.11]epoch 2/100: 295it [06:29,  2.52it/s, loss=4.17]epoch 2/100: 295it [06:29,  2.52it/s, loss=4.01]epoch 2/100: 296it [06:29,  2.54it/s, loss=4.17]epoch 2/100: 296it [06:29,  2.54it/s, loss=4.01]epoch 2/100: 296it [06:30,  2.54it/s, loss=4.1] epoch 2/100: 296it [06:30,  2.54it/s, loss=4.19]epoch 2/100: 297it [06:30,  2.55it/s, loss=4.19]epoch 2/100: 297it [06:30,  2.55it/s, loss=4.1]epoch 2/100: 297it [06:30,  2.55it/s, loss=3.94]epoch 2/100: 297it [06:30,  2.55it/s, loss=4.11]epoch 2/100: 298it [06:30,  2.57it/s, loss=3.94]epoch 2/100: 298it [06:30,  2.57it/s, loss=4.11]epoch 2/100: 298it [06:30,  2.57it/s, loss=4.26]epoch 2/100: 298it [06:30,  2.57it/s, loss=4.07]epoch 2/100: 299it [06:30,  2.58it/s, loss=4.26]epoch 2/100: 299it [06:30,  2.58it/s, loss=4.07]epoch 2/100: 299it [06:31,  2.58it/s, loss=4.22]epoch 2/100: 299it [06:31,  2.58it/s, loss=4.08]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.33s/it][A
val: 2it [00:03,  1.52s/it][A
val: 3it [00:03,  1.06it/s][A
val: 4it [00:04,  1.50it/s][A
val: 1it [00:04,  4.27s/it][A
val: 5it [00:04,  1.94it/s][A
val: 2it [00:04,  1.91s/it][A
val: 6it [00:04,  2.35it/s][A
val: 3it [00:04,  1.15s/it][A
val: 7it [00:04,  2.72it/s][A
val: 4it [00:05,  1.25it/s][A
val: 8it [00:05,  3.03it/s][A
val: 5it [00:05,  1.66it/s][A
val: 9it [00:05,  3.27it/s][A
val: 6it [00:05,  2.07it/s][A
val: 10it [00:05,  3.47it/s][A
val: 7it [00:05,  2.46it/s][A
val: 11it [00:05,  3.61it/s][A
val: 8it [00:06,  2.79it/s][A
val: 12it [00:06,  3.72it/s][A
val: 9it [00:06,  3.08it/s][A
val: 13it [00:06,  3.80it/s][A
val: 10it [00:06,  3.31it/s][A
val: 14it [00:06,  3.86it/s][A
val: 11it [00:06,  3.48it/s][A
val: 15it [00:06,  3.90it/s][A
val: 12it [00:07,  3.62it/s][A
val: 16it [00:07,  3.93it/s][A
val: 13it [00:07,  3.71it/s][A
val: 17it [00:07,  3.95it/s][A
val: 14it [00:07,  3.78it/s][A
val: 18it [00:07,  3.97it/s][A
val: 15it [00:07,  3.84it/s][A
val: 19it [00:07,  3.98it/s][A
val: 16it [00:08,  3.88it/s][A
val: 20it [00:08,  3.99it/s][A
val: 17it [00:08,  3.90it/s][A
val: 21it [00:08,  3.99it/s][A
val: 18it [00:08,  3.91it/s][A
val: 22it [00:08,  3.99it/s][A
val: 19it [00:08,  3.93it/s][A
val: 23it [00:08,  4.00it/s][A
val: 20it [00:09,  3.93it/s][A
val: 24it [00:09,  4.00it/s][A
                            [A
val: 21it [00:09,  3.94it/s][A
val: 22it [00:09,  3.95it/s][A
val: 23it [00:09,  3.95it/s][A
val: 24it [00:10,  3.95it/s][A
                            [Aepoch 2/100: 299it [06:41,  2.58it/s, loss=4.22]epoch 2/100: 299it [06:51,  2.58it/s, best_val=4.1, loss=4.08, val=4.1]epoch 2/100: 300it [06:51,  6.43s/it, loss=4.22]epoch 2/100: 300it [06:51,  6.43s/it, best_val=4.1, loss=4.08, val=4.1]epoch 2/100: 300it [06:51,  6.43s/it, loss=4.18]epoch 2/100: 300it [06:51,  6.43s/it, loss=4.2]                        epoch 2/100: 301it [06:51,  4.62s/it, loss=4.18]epoch 2/100: 301it [06:51,  4.62s/it, loss=4.2]epoch 2/100: 301it [06:52,  4.62s/it, loss=4.17]epoch 2/100: 301it [06:52,  4.62s/it, loss=4.03]epoch 2/100: 302it [06:52,  3.35s/it, loss=4.03]epoch 2/100: 302it [06:52,  3.35s/it, loss=4.17]epoch 2/100: 302it [06:52,  3.35s/it, loss=4.04]epoch 2/100: 302it [06:52,  3.35s/it, loss=3.85]epoch 2/100: 303it [06:52,  2.46s/it, loss=4.04]epoch 2/100: 303it [06:52,  2.46s/it, loss=3.85]epoch 2/100: 303it [06:52,  2.46s/it, loss=4.18]epoch 2/100: 303it [06:52,  2.46s/it, loss=4.02]epoch 2/100: 304it [06:52,  1.84s/it, loss=4.02]epoch 2/100: 304it [06:52,  1.84s/it, loss=4.18]epoch 2/100: 304it [06:53,  1.84s/it, loss=4.09]epoch 2/100: 304it [06:53,  1.84s/it, loss=4.23]epoch 2/100: 305it [06:53,  1.40s/it, loss=4.09]epoch 2/100: 305it [06:53,  1.40s/it, loss=4.23]epoch 2/100: 305it [06:53,  1.40s/it, loss=4.06]epoch 2/100: 305it [06:53,  1.40s/it, loss=4.34]epoch 2/100: 306it [06:53,  1.10s/it, loss=4.06]epoch 2/100: 306it [06:53,  1.10s/it, loss=4.34]epoch 2/100: 306it [06:54,  1.10s/it, loss=4.16]epoch 2/100: 306it [06:54,  1.10s/it, loss=4.31]epoch 2/100: 307it [06:54,  1.13it/s, loss=4.16]epoch 2/100: 307it [06:54,  1.13it/s, loss=4.31]epoch 2/100: 307it [06:54,  1.13it/s, loss=4.21]epoch 2/100: 307it [06:54,  1.13it/s, loss=4.28]epoch 2/100: 308it [06:54,  1.36it/s, loss=4.21]epoch 2/100: 308it [06:54,  1.36it/s, loss=4.28]epoch 2/100: 308it [06:54,  1.36it/s, loss=4.05]epoch 2/100: 308it [06:54,  1.36it/s, loss=4.24]epoch 2/100: 309it [06:54,  1.59it/s, loss=4.24]epoch 2/100: 309it [06:54,  1.59it/s, loss=4.05]epoch 2/100: 309it [06:55,  1.59it/s, loss=4.03]epoch 2/100: 309it [06:55,  1.59it/s, loss=4.16]epoch 2/100: 310it [06:55,  1.80it/s, loss=4.03]epoch 2/100: 310it [06:55,  1.80it/s, loss=4.16]epoch 2/100: 310it [06:55,  1.80it/s, loss=4.2] epoch 2/100: 310it [06:55,  1.80it/s, loss=4.19]epoch 2/100: 311it [06:55,  1.98it/s, loss=4.2]epoch 2/100: 311it [06:55,  1.98it/s, loss=4.19]epoch 2/100: 311it [06:56,  1.98it/s, loss=3.91]epoch 2/100: 311it [06:56,  1.98it/s, loss=4.11]epoch 2/100: 312it [06:56,  2.13it/s, loss=3.91]epoch 2/100: 312it [06:56,  2.13it/s, loss=4.11]epoch 2/100: 312it [06:56,  2.13it/s, loss=4.16]epoch 2/100: 312it [06:56,  2.13it/s, loss=4.41]epoch 2/100: 313it [06:56,  2.25it/s, loss=4.16]epoch 2/100: 313it [06:56,  2.25it/s, loss=4.41]epoch 2/100: 313it [06:56,  2.25it/s, loss=4.23]epoch 2/100: 313it [06:56,  2.25it/s, loss=4.1] epoch 2/100: 314it [06:56,  2.34it/s, loss=4.23]epoch 2/100: 314it [06:56,  2.34it/s, loss=4.1]epoch 2/100: 314it [06:57,  2.34it/s, loss=4.1]epoch 2/100: 314it [06:57,  2.34it/s, loss=4.17]epoch 2/100: 315it [06:57,  2.41it/s, loss=4.17]epoch 2/100: 315it [06:57,  2.41it/s, loss=4.1]epoch 2/100: 315it [06:57,  2.41it/s, loss=4.08]epoch 2/100: 315it [06:57,  2.41it/s, loss=4.21]epoch 2/100: 316it [06:57,  2.46it/s, loss=4.21]epoch 2/100: 316it [06:57,  2.46it/s, loss=4.08]epoch 2/100: 316it [06:58,  2.46it/s, loss=4.24]epoch 2/100: 316it [06:58,  2.46it/s, loss=3.92]epoch 2/100: 317it [06:58,  2.49it/s, loss=4.24]epoch 2/100: 317it [06:58,  2.49it/s, loss=3.92]epoch 2/100: 317it [06:58,  2.49it/s, loss=4.12]epoch 2/100: 317it [06:58,  2.49it/s, loss=4.21]epoch 2/100: 318it [06:58,  2.52it/s, loss=4.12]epoch 2/100: 318it [06:58,  2.52it/s, loss=4.21]epoch 2/100: 318it [06:58,  2.52it/s, loss=3.98]epoch 2/100: 318it [06:58,  2.52it/s, loss=4.28]epoch 2/100: 319it [06:58,  2.54it/s, loss=3.98]epoch 2/100: 319it [06:58,  2.54it/s, loss=4.28]epoch 2/100: 319it [06:59,  2.54it/s, loss=4.18]epoch 2/100: 319it [06:59,  2.54it/s, loss=4.08]epoch 2/100: 320it [06:59,  2.55it/s, loss=4.18]epoch 2/100: 320it [06:59,  2.55it/s, loss=4.08]epoch 2/100: 320it [06:59,  2.55it/s, loss=4.21]epoch 2/100: 320it [06:59,  2.55it/s, loss=4.28]epoch 2/100: 321it [06:59,  2.56it/s, loss=4.28]epoch 2/100: 321it [06:59,  2.56it/s, loss=4.21]epoch 2/100: 321it [06:59,  2.56it/s, loss=4.16]epoch 2/100: 321it [06:59,  2.56it/s, loss=4.01]epoch 2/100: 322it [06:59,  2.57it/s, loss=4.16]epoch 2/100: 322it [06:59,  2.57it/s, loss=4.01]epoch 2/100: 322it [07:00,  2.57it/s, loss=4.15]epoch 2/100: 322it [07:00,  2.57it/s, loss=4.09]epoch 2/100: 323it [07:00,  2.58it/s, loss=4.15]epoch 2/100: 323it [07:00,  2.58it/s, loss=4.09]epoch 2/100: 323it [07:00,  2.58it/s, loss=4.1] epoch 2/100: 323it [07:00,  2.58it/s, loss=3.96]epoch 2/100: 324it [07:00,  2.58it/s, loss=4.1]epoch 2/100: 324it [07:00,  2.58it/s, loss=3.96]epoch 2/100: 324it [07:01,  2.58it/s, loss=4.15]epoch 2/100: 324it [07:01,  2.58it/s, loss=4.2] 
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.37s/it][A
val: 2it [00:03,  1.54s/it][A
val: 3it [00:03,  1.05it/s][A
val: 4it [00:04,  1.49it/s][A
val: 5it [00:04,  1.92it/s][A
val: 6it [00:04,  2.33it/s][A
val: 7it [00:04,  2.70it/s][A
val: 8it [00:05,  3.01it/s][A
val: 9it [00:05,  3.26it/s][A
val: 10it [00:05,  3.45it/s][A
val: 11it [00:05,  3.60it/s][A
val: 12it [00:06,  3.70it/s][A
val: 13it [00:06,  3.78it/s][A
val: 1it [00:06,  6.63s/it][A
val: 14it [00:06,  3.83it/s][A
val: 2it [00:06,  2.88s/it][A
val: 15it [00:06,  3.87it/s][A
val: 3it [00:07,  1.68s/it][A
val: 16it [00:07,  3.89it/s][A
val: 4it [00:07,  1.12s/it][A
val: 17it [00:07,  3.91it/s][A
val: 5it [00:07,  1.24it/s][A
val: 18it [00:07,  3.93it/s][A
val: 6it [00:07,  1.62it/s][A
val: 19it [00:07,  3.95it/s][A
val: 20it [00:08,  3.96it/s][A
val: 7it [00:08,  2.01it/s][A
val: 21it [00:08,  3.96it/s][A
val: 8it [00:08,  2.38it/s][A
val: 22it [00:08,  3.97it/s][A
val: 9it [00:08,  2.72it/s][A
val: 23it [00:08,  3.98it/s][A
val: 10it [00:08,  3.01it/s][A
val: 24it [00:09,  3.98it/s][A
val: 11it [00:09,  3.25it/s][A
                            [A
val: 12it [00:09,  3.44it/s][A
val: 13it [00:09,  3.58it/s][A
val: 14it [00:09,  3.68it/s][A
val: 15it [00:10,  3.76it/s][A
val: 16it [00:10,  3.82it/s][A
val: 17it [00:10,  3.86it/s][A
val: 18it [00:10,  3.89it/s][A
val: 19it [00:11,  3.91it/s][A
val: 20it [00:11,  3.92it/s][A
val: 21it [00:11,  3.93it/s][A
val: 22it [00:11,  3.94it/s][A
val: 23it [00:12,  3.95it/s][A
val: 24it [00:12,  3.95it/s][A
                            [Aepoch 2/100: 324it [07:13,  2.58it/s, loss=4.2]epoch 2/100: 324it [07:23,  2.58it/s, best_val=4.08, loss=4.15, val=4.08]epoch 2/100: 325it [07:23,  7.23s/it, loss=4.2]epoch 2/100: 325it [07:23,  7.23s/it, best_val=4.08, loss=4.15, val=4.08]epoch 2/100: 325it [07:24,  7.23s/it, loss=4.26]epoch 2/100: 325it [07:24,  7.23s/it, loss=3.97]                         epoch 2/100: 326it [07:24,  5.18s/it, loss=4.26]epoch 2/100: 326it [07:24,  5.18s/it, loss=3.97]epoch 2/100: 326it [07:24,  5.18s/it, loss=4.27]epoch 2/100: 326it [07:24,  5.18s/it, loss=4.16]epoch 2/100: 327it [07:24,  3.74s/it, loss=4.27]epoch 2/100: 327it [07:24,  3.74s/it, loss=4.16]epoch 2/100: 327it [07:25,  3.74s/it, loss=4.14]epoch 2/100: 327it [07:25,  3.74s/it, loss=4.26]epoch 2/100: 328it [07:25,  2.74s/it, loss=4.14]epoch 2/100: 328it [07:25,  2.74s/it, loss=4.26]epoch 2/100: 328it [07:25,  2.74s/it, loss=4.32]epoch 2/100: 328it [07:25,  2.74s/it, loss=4.05]epoch 2/100: 329it [07:25,  2.03s/it, loss=4.05]epoch 2/100: 329it [07:25,  2.03s/it, loss=4.32]epoch 2/100: 329it [07:25,  2.03s/it, loss=4.2] epoch 2/100: 329it [07:25,  2.03s/it, loss=4.05]epoch 2/100: 330it [07:25,  1.54s/it, loss=4.2]epoch 2/100: 330it [07:25,  1.54s/it, loss=4.05]epoch 2/100: 330it [07:26,  1.54s/it, loss=4.06]epoch 2/100: 330it [07:26,  1.54s/it, loss=4.25]epoch 2/100: 331it [07:26,  1.20s/it, loss=4.25]epoch 2/100: 331it [07:26,  1.20s/it, loss=4.06]epoch 2/100: 331it [07:26,  1.20s/it, loss=4.17]epoch 2/100: 331it [07:26,  1.20s/it, loss=4.05]epoch 2/100: 332it [07:26,  1.05it/s, loss=4.17]epoch 2/100: 332it [07:26,  1.05it/s, loss=4.05]epoch 2/100: 332it [07:27,  1.05it/s, loss=4.08]epoch 2/100: 332it [07:27,  1.05it/s, loss=4.11]epoch 2/100: 333it [07:27,  1.27it/s, loss=4.11]epoch 2/100: 333it [07:27,  1.27it/s, loss=4.08]epoch 2/100: 333it [07:27,  1.27it/s, loss=4.14]epoch 2/100: 333it [07:27,  1.27it/s, loss=4.1] epoch 2/100: 334it [07:27,  1.50it/s, loss=4.14]epoch 2/100: 334it [07:27,  1.50it/s, loss=4.1]epoch 2/100: 334it [07:27,  1.50it/s, loss=4.18]epoch 2/100: 334it [07:27,  1.50it/s, loss=4.14]epoch 2/100: 335it [07:27,  1.72it/s, loss=4.18]epoch 2/100: 335it [07:27,  1.72it/s, loss=4.14]epoch 2/100: 335it [07:28,  1.72it/s, loss=3.95]epoch 2/100: 335it [07:28,  1.72it/s, loss=3.99]epoch 2/100: 336it [07:28,  1.90it/s, loss=3.95]epoch 2/100: 336it [07:28,  1.90it/s, loss=3.99]epoch 2/100: 336it [07:28,  1.90it/s, loss=4.12]epoch 2/100: 336it [07:28,  1.90it/s, loss=4.18]epoch 2/100: 337it [07:28,  2.06it/s, loss=4.12]epoch 2/100: 337it [07:28,  2.06it/s, loss=4.18]epoch 2/100: 337it [07:28,  2.06it/s, loss=4.07]epoch 2/100: 337it [07:29,  2.06it/s, loss=4.05]epoch 2/100: 338it [07:28,  2.19it/s, loss=4.07]epoch 2/100: 338it [07:29,  2.19it/s, loss=4.05]epoch 2/100: 338it [07:29,  2.19it/s, loss=4.28]epoch 2/100: 338it [07:29,  2.19it/s, loss=3.97]epoch 2/100: 339it [07:29,  2.29it/s, loss=4.28]epoch 2/100: 339it [07:29,  2.29it/s, loss=3.97]epoch 2/100: 339it [07:29,  2.29it/s, loss=4.09]epoch 2/100: 339it [07:29,  2.29it/s, loss=4.19]epoch 2/100: 340it [07:29,  2.37it/s, loss=4.09]epoch 2/100: 340it [07:29,  2.37it/s, loss=4.19]epoch 2/100: 340it [07:30,  2.37it/s, loss=4.38]epoch 2/100: 340it [07:30,  2.37it/s, loss=4.16]epoch 2/100: 341it [07:30,  2.43it/s, loss=4.38]epoch 2/100: 341it [07:30,  2.43it/s, loss=4.16]epoch 2/100: 341it [07:30,  2.43it/s, loss=4.02]epoch 2/100: 341it [07:30,  2.43it/s, loss=4.03]epoch 2/100: 342it [07:30,  2.47it/s, loss=4.03]epoch 2/100: 342it [07:30,  2.47it/s, loss=4.02]epoch 2/100: 342it [07:30,  2.47it/s, loss=4.15]epoch 2/100: 342it [07:30,  2.47it/s, loss=3.83]epoch 2/100: 343it [07:30,  2.51it/s, loss=4.15]epoch 2/100: 343it [07:30,  2.51it/s, loss=3.83]epoch 2/100: 343it [07:31,  2.51it/s, loss=4.1] epoch 2/100: 343it [07:31,  2.51it/s, loss=4.03]epoch 2/100: 344it [07:31,  2.53it/s, loss=4.1]epoch 2/100: 344it [07:31,  2.53it/s, loss=4.03]epoch 2/100: 344it [07:31,  2.53it/s, loss=4.24]epoch 2/100: 344it [07:31,  2.53it/s, loss=4.3]epoch 2/100: 345it [07:31,  2.54it/s, loss=4.24]epoch 2/100: 345it [07:31,  2.54it/s, loss=4.3]epoch 2/100: 345it [07:32,  2.54it/s, loss=4.02]epoch 2/100: 345it [07:32,  2.54it/s, loss=4.12]epoch 2/100: 346it [07:32,  2.55it/s, loss=4.02]epoch 2/100: 346it [07:32,  2.55it/s, loss=4.12]epoch 2/100: 346it [07:32,  2.55it/s, loss=4.03]epoch 2/100: 346it [07:32,  2.55it/s, loss=4.01]epoch 2/100: 347it [07:32,  2.56it/s, loss=4.03]epoch 2/100: 347it [07:32,  2.56it/s, loss=4.01]epoch 2/100: 347it [07:32,  2.56it/s, loss=4.02]epoch 2/100: 347it [07:32,  2.56it/s, loss=4.4] epoch 2/100: 348it [07:32,  2.57it/s, loss=4.02]epoch 2/100: 348it [07:32,  2.57it/s, loss=4.4]epoch 2/100: 348it [07:33,  2.57it/s, loss=4.19]epoch 2/100: 348it [07:33,  2.57it/s, loss=3.97]epoch 2/100: 349it [07:33,  2.57it/s, loss=4.19]epoch 2/100: 349it [07:33,  2.57it/s, loss=3.97]epoch 2/100: 349it [07:33,  2.57it/s, loss=4.15]epoch 2/100: 349it [07:33,  2.57it/s, loss=4.09]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.69s/it][A
val: 2it [00:03,  1.67s/it][A
val: 3it [00:04,  1.02s/it][A
val: 1it [00:04,  4.25s/it][A
val: 4it [00:04,  1.40it/s][A
val: 5it [00:04,  1.82it/s][A
val: 6it [00:04,  2.23it/s][A
val: 7it [00:05,  2.61it/s][A
val: 8it [00:05,  2.93it/s][A
val: 2it [00:05,  2.51s/it][A
val: 9it [00:05,  3.19it/s][A
val: 3it [00:05,  1.48s/it][A
val: 10it [00:05,  3.39it/s][A
val: 4it [00:06,  1.00it/s][A
val: 11it [00:06,  3.55it/s][A
val: 5it [00:06,  1.37it/s][A
val: 12it [00:06,  3.67it/s][A
val: 6it [00:06,  1.76it/s][A
val: 13it [00:06,  3.76it/s][A
val: 7it [00:06,  2.16it/s][A
val: 14it [00:06,  3.82it/s][A
val: 8it [00:07,  2.52it/s][A
val: 15it [00:07,  3.87it/s][A
val: 9it [00:07,  2.84it/s][A
val: 16it [00:07,  3.90it/s][A
val: 10it [00:07,  3.11it/s][A
val: 17it [00:07,  3.92it/s][A
val: 11it [00:07,  3.33it/s][A
val: 18it [00:07,  3.93it/s][A
val: 12it [00:08,  3.49it/s][A
val: 19it [00:08,  3.95it/s][A
val: 13it [00:08,  3.62it/s][A
val: 20it [00:08,  3.96it/s][A
val: 14it [00:08,  3.71it/s][A
val: 21it [00:08,  3.97it/s][A
val: 15it [00:08,  3.78it/s][A
val: 22it [00:08,  3.97it/s][A
val: 16it [00:09,  3.83it/s][A
val: 23it [00:09,  3.98it/s][A
val: 17it [00:09,  3.87it/s][A
val: 24it [00:09,  3.98it/s][A
val: 18it [00:09,  3.89it/s][A
                            [A
val: 19it [00:09,  3.91it/s][A
val: 20it [00:10,  3.92it/s][A
val: 21it [00:10,  3.94it/s][A
val: 22it [00:10,  3.94it/s][A
val: 23it [00:10,  3.94it/s][A
val: 24it [00:11,  3.94it/s][A
                            [Aepoch 2/100: 349it [07:44,  2.57it/s, loss=4.15]epoch 2/100: 349it [07:56,  2.57it/s, best_val=4.07, loss=4.09, val=4.07]epoch 2/100: 350it [07:56,  7.17s/it, loss=4.15]epoch 2/100: 350it [07:56,  7.17s/it, best_val=4.07, loss=4.09, val=4.07]epoch 2/100: 350it [07:56,  7.17s/it, loss=4.13]                         epoch 2/100: 350it [07:56,  7.17s/it, loss=4.15]epoch 2/100: 351it [07:56,  5.14s/it, loss=4.15]epoch 2/100: 351it [07:56,  5.14s/it, loss=4.13]epoch 2/100: 351it [07:57,  5.14s/it, loss=3.91]epoch 2/100: 351it [07:57,  5.14s/it, loss=3.95]epoch 2/100: 352it [07:57,  3.71s/it, loss=3.91]epoch 2/100: 352it [07:57,  3.71s/it, loss=3.95]epoch 2/100: 352it [07:57,  3.71s/it, loss=3.97]epoch 2/100: 352it [07:57,  3.71s/it, loss=4.26]epoch 2/100: 353it [07:57,  2.72s/it, loss=3.97]epoch 2/100: 353it [07:57,  2.72s/it, loss=4.26]epoch 2/100: 353it [07:57,  2.72s/it, loss=4.04]epoch 2/100: 353it [07:57,  2.72s/it, loss=4.11]epoch 2/100: 354it [07:57,  2.02s/it, loss=4.11]epoch 2/100: 354it [07:57,  2.02s/it, loss=4.04]epoch 2/100: 354it [07:58,  2.02s/it, loss=4.2] epoch 2/100: 354it [07:58,  2.02s/it, loss=3.97]epoch 2/100: 355it [07:58,  1.53s/it, loss=4.2]epoch 2/100: 355it [07:58,  1.53s/it, loss=3.97]epoch 2/100: 355it [07:58,  1.53s/it, loss=4.16]epoch 2/100: 355it [07:58,  1.53s/it, loss=4.12]epoch 2/100: 356it [07:58,  1.18s/it, loss=4.16]epoch 2/100: 356it [07:58,  1.18s/it, loss=4.12]epoch 2/100: 356it [07:58,  1.18s/it, loss=3.91]epoch 2/100: 356it [07:58,  1.18s/it, loss=4]   epoch 2/100: 357it [07:58,  1.06it/s, loss=3.91]epoch 2/100: 357it [07:58,  1.06it/s, loss=4]epoch 2/100: 357it [07:59,  1.06it/s, loss=3.96]epoch 2/100: 357it [07:59,  1.06it/s, loss=4.18]epoch 2/100: 358it [07:59,  1.28it/s, loss=3.96]epoch 2/100: 358it [07:59,  1.28it/s, loss=4.18]epoch 2/100: 358it [07:59,  1.28it/s, loss=4.17]epoch 2/100: 358it [07:59,  1.28it/s, loss=4.25]epoch 2/100: 359it [07:59,  1.51it/s, loss=4.17]epoch 2/100: 359it [07:59,  1.51it/s, loss=4.25]epoch 2/100: 359it [08:00,  1.51it/s, loss=4.1] epoch 2/100: 359it [08:00,  1.51it/s, loss=4.17]epoch 2/100: 360it [08:00,  1.72it/s, loss=4.1]epoch 2/100: 360it [08:00,  1.72it/s, loss=4.17]epoch 2/100: 360it [08:00,  1.72it/s, loss=4.1]epoch 2/100: 360it [08:00,  1.72it/s, loss=4.04]epoch 2/100: 361it [08:00,  1.91it/s, loss=4.1]epoch 2/100: 361it [08:00,  1.91it/s, loss=4.04]epoch 2/100: 361it [08:00,  1.91it/s, loss=4.18]epoch 2/100: 361it [08:00,  1.91it/s, loss=3.9] epoch 2/100: 362it [08:00,  2.07it/s, loss=4.18]epoch 2/100: 362it [08:00,  2.07it/s, loss=3.9]epoch 2/100: 362it [08:01,  2.07it/s, loss=4.08]epoch 2/100: 362it [08:01,  2.07it/s, loss=4.16]epoch 2/100: 363it [08:01,  2.20it/s, loss=4.08]epoch 2/100: 363it [08:01,  2.20it/s, loss=4.16]epoch 2/100: 363it [08:01,  2.20it/s, loss=4.16]epoch 2/100: 363it [08:01,  2.20it/s, loss=4.15]epoch 2/100: 364it [08:01,  2.31it/s, loss=4.15]epoch 2/100: 364it [08:01,  2.31it/s, loss=4.16]epoch 2/100: 364it [08:02,  2.31it/s, loss=4.18]epoch 2/100: 364it [08:02,  2.31it/s, loss=4.12]epoch 2/100: 365it [08:02,  2.39it/s, loss=4.18]epoch 2/100: 365it [08:02,  2.39it/s, loss=4.12]epoch 2/100: 365it [08:02,  2.39it/s, loss=4.1] epoch 2/100: 365it [08:02,  2.39it/s, loss=4.1] epoch 2/100: 366it [08:02,  2.45it/s, loss=4.1]epoch 2/100: 366it [08:02,  2.45it/s, loss=4.1]epoch 2/100: 366it [08:02,  2.45it/s, loss=4.09]epoch 2/100: 366it [08:02,  2.45it/s, loss=3.93]epoch 2/100: 367it [08:02,  2.49it/s, loss=4.09]epoch 2/100: 367it [08:02,  2.49it/s, loss=3.93]epoch 2/100: 367it [08:03,  2.49it/s, loss=4.14]epoch 2/100: 367it [08:03,  2.49it/s, loss=3.7] epoch 2/100: 368it [08:03,  2.52it/s, loss=4.14]epoch 2/100: 368it [08:03,  2.52it/s, loss=3.7]epoch 2/100: 368it [08:03,  2.52it/s, loss=4.18]epoch 2/100: 368it [08:03,  2.52it/s, loss=4.12]epoch 2/100: 369it [08:03,  2.54it/s, loss=4.18]epoch 2/100: 369it [08:03,  2.54it/s, loss=4.12]epoch 2/100: 369it [08:04,  2.54it/s, loss=4.17]epoch 2/100: 369it [08:04,  2.54it/s, loss=4.05]epoch 2/100: 370it [08:04,  2.55it/s, loss=4.17]epoch 2/100: 370it [08:04,  2.55it/s, loss=4.05]epoch 2/100: 370it [08:04,  2.55it/s, loss=4.2] epoch 2/100: 370it [08:04,  2.55it/s, loss=4.09]epoch 2/100: 371it [08:04,  2.57it/s, loss=4.2]epoch 2/100: 371it [08:04,  2.57it/s, loss=4.09]epoch 2/100: 371it [08:04,  2.57it/s, loss=3.96]epoch 2/100: 371it [08:04,  2.57it/s, loss=4.11]epoch 2/100: 372it [08:04,  2.57it/s, loss=3.96]epoch 2/100: 372it [08:04,  2.57it/s, loss=4.11]epoch 2/100: 372it [08:05,  2.57it/s, loss=4.19]epoch 2/100: 372it [08:05,  2.57it/s, loss=4.12]epoch 2/100: 373it [08:05,  2.58it/s, loss=4.19]epoch 2/100: 373it [08:05,  2.58it/s, loss=4.12]epoch 2/100: 373it [08:05,  2.58it/s, loss=3.93]epoch 2/100: 373it [08:05,  2.58it/s, loss=4.13]epoch 2/100: 374it [08:05,  2.58it/s, loss=3.93]epoch 2/100: 374it [08:05,  2.58it/s, loss=4.13]epoch 2/100: 374it [08:05,  2.58it/s, loss=4.11]epoch 2/100: 374it [08:05,  2.58it/s, loss=4.19]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:23, 23.09s/it][A
val: 1it [00:23, 23.10s/it][A
val: 2it [00:23,  9.66s/it][A
val: 2it [00:23,  9.66s/it][A
val: 3it [00:23,  5.36s/it][A
val: 3it [00:23,  5.36s/it][A

val: 4it [00:23,  3.34s/it][Aval: 4it [00:23,  3.34s/it][A
val: 5it [00:24,  2.23s/it][A
val: 5it [00:24,  2.23s/it][A

val: 6it [00:24,  1.56s/it][Aval: 6it [00:24,  1.56s/it][A
val: 7it [00:24,  1.13s/it][A
val: 7it [00:24,  1.13s/it][A
val: 8it [00:24,  1.18it/s][A
val: 8it [00:24,  1.18it/s][A
val: 9it [00:25,  1.51it/s][A
val: 9it [00:25,  1.51it/s][A
val: 10it [00:25,  1.87it/s][A
val: 10it [00:25,  1.87it/s][A
val: 11it [00:25,  2.23it/s][A
val: 11it [00:25,  2.23it/s][A
val: 12it [00:25,  2.58it/s][A
val: 12it [00:25,  2.58it/s][A
val: 13it [00:26,  2.89it/s][A
val: 13it [00:26,  2.89it/s][A
val: 14it [00:26,  3.15it/s][A
val: 14it [00:26,  3.15it/s][A
val: 15it [00:26,  3.37it/s][A
val: 15it [00:26,  3.36it/s][A
val: 16it [00:26,  3.54it/s][A
val: 16it [00:26,  3.53it/s][A
val: 17it [00:27,  3.66it/s][A
val: 17it [00:27,  3.66it/s][A
val: 18it [00:27,  3.76it/s][A
val: 18it [00:27,  3.76it/s][A
val: 19it [00:27,  3.82it/s][A
val: 19it [00:27,  3.83it/s][A
val: 20it [00:27,  3.87it/s][A
val: 20it [00:27,  3.88it/s][A
val: 21it [00:28,  3.91it/s][A
val: 21it [00:28,  3.91it/s][A
val: 22it [00:28,  3.94it/s][A
val: 22it [00:28,  3.93it/s][A
val: 23it [00:28,  3.96it/s][A
val: 23it [00:28,  3.95it/s][A
val: 24it [00:28,  3.97it/s][A
val: 24it [00:28,  3.96it/s][A
                            [A
                            [Aepoch 2/100: 374it [08:34,  2.58it/s, loss=4.19]epoch 2/100: 374it [08:43,  2.58it/s, best_val=4.06, loss=4.11, val=4.06]epoch 2/100: 375it [08:43, 11.63s/it, loss=4.19]epoch 2/100: 375it [08:43, 11.63s/it, best_val=4.06, loss=4.11, val=4.06]epoch 2/100: 375it [08:43, 11.63s/it, loss=3.98]epoch 2/100: 375it [08:43, 11.63s/it, loss=4.11]                         epoch 2/100: 376it [08:43,  8.26s/it, loss=3.98]epoch 2/100: 376it [08:43,  8.26s/it, loss=4.11]epoch 2/100: 376it [08:44,  8.26s/it, loss=4.25]epoch 2/100: 376it [08:44,  8.26s/it, loss=4.01]epoch 2/100: 377it [08:44,  5.90s/it, loss=4.25]epoch 2/100: 377it [08:44,  5.90s/it, loss=4.01]epoch 2/100: 377it [08:44,  5.90s/it, loss=3.94]epoch 2/100: 377it [08:44,  5.90s/it, loss=4.04]epoch 2/100: 378it [08:44,  4.25s/it, loss=3.94]epoch 2/100: 378it [08:44,  4.25s/it, loss=4.04]epoch 2/100: 378it [08:44,  4.25s/it, loss=3.78]epoch 2/100: 378it [08:44,  4.25s/it, loss=4.23]epoch 2/100: 379it [08:44,  3.09s/it, loss=3.78]epoch 2/100: 379it [08:44,  3.09s/it, loss=4.23]epoch 2/100: 379it [08:45,  3.09s/it, loss=3.97]epoch 2/100: 379it [08:45,  3.09s/it, loss=4.12]epoch 2/100: 380it [08:45,  2.28s/it, loss=3.97]epoch 2/100: 380it [08:45,  2.28s/it, loss=4.12]epoch 2/100: 380it [08:45,  2.28s/it, loss=4.04]epoch 2/100: 380it [08:45,  2.28s/it, loss=3.99]epoch 2/100: 381it [08:45,  1.71s/it, loss=4.04]epoch 2/100: 381it [08:45,  1.71s/it, loss=3.99]epoch 2/100: 381it [08:46,  1.71s/it, loss=3.95]epoch 2/100: 381it [08:46,  1.71s/it, loss=4.21]epoch 2/100: 382it [08:46,  1.31s/it, loss=3.95]epoch 2/100: 382it [08:46,  1.31s/it, loss=4.21]epoch 2/100: 382it [08:46,  1.31s/it, loss=4.26]epoch 2/100: 382it [08:46,  1.31s/it, loss=4]   epoch 2/100: 383it [08:46,  1.03s/it, loss=4.26]epoch 2/100: 383it [08:46,  1.03s/it, loss=4]epoch 2/100: 383it [08:46,  1.03s/it, loss=4]   epoch 2/100: 383it [08:46,  1.03s/it, loss=4.28]epoch 2/100: 384it [08:46,  1.19it/s, loss=4]epoch 2/100: 384it [08:46,  1.19it/s, loss=4.28]epoch 2/100: 384it [08:47,  1.19it/s, loss=4.06]epoch 2/100: 384it [08:47,  1.19it/s, loss=4.15]epoch 2/100: 385it [08:47,  1.42it/s, loss=4.06]epoch 2/100: 385it [08:47,  1.42it/s, loss=4.15]epoch 2/100: 385it [08:47,  1.42it/s, loss=4.06]epoch 2/100: 385it [08:47,  1.42it/s, loss=4.04]epoch 2/100: 386it [08:47,  1.64it/s, loss=4.06]epoch 2/100: 386it [08:47,  1.64it/s, loss=4.04]epoch 2/100: 386it [08:48,  1.64it/s, loss=3.99]epoch 2/100: 386it [08:48,  1.64it/s, loss=4.17]epoch 2/100: 387it [08:48,  1.85it/s, loss=4.17]epoch 2/100: 387it [08:48,  1.85it/s, loss=3.99]epoch 2/100: 387it [08:48,  1.85it/s, loss=4.05]epoch 2/100: 387it [08:48,  1.85it/s, loss=4.19]epoch 2/100: 388it [08:48,  2.02it/s, loss=4.05]epoch 2/100: 388it [08:48,  2.02it/s, loss=4.19]epoch 2/100: 388it [08:48,  2.02it/s, loss=4.22]epoch 2/100: 388it [08:48,  2.02it/s, loss=4.11]epoch 2/100: 389it [08:48,  2.17it/s, loss=4.22]epoch 2/100: 389it [08:48,  2.17it/s, loss=4.11]epoch 2/100: 389it [08:49,  2.17it/s, loss=4.05]epoch 2/100: 389it [08:49,  2.17it/s, loss=4.11]epoch 2/100: 390it [08:49,  2.28it/s, loss=4.05]epoch 2/100: 390it [08:49,  2.28it/s, loss=4.11]epoch 2/100: 390it [08:49,  2.28it/s, loss=4.04]epoch 2/100: 390it [08:49,  2.28it/s, loss=4.06]epoch 2/100: 391it [08:49,  2.37it/s, loss=4.04]epoch 2/100: 391it [08:49,  2.37it/s, loss=4.06]epoch 2/100: 391it [08:49,  2.37it/s, loss=4.17]epoch 2/100: 391it [08:50,  2.37it/s, loss=4.02]epoch 2/100: 392it [08:49,  2.43it/s, loss=4.17]epoch 2/100: 392it [08:50,  2.43it/s, loss=4.02]epoch 2/100: 392it [08:50,  2.43it/s, loss=4]   epoch 2/100: 392it [08:50,  2.43it/s, loss=3.83]epoch 2/100: 393it [08:50,  2.47it/s, loss=4]epoch 2/100: 393it [08:50,  2.47it/s, loss=3.83]epoch 2/100: 393it [08:50,  2.47it/s, loss=4.12]epoch 2/100: 393it [08:50,  2.47it/s, loss=4.05]epoch 2/100: 394it [08:50,  2.51it/s, loss=4.12]epoch 2/100: 394it [08:50,  2.51it/s, loss=4.05]epoch 2/100: 394it [08:51,  2.51it/s, loss=4.07]epoch 2/100: 394it [08:51,  2.51it/s, loss=4.01]epoch 2/100: 395it [08:51,  2.53it/s, loss=4.07]epoch 2/100: 395it [08:51,  2.53it/s, loss=4.01]epoch 2/100: 395it [08:51,  2.53it/s, loss=3.95]epoch 2/100: 395it [08:51,  2.53it/s, loss=3.93]epoch 2/100: 396it [08:51,  2.55it/s, loss=3.95]epoch 2/100: 396it [08:51,  2.55it/s, loss=3.93]epoch 2/100: 396it [08:51,  2.55it/s, loss=3.77]epoch 2/100: 396it [08:51,  2.55it/s, loss=4.03]epoch 2/100: 397it [08:51,  2.56it/s, loss=4.03]epoch 2/100: 397it [08:51,  2.56it/s, loss=3.77]epoch 2/100: 397it [08:52,  2.56it/s, loss=3.95]epoch 2/100: 397it [08:52,  2.56it/s, loss=3.92]epoch 2/100: 398it [08:52,  2.57it/s, loss=3.95]epoch 2/100: 398it [08:52,  2.57it/s, loss=3.92]epoch 2/100: 398it [08:52,  2.57it/s, loss=4.04]epoch 2/100: 398it [08:52,  2.57it/s, loss=4.12]epoch 2/100: 399it [08:52,  2.58it/s, loss=4.04]epoch 2/100: 399it [08:52,  2.58it/s, loss=4.12]epoch 2/100: 399it [08:53,  2.58it/s, loss=4.32]epoch 2/100: 399it [08:53,  2.58it/s, loss=3.9] 
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.38s/it][A
val: 1it [00:03,  3.39s/it][A
val: 2it [00:03,  1.54s/it][A
val: 2it [00:03,  1.54s/it][A
val: 3it [00:03,  1.05it/s][A
val: 3it [00:03,  1.05it/s][A
val: 4it [00:04,  1.48it/s][A
val: 4it [00:04,  1.48it/s][A
val: 5it [00:04,  1.91it/s][A
val: 5it [00:04,  1.91it/s][A
val: 6it [00:04,  2.32it/s][A
val: 6it [00:04,  2.32it/s][A
val: 7it [00:04,  2.68it/s][A
val: 7it [00:04,  2.68it/s][A
val: 8it [00:05,  2.99it/s][A
val: 8it [00:05,  2.98it/s][A
val: 9it [00:05,  3.24it/s][A
val: 9it [00:05,  3.22it/s][A
val: 10it [00:05,  3.43it/s][A
val: 10it [00:05,  3.41it/s][A
val: 11it [00:05,  3.58it/s][A
val: 11it [00:05,  3.56it/s][A
val: 12it [00:06,  3.69it/s][A
val: 12it [00:06,  3.66it/s][A
val: 13it [00:06,  3.76it/s][A
val: 13it [00:06,  3.74it/s][A
val: 14it [00:06,  3.82it/s][A
val: 14it [00:06,  3.80it/s][A
val: 15it [00:06,  3.86it/s][A
val: 15it [00:06,  3.84it/s][A
val: 16it [00:07,  3.89it/s][A
val: 16it [00:07,  3.86it/s][A
val: 17it [00:07,  3.91it/s][A
val: 17it [00:07,  3.89it/s][A
val: 18it [00:07,  3.92it/s][A
val: 18it [00:07,  3.91it/s][A
val: 19it [00:07,  3.94it/s][A
val: 19it [00:07,  3.92it/s][A
val: 20it [00:08,  3.94it/s][A
val: 20it [00:08,  3.92it/s][A
val: 21it [00:08,  3.94it/s][A
val: 21it [00:08,  3.93it/s][A
val: 22it [00:08,  3.95it/s][A
val: 22it [00:08,  3.94it/s][A
val: 23it [00:08,  3.96it/s][A
val: 23it [00:08,  3.95it/s][A
val: 24it [00:09,  3.96it/s][A
val: 24it [00:09,  3.96it/s][A
                            [A
                            [Aepoch 2/100: 399it [09:02,  2.58it/s, loss=4.32]epoch 2/100: 399it [09:12,  2.58it/s, best_val=4.05, loss=3.9, val=4.05]epoch 2/100: 400it [09:12,  6.08s/it, loss=4.32]epoch 2/100: 400it [09:12,  6.08s/it, best_val=4.05, loss=3.9, val=4.05]epoch 2/100: 400it [09:12,  6.08s/it, loss=4.09]epoch 2/100: 400it [09:12,  6.08s/it, loss=4.03]                        epoch 2/100: 401it [09:12,  4.38s/it, loss=4.09]epoch 2/100: 401it [09:12,  4.38s/it, loss=4.03]epoch 2/100: 401it [09:12,  4.38s/it, loss=4.08]epoch 2/100: 401it [09:12,  4.38s/it, loss=4.07]epoch 2/100: 402it [09:12,  3.18s/it, loss=4.08]epoch 2/100: 402it [09:12,  3.18s/it, loss=4.07]epoch 2/100: 402it [09:13,  3.18s/it, loss=4.04]epoch 2/100: 402it [09:13,  3.18s/it, loss=4.22]epoch 2/100: 403it [09:13,  2.34s/it, loss=4.04]epoch 2/100: 403it [09:13,  2.34s/it, loss=4.22]epoch 2/100: 403it [09:13,  2.34s/it, loss=4.09]epoch 2/100: 403it [09:13,  2.34s/it, loss=4.04]epoch 2/100: 404it [09:13,  1.76s/it, loss=4.09]epoch 2/100: 404it [09:13,  1.76s/it, loss=4.04]epoch 2/100: 404it [09:14,  1.76s/it, loss=3.93]epoch 2/100: 404it [09:14,  1.76s/it, loss=3.87]epoch 2/100: 405it [09:14,  1.34s/it, loss=3.93]epoch 2/100: 405it [09:14,  1.34s/it, loss=3.87]epoch 2/100: 405it [09:14,  1.34s/it, loss=4.05]epoch 2/100: 405it [09:14,  1.34s/it, loss=4.08]epoch 2/100: 406it [09:14,  1.06s/it, loss=4.05]epoch 2/100: 406it [09:14,  1.06s/it, loss=4.08]epoch 2/100: 406it [09:14,  1.06s/it, loss=4.15]epoch 2/100: 406it [09:14,  1.06s/it, loss=3.92]epoch 2/100: 407it [09:14,  1.17it/s, loss=4.15]epoch 2/100: 407it [09:14,  1.17it/s, loss=3.92]epoch 2/100: 407it [09:15,  1.17it/s, loss=4]   epoch 2/100: 407it [09:15,  1.17it/s, loss=4.06]epoch 2/100: 408it [09:15,  1.40it/s, loss=4]epoch 2/100: 408it [09:15,  1.40it/s, loss=4.06]epoch 2/100: 408it [09:15,  1.40it/s, loss=4.21]epoch 2/100: 408it [09:15,  1.40it/s, loss=3.94]epoch 2/100: 409it [09:15,  1.62it/s, loss=4.21]epoch 2/100: 409it [09:15,  1.62it/s, loss=3.94]epoch 2/100: 409it [09:15,  1.62it/s, loss=4.36]epoch 2/100: 409it [09:15,  1.62it/s, loss=3.99]epoch 2/100: 410it [09:15,  1.83it/s, loss=4.36]epoch 2/100: 410it [09:15,  1.83it/s, loss=3.99]epoch 2/100: 410it [09:16,  1.83it/s, loss=3.91]epoch 2/100: 410it [09:16,  1.83it/s, loss=4]   epoch 2/100: 411it [09:16,  2.01it/s, loss=4]epoch 2/100: 411it [09:16,  2.01it/s, loss=3.91]epoch 2/100: 411it [09:16,  2.01it/s, loss=4.18]epoch 2/100: 411it [09:16,  2.01it/s, loss=4.07]epoch 2/100: 412it [09:16,  2.15it/s, loss=4.18]epoch 2/100: 412it [09:16,  2.15it/s, loss=4.07]epoch 2/100: 412it [09:17,  2.15it/s, loss=4.07]epoch 2/100: 412it [09:17,  2.15it/s, loss=4.22]epoch 2/100: 413it [09:17,  2.27it/s, loss=4.22]epoch 2/100: 413it [09:17,  2.27it/s, loss=4.07]epoch 2/100: 413it [09:17,  2.27it/s, loss=4.13]epoch 2/100: 413it [09:17,  2.27it/s, loss=4.02]epoch 2/100: 414it [09:17,  2.36it/s, loss=4.02]epoch 2/100: 414it [09:17,  2.36it/s, loss=4.13]epoch 2/100: 414it [09:17,  2.36it/s, loss=4.18]epoch 2/100: 414it [09:17,  2.36it/s, loss=4.24]epoch 2/100: 415it [09:17,  2.43it/s, loss=4.24]epoch 2/100: 415it [09:17,  2.43it/s, loss=4.18]epoch 2/100: 415it [09:18,  2.43it/s, loss=4.06]epoch 2/100: 415it [09:18,  2.43it/s, loss=4.02]epoch 2/100: 416it [09:18,  2.48it/s, loss=4.06]epoch 2/100: 416it [09:18,  2.48it/s, loss=4.02]epoch 2/100: 416it [09:18,  2.48it/s, loss=4.09]epoch 2/100: 416it [09:18,  2.48it/s, loss=4.2] epoch 2/100: 417it [09:18,  2.51it/s, loss=4.09]epoch 2/100: 417it [09:18,  2.51it/s, loss=4.2]epoch 2/100: 417it [09:19,  2.51it/s, loss=4.15]epoch 2/100: 417it [09:19,  2.51it/s, loss=4.15]epoch 2/100: 418it [09:19,  2.54it/s, loss=4.15]epoch 2/100: 418it [09:19,  2.54it/s, loss=4.15]epoch 2/100: 418it [09:19,  2.54it/s, loss=4.09]epoch 2/100: 418it [09:19,  2.54it/s, loss=4.07]epoch 2/100: 419it [09:19,  2.56it/s, loss=4.09]epoch 2/100: 419it [09:19,  2.56it/s, loss=4.07]epoch 2/100: 419it [09:19,  2.56it/s, loss=3.96]epoch 2/100: 419it [09:19,  2.56it/s, loss=4.18]epoch 2/100: 420it [09:19,  2.57it/s, loss=4.18]epoch 2/100: 420it [09:19,  2.57it/s, loss=3.96]epoch 2/100: 420it [09:20,  2.57it/s, loss=3.99]epoch 2/100: 420it [09:20,  2.57it/s, loss=4.16]epoch 2/100: 421it [09:20,  2.58it/s, loss=3.99]epoch 2/100: 421it [09:20,  2.58it/s, loss=4.16]epoch 2/100: 421it [09:20,  2.58it/s, loss=4.27]epoch 2/100: 421it [09:20,  2.58it/s, loss=4.09]epoch 2/100: 422it [09:20,  2.58it/s, loss=4.09]epoch 2/100: 422it [09:20,  2.58it/s, loss=4.27]epoch 2/100: 422it [09:20,  2.58it/s, loss=3.87]epoch 2/100: 422it [09:20,  2.58it/s, loss=4.11]epoch 2/100: 423it [09:20,  2.59it/s, loss=3.87]epoch 2/100: 423it [09:20,  2.59it/s, loss=4.11]epoch 2/100: 423it [09:21,  2.59it/s, loss=4.26]epoch 2/100: 423it [09:21,  2.59it/s, loss=4.16]epoch 2/100: 424it [09:21,  2.59it/s, loss=4.16]epoch 2/100: 424it [09:21,  2.59it/s, loss=4.26]epoch 2/100: 424it [09:21,  2.59it/s, loss=4.17]epoch 2/100: 424it [09:21,  2.59it/s, loss=4.07]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.55s/it][A
val: 1it [00:03,  3.56s/it][A
val: 2it [00:03,  1.61s/it][A
val: 2it [00:03,  1.61s/it][A
val: 3it [00:04,  1.01it/s][A
val: 3it [00:04,  1.01it/s][A
val: 4it [00:04,  1.43it/s][A
val: 4it [00:04,  1.43it/s][A
val: 5it [00:04,  1.86it/s][A
val: 5it [00:04,  1.86it/s][A
val: 6it [00:04,  2.28it/s][A
val: 6it [00:04,  2.27it/s][A
val: 7it [00:05,  2.65it/s][A
val: 7it [00:05,  2.64it/s][A
val: 8it [00:05,  2.96it/s][A
val: 8it [00:05,  2.95it/s][A
val: 9it [00:05,  3.22it/s][A
val: 9it [00:05,  3.21it/s][A
val: 10it [00:05,  3.43it/s][A
val: 10it [00:05,  3.42it/s][A
val: 11it [00:06,  3.58it/s][A
val: 11it [00:06,  3.57it/s][A
val: 12it [00:06,  3.70it/s][A
val: 12it [00:06,  3.69it/s][A
val: 13it [00:06,  3.79it/s][A
val: 13it [00:06,  3.77it/s][A
val: 14it [00:06,  3.84it/s][A
val: 14it [00:06,  3.83it/s][A
val: 15it [00:07,  3.89it/s][A
val: 15it [00:07,  3.87it/s][A
val: 16it [00:07,  3.92it/s][A
val: 16it [00:07,  3.90it/s][A
val: 17it [00:07,  3.94it/s][A
val: 17it [00:07,  3.93it/s][A
val: 18it [00:07,  3.96it/s][A
val: 18it [00:07,  3.95it/s][A
val: 19it [00:08,  3.97it/s][A
val: 19it [00:08,  3.94it/s][A
val: 20it [00:08,  3.98it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.98it/s][A
val: 21it [00:08,  3.96it/s][A
val: 22it [00:08,  3.98it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:09,  3.99it/s][A
val: 23it [00:09,  3.97it/s][A
val: 24it [00:09,  3.99it/s][A
val: 24it [00:09,  3.97it/s][A
                            [A
                            [Aepoch 2/100: 424it [09:31,  2.59it/s, loss=4.17]epoch 2/100: 424it [09:40,  2.59it/s, best_val=4.04, loss=4.07, val=4.04]epoch 2/100: 425it [09:40,  6.04s/it, loss=4.17]epoch 2/100: 425it [09:40,  6.04s/it, best_val=4.04, loss=4.07, val=4.04]epoch 2/100: 425it [09:40,  6.04s/it, loss=3.92]epoch 2/100: 425it [09:40,  6.04s/it, loss=4]                            epoch 2/100: 426it [09:40,  4.35s/it, loss=3.92]epoch 2/100: 426it [09:40,  4.35s/it, loss=4]epoch 2/100: 426it [09:41,  4.35s/it, loss=3.88]epoch 2/100: 426it [09:41,  4.35s/it, loss=4.02]epoch 2/100: 427it [09:41,  3.16s/it, loss=3.88]epoch 2/100: 427it [09:41,  3.16s/it, loss=4.02]epoch 2/100: 427it [09:41,  3.16s/it, loss=3.98]epoch 2/100: 427it [09:41,  3.16s/it, loss=3.87]epoch 2/100: 428it [09:41,  2.33s/it, loss=3.87]epoch 2/100: 428it [09:41,  2.33s/it, loss=3.98]epoch 2/100: 428it [09:42,  2.33s/it, loss=3.98]epoch 2/100: 428it [09:42,  2.33s/it, loss=4.07]epoch 2/100: 429it [09:42,  1.74s/it, loss=3.98]epoch 2/100: 429it [09:42,  1.74s/it, loss=4.07]epoch 2/100: 429it [09:42,  1.74s/it, loss=4.13]epoch 2/100: 429it [09:42,  1.74s/it, loss=4.03]epoch 2/100: 430it [09:42,  1.34s/it, loss=4.03]epoch 2/100: 430it [09:42,  1.34s/it, loss=4.13]epoch 2/100: 430it [09:42,  1.34s/it, loss=3.99]epoch 2/100: 430it [09:42,  1.34s/it, loss=4.13]epoch 2/100: 431it [09:42,  1.05s/it, loss=4.13]epoch 2/100: 431it [09:42,  1.05s/it, loss=3.99]epoch 2/100: 431it [09:43,  1.05s/it, loss=4.2] epoch 2/100: 431it [09:43,  1.05s/it, loss=3.97]epoch 2/100: 432it [09:43,  1.17it/s, loss=3.97]epoch 2/100: 432it [09:43,  1.17it/s, loss=4.2]epoch 2/100: 432it [09:43,  1.17it/s, loss=4.11]epoch 2/100: 432it [09:43,  1.17it/s, loss=3.97]epoch 2/100: 433it [09:43,  1.41it/s, loss=3.97]epoch 2/100: 433it [09:43,  1.41it/s, loss=4.11]epoch 2/100: 433it [09:44,  1.41it/s, loss=4.15]epoch 2/100: 433it [09:44,  1.41it/s, loss=4.02]epoch 2/100: 434it [09:44,  1.63it/s, loss=4.15]epoch 2/100: 434it [09:44,  1.63it/s, loss=4.02]epoch 2/100: 434it [09:44,  1.63it/s, loss=4.05]epoch 2/100: 434it [09:44,  1.63it/s, loss=4.14]epoch 2/100: 435it [09:44,  1.83it/s, loss=4.14]epoch 2/100: 435it [09:44,  1.83it/s, loss=4.05]epoch 2/100: 435it [09:44,  1.83it/s, loss=3.97]epoch 2/100: 435it [09:44,  1.83it/s, loss=3.84]epoch 2/100: 436it [09:44,  2.01it/s, loss=3.97]epoch 2/100: 436it [09:44,  2.01it/s, loss=3.84]epoch 2/100: 436it [09:45,  2.01it/s, loss=4.1] epoch 2/100: 436it [09:45,  2.01it/s, loss=3.93]epoch 2/100: 437it [09:45,  2.16it/s, loss=4.1]epoch 2/100: 437it [09:45,  2.16it/s, loss=3.93]epoch 2/100: 437it [09:45,  2.16it/s, loss=3.99]epoch 2/100: 437it [09:45,  2.16it/s, loss=4.1] epoch 2/100: 438it [09:45,  2.27it/s, loss=3.99]epoch 2/100: 438it [09:45,  2.27it/s, loss=4.1]epoch 2/100: 438it [09:45,  2.27it/s, loss=4]   epoch 2/100: 438it [09:45,  2.27it/s, loss=4.03]epoch 2/100: 439it [09:45,  2.36it/s, loss=4]epoch 2/100: 439it [09:45,  2.36it/s, loss=4.03]epoch 2/100: 439it [09:46,  2.36it/s, loss=4.06]epoch 2/100: 439it [09:46,  2.36it/s, loss=4.08]epoch 2/100: 440it [09:46,  2.43it/s, loss=4.06]epoch 2/100: 440it [09:46,  2.43it/s, loss=4.08]epoch 2/100: 440it [09:46,  2.43it/s, loss=4.25]epoch 2/100: 440it [09:46,  2.43it/s, loss=3.91]epoch 2/100: 441it [09:46,  2.47it/s, loss=4.25]epoch 2/100: 441it [09:46,  2.47it/s, loss=3.91]epoch 2/100: 441it [09:47,  2.47it/s, loss=4.05]epoch 2/100: 441it [09:47,  2.47it/s, loss=4.03]epoch 2/100: 442it [09:47,  2.51it/s, loss=4.03]epoch 2/100: 442it [09:47,  2.51it/s, loss=4.05]epoch 2/100: 442it [09:47,  2.51it/s, loss=4.13]epoch 2/100: 442it [09:47,  2.51it/s, loss=4.21]epoch 2/100: 443it [09:47,  2.53it/s, loss=4.13]epoch 2/100: 443it [09:47,  2.53it/s, loss=4.21]epoch 2/100: 443it [09:47,  2.53it/s, loss=4.11]epoch 2/100: 443it [09:47,  2.53it/s, loss=4.04]epoch 2/100: 444it [09:47,  2.55it/s, loss=4.11]epoch 2/100: 444it [09:47,  2.55it/s, loss=4.04]epoch 2/100: 444it [09:48,  2.55it/s, loss=4.08]epoch 2/100: 444it [09:48,  2.55it/s, loss=4.2] epoch 2/100: 445it [09:48,  2.56it/s, loss=4.08]epoch 2/100: 445it [09:48,  2.56it/s, loss=4.2]epoch 2/100: 445it [09:48,  2.56it/s, loss=4.09]epoch 2/100: 445it [09:48,  2.56it/s, loss=4.2]epoch 2/100: 446it [09:48,  2.57it/s, loss=4.09]epoch 2/100: 446it [09:48,  2.57it/s, loss=4.2]epoch 2/100: 446it [09:49,  2.57it/s, loss=3.96]epoch 2/100: 446it [09:49,  2.57it/s, loss=4.1]epoch 2/100: 447it [09:49,  2.57it/s, loss=3.96]epoch 2/100: 447it [09:49,  2.57it/s, loss=4.1]epoch 2/100: 447it [09:49,  2.57it/s, loss=3.82]epoch 2/100: 447it [09:49,  2.57it/s, loss=4.15]epoch 2/100: 448it [09:49,  2.58it/s, loss=3.82]epoch 2/100: 448it [09:49,  2.58it/s, loss=4.15]epoch 2/100: 448it [09:49,  2.58it/s, loss=3.96]epoch 2/100: 448it [09:49,  2.58it/s, loss=4]   epoch 2/100: 449it [09:49,  2.58it/s, loss=3.96]epoch 2/100: 449it [09:49,  2.58it/s, loss=4]epoch 2/100: 449it [09:50,  2.58it/s, loss=3.83]epoch 2/100: 449it [09:50,  2.58it/s, loss=4.22]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.79s/it][A
val: 2it [00:04,  1.71s/it][A
val: 3it [00:04,  1.04s/it][A
val: 4it [00:04,  1.37it/s][A
val: 5it [00:04,  1.79it/s][A
val: 6it [00:05,  2.21it/s][A
val: 7it [00:05,  2.59it/s][A
val: 8it [00:05,  2.91it/s][A
val: 9it [00:05,  3.18it/s][A
val: 10it [00:06,  3.40it/s][A
val: 11it [00:06,  3.56it/s][A
val: 12it [00:06,  3.68it/s][A
val: 13it [00:06,  3.77it/s][A
val: 14it [00:07,  3.84it/s][A
val: 15it [00:07,  3.88it/s][A
val: 16it [00:07,  3.91it/s][A
val: 17it [00:07,  3.94it/s][A
val: 18it [00:08,  3.95it/s][A
val: 19it [00:08,  3.96it/s][A
val: 20it [00:08,  3.97it/s][A
val: 21it [00:08,  3.98it/s][A
val: 22it [00:09,  3.98it/s][A
val: 23it [00:09,  3.98it/s][A
val: 24it [00:09,  3.99it/s][A
                            [A
val: 1it [00:16, 16.07s/it][A
val: 2it [00:16,  6.76s/it][A
val: 3it [00:16,  3.79s/it][A
val: 4it [00:16,  2.39s/it][A
val: 5it [00:17,  1.62s/it][A
val: 6it [00:17,  1.15s/it][A
val: 7it [00:17,  1.16it/s][A
val: 8it [00:17,  1.50it/s][A
val: 9it [00:18,  1.87it/s][A
val: 10it [00:18,  2.23it/s][A
val: 11it [00:18,  2.58it/s][A
val: 12it [00:18,  2.89it/s][A
val: 13it [00:19,  3.15it/s][A
val: 14it [00:19,  3.36it/s][A
val: 15it [00:19,  3.53it/s][A
val: 16it [00:19,  3.66it/s][A
val: 17it [00:20,  3.75it/s][A
val: 18it [00:20,  3.83it/s][A
val: 19it [00:20,  3.87it/s][A
val: 20it [00:20,  3.91it/s][A
val: 21it [00:21,  3.93it/s][A
val: 22it [00:21,  3.95it/s][A
val: 23it [00:21,  3.96it/s][A
val: 24it [00:21,  3.97it/s][A
                            [Aepoch 2/100: 449it [10:12,  2.58it/s, loss=3.83]epoch 2/100: 449it [10:21,  2.58it/s, best_val=4.02, loss=4.22, val=4.02]epoch 2/100: 450it [10:21,  9.87s/it, loss=3.83]epoch 2/100: 450it [10:21,  9.87s/it, best_val=4.02, loss=4.22, val=4.02]epoch 2/100: 450it [10:22,  9.87s/it, loss=4.27]epoch 2/100: 450it [10:22,  9.87s/it, loss=4.14]                         epoch 2/100: 451it [10:22,  7.03s/it, loss=4.27]epoch 2/100: 451it [10:22,  7.03s/it, loss=4.14]epoch 2/100: 451it [10:22,  7.03s/it, loss=3.96]epoch 2/100: 451it [10:22,  7.03s/it, loss=4.03]epoch 2/100: 452it [10:22,  5.03s/it, loss=3.96]epoch 2/100: 452it [10:22,  5.03s/it, loss=4.03]epoch 2/100: 452it [10:22,  5.03s/it, loss=4.09]epoch 2/100: 452it [10:22,  5.03s/it, loss=3.98]epoch 2/100: 453it [10:22,  3.64s/it, loss=3.98]epoch 2/100: 453it [10:22,  3.64s/it, loss=4.09]epoch 2/100: 453it [10:23,  3.64s/it, loss=3.91]epoch 2/100: 453it [10:23,  3.64s/it, loss=3.99]epoch 2/100: 454it [10:23,  2.66s/it, loss=3.91]epoch 2/100: 454it [10:23,  2.66s/it, loss=3.99]epoch 2/100: 454it [10:23,  2.66s/it, loss=3.9] epoch 2/100: 454it [10:23,  2.66s/it, loss=3.9] epoch 2/100: 455it [10:23,  1.98s/it, loss=3.9]epoch 2/100: 455it [10:23,  1.98s/it, loss=3.9]epoch 2/100: 455it [10:24,  1.98s/it, loss=3.92]epoch 2/100: 455it [10:24,  1.98s/it, loss=4.25]epoch 2/100: 456it [10:24,  1.50s/it, loss=3.92]epoch 2/100: 456it [10:24,  1.50s/it, loss=4.25]epoch 2/100: 456it [10:24,  1.50s/it, loss=4.05]epoch 2/100: 456it [10:24,  1.50s/it, loss=3.96]epoch 2/100: 457it [10:24,  1.17s/it, loss=4.05]epoch 2/100: 457it [10:24,  1.17s/it, loss=3.96]epoch 2/100: 457it [10:24,  1.17s/it, loss=4.25]epoch 2/100: 457it [10:24,  1.17s/it, loss=4.16]epoch 2/100: 458it [10:24,  1.07it/s, loss=4.25]epoch 2/100: 458it [10:24,  1.07it/s, loss=4.16]epoch 2/100: 458it [10:25,  1.07it/s, loss=4.02]epoch 2/100: 458it [10:25,  1.07it/s, loss=4.03]epoch 2/100: 459it [10:25,  1.30it/s, loss=4.03]epoch 2/100: 459it [10:25,  1.30it/s, loss=4.02]epoch 2/100: 459it [10:25,  1.30it/s, loss=4.02]epoch 2/100: 459it [10:25,  1.30it/s, loss=3.87]epoch 2/100: 460it [10:25,  1.53it/s, loss=4.02]epoch 2/100: 460it [10:25,  1.53it/s, loss=3.87]epoch 2/100: 460it [10:26,  1.53it/s, loss=4.31]epoch 2/100: 460it [10:26,  1.53it/s, loss=3.98]epoch 2/100: 461it [10:26,  1.75it/s, loss=3.98]epoch 2/100: 461it [10:26,  1.75it/s, loss=4.31]epoch 2/100: 461it [10:26,  1.75it/s, loss=4.13]epoch 2/100: 461it [10:26,  1.75it/s, loss=4.14]epoch 2/100: 462it [10:26,  1.94it/s, loss=4.13]epoch 2/100: 462it [10:26,  1.94it/s, loss=4.14]epoch 2/100: 462it [10:26,  1.94it/s, loss=3.97]epoch 2/100: 462it [10:26,  1.94it/s, loss=4.05]epoch 2/100: 463it [10:26,  2.10it/s, loss=3.97]epoch 2/100: 463it [10:26,  2.10it/s, loss=4.05]epoch 2/100: 463it [10:27,  2.10it/s, loss=3.95]epoch 2/100: 463it [10:27,  2.10it/s, loss=4.27]epoch 2/100: 464it [10:27,  2.22it/s, loss=3.95]epoch 2/100: 464it [10:27,  2.22it/s, loss=4.27]epoch 2/100: 464it [10:27,  2.22it/s, loss=4.08]epoch 2/100: 464it [10:27,  2.22it/s, loss=3.93]epoch 2/100: 465it [10:27,  2.32it/s, loss=3.93]epoch 2/100: 465it [10:27,  2.32it/s, loss=4.08]epoch 2/100: 465it [10:27,  2.32it/s, loss=4.03]epoch 2/100: 465it [10:27,  2.32it/s, loss=4.03]epoch 2/100: 466it [10:27,  2.40it/s, loss=4.03]epoch 2/100: 466it [10:27,  2.40it/s, loss=4.03]epoch 2/100: 466it [10:28,  2.40it/s, loss=3.94]epoch 2/100: 466it [10:28,  2.40it/s, loss=4.04]epoch 2/100: 467it [10:28,  2.46it/s, loss=3.94]epoch 2/100: 467it [10:28,  2.46it/s, loss=4.04]epoch 2/100: 467it [10:28,  2.46it/s, loss=4.15]epoch 2/100: 467it [10:28,  2.46it/s, loss=3.86]epoch 2/100: 468it [10:28,  2.50it/s, loss=3.86]epoch 2/100: 468it [10:28,  2.50it/s, loss=4.15]epoch 2/100: 468it [10:29,  2.50it/s, loss=4.09]epoch 2/100: 468it [10:29,  2.50it/s, loss=4.25]epoch 2/100: 469it [10:29,  2.53it/s, loss=4.09]epoch 2/100: 469it [10:29,  2.53it/s, loss=4.25]epoch 2/100: 469it [10:29,  2.53it/s, loss=4.07]epoch 2/100: 469it [10:29,  2.53it/s, loss=4.08]epoch 2/100: 470it [10:29,  2.55it/s, loss=4.07]epoch 2/100: 470it [10:29,  2.55it/s, loss=4.08]epoch 2/100: 470it [10:29,  2.55it/s, loss=4.04]epoch 2/100: 470it [10:29,  2.55it/s, loss=3.99]epoch 2/100: 471it [10:29,  2.56it/s, loss=4.04]epoch 2/100: 471it [10:29,  2.56it/s, loss=3.99]epoch 2/100: 471it [10:30,  2.56it/s, loss=4.26]epoch 2/100: 471it [10:30,  2.56it/s, loss=3.89]epoch 2/100: 472it [10:30,  2.57it/s, loss=4.26]epoch 2/100: 472it [10:30,  2.57it/s, loss=3.89]epoch 2/100: 472it [10:30,  2.57it/s, loss=4.02]epoch 2/100: 472it [10:30,  2.57it/s, loss=3.95]epoch 2/100: 473it [10:30,  2.58it/s, loss=4.02]epoch 2/100: 473it [10:30,  2.58it/s, loss=3.95]epoch 2/100: 473it [10:31,  2.58it/s, loss=4.01]epoch 2/100: 473it [10:31,  2.58it/s, loss=4.03]epoch 2/100: 474it [10:31,  2.59it/s, loss=4.03]epoch 2/100: 474it [10:31,  2.59it/s, loss=4.01]epoch 2/100: 474it [10:31,  2.59it/s, loss=4]   epoch 2/100: 474it [10:31,  2.59it/s, loss=4.21]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:03,  3.22s/it][A
val: 1it [00:03,  3.44s/it][A
val: 2it [00:03,  1.57s/it][A
val: 3it [00:03,  1.03it/s][A
val: 4it [00:04,  1.46it/s][A
val: 5it [00:04,  1.89it/s][A
val: 6it [00:04,  2.30it/s][A
val: 7it [00:04,  2.67it/s][A
val: 8it [00:05,  2.98it/s][A
val: 9it [00:05,  3.24it/s][A
val: 10it [00:05,  3.43it/s][A
val: 11it [00:05,  3.58it/s][A
val: 12it [00:06,  3.69it/s][A
val: 13it [00:06,  3.77it/s][A
val: 14it [00:06,  3.83it/s][A
val: 15it [00:06,  3.87it/s][A
val: 16it [00:07,  3.91it/s][A
val: 17it [00:07,  3.93it/s][A
val: 18it [00:07,  3.95it/s][A
val: 19it [00:07,  3.96it/s][A
val: 20it [00:08,  3.96it/s][A
val: 21it [00:08,  3.98it/s][A
val: 22it [00:08,  3.97it/s][A
val: 23it [00:08,  3.98it/s][A
val: 24it [00:09,  3.98it/s][A
                            [A
val: 2it [00:27, 15.33s/it][A
val: 3it [00:27,  8.45s/it][A
val: 4it [00:27,  5.21s/it][A
val: 5it [00:27,  3.42s/it][A
val: 6it [00:28,  2.34s/it][A
val: 7it [00:28,  1.66s/it][A
val: 8it [00:28,  1.21s/it][A
val: 9it [00:28,  1.10it/s][A
val: 10it [00:29,  1.42it/s][A
val: 11it [00:29,  1.77it/s][A
val: 12it [00:29,  2.13it/s][A
val: 13it [00:29,  2.48it/s][A
val: 14it [00:30,  2.80it/s][A
val: 15it [00:30,  3.08it/s][A
val: 16it [00:30,  3.31it/s][A
val: 17it [00:30,  3.49it/s][A
val: 18it [00:31,  3.63it/s][A
val: 19it [00:31,  3.74it/s][A
val: 20it [00:31,  3.82it/s][A
val: 21it [00:31,  3.87it/s][A
val: 22it [00:32,  3.91it/s][A
val: 23it [00:32,  3.94it/s][A
val: 24it [00:32,  3.96it/s][A
                            [Aepoch 2/100: 474it [11:04,  2.59it/s, loss=4.21]epoch 2/100: 474it [11:12,  2.59it/s, best_val=4.02, loss=4, val=4.02]epoch 2/100: 475it [11:12, 12.83s/it, loss=4.21]epoch 2/100: 475it [11:12, 12.83s/it, best_val=4.02, loss=4, val=4.02]epoch 2/100: 475it [11:13, 12.83s/it, loss=3.94]                      epoch 2/100: 475it [11:13, 12.83s/it, loss=3.92]epoch 2/100: 476it [11:13,  9.10s/it, loss=3.92]epoch 2/100: 476it [11:13,  9.10s/it, loss=3.94]epoch 2/100: 476it [11:13,  9.10s/it, loss=3.97]epoch 2/100: 476it [11:13,  9.10s/it, loss=4.16]epoch 2/100: 477it [11:13,  6.48s/it, loss=3.97]epoch 2/100: 477it [11:13,  6.48s/it, loss=4.16]epoch 2/100: 477it [11:14,  6.48s/it, loss=4.07]epoch 2/100: 477it [11:14,  6.48s/it, loss=4.03]epoch 2/100: 478it [11:14,  4.66s/it, loss=4.07]epoch 2/100: 478it [11:14,  4.66s/it, loss=4.03]epoch 2/100: 478it [11:14,  4.66s/it, loss=4.06]epoch 2/100: 478it [11:14,  4.66s/it, loss=4.1] epoch 2/100: 479it [11:14,  3.44s/it, loss=4.06]epoch 2/100: 479it [11:14,  3.44s/it, loss=4.1]epoch 2/100: 479it [11:15,  3.44s/it, loss=3.85]epoch 2/100: 479it [11:15,  3.44s/it, loss=4.22]epoch 2/100: 480it [11:15,  2.53s/it, loss=3.85]epoch 2/100: 480it [11:15,  2.53s/it, loss=4.22]epoch 2/100: 480it [11:15,  2.53s/it, loss=4.33]epoch 2/100: 480it [11:15,  2.53s/it, loss=4.03]epoch 2/100: 481it [11:15,  1.88s/it, loss=4.33]epoch 2/100: 481it [11:15,  1.88s/it, loss=4.03]epoch 2/100: 481it [11:15,  1.88s/it, loss=3.95]epoch 2/100: 481it [11:15,  1.88s/it, loss=3.84]epoch 2/100: 482it [11:15,  1.43s/it, loss=3.95]epoch 2/100: 482it [11:15,  1.43s/it, loss=3.84]epoch 2/100: 482it [11:16,  1.43s/it, loss=4.1] epoch 2/100: 482it [11:16,  1.43s/it, loss=3.99]epoch 2/100: 483it [11:16,  1.12s/it, loss=4.1]epoch 2/100: 483it [11:16,  1.12s/it, loss=3.99]epoch 2/100: 483it [11:16,  1.12s/it, loss=4.11]epoch 2/100: 483it [11:16,  1.12s/it, loss=3.97]epoch 2/100: 484it [11:16,  1.11it/s, loss=4.11]epoch 2/100: 484it [11:16,  1.11it/s, loss=3.97]epoch 2/100: 484it [11:17,  1.11it/s, loss=4.07]epoch 2/100: 484it [11:17,  1.11it/s, loss=4.12]epoch 2/100: 485it [11:17,  1.34it/s, loss=4.07]epoch 2/100: 485it [11:17,  1.34it/s, loss=4.12]epoch 2/100: 485it [11:17,  1.34it/s, loss=4.09]epoch 2/100: 485it [11:17,  1.34it/s, loss=3.98]epoch 2/100: 486it [11:17,  1.57it/s, loss=3.98]epoch 2/100: 486it [11:17,  1.57it/s, loss=4.09]epoch 2/100: 486it [11:17,  1.57it/s, loss=4.24]epoch 2/100: 486it [11:17,  1.57it/s, loss=4.21]epoch 2/100: 487it [11:17,  1.78it/s, loss=4.21]epoch 2/100: 487it [11:17,  1.78it/s, loss=4.24]epoch 2/100: 487it [11:18,  1.78it/s, loss=3.96]epoch 2/100: 487it [11:18,  1.78it/s, loss=3.99]epoch 2/100: 488it [11:18,  1.97it/s, loss=3.99]epoch 2/100: 488it [11:18,  1.97it/s, loss=3.96]epoch 2/100: 488it [11:18,  1.39s/it, loss=3.96]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 2/100: 488it [11:18,  1.39s/it, loss=3.99]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 3/100: 0it [00:00, ?it/s]Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 3/100: 0it [00:00, ?it/s]Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
epoch 3/100: 0it [00:03, ?it/s, loss=4.22]epoch 3/100: 0it [00:03, ?it/s, loss=4.01]epoch 3/100: 1it [00:03,  3.44s/it, loss=4.22]epoch 3/100: 1it [00:03,  3.46s/it, loss=4.01]epoch 3/100: 1it [00:03,  3.44s/it, loss=4.02]epoch 3/100: 1it [00:03,  3.46s/it, loss=4.01]epoch 3/100: 2it [00:03,  1.64s/it, loss=4.02]epoch 3/100: 2it [00:03,  1.65s/it, loss=4.01]epoch 3/100: 2it [00:04,  1.64s/it, loss=3.94]epoch 3/100: 2it [00:04,  1.65s/it, loss=4.22]epoch 3/100: 3it [00:04,  1.07s/it, loss=3.94]epoch 3/100: 3it [00:04,  1.07s/it, loss=4.22]epoch 3/100: 3it [00:04,  1.07s/it, loss=3.96]epoch 3/100: 3it [00:04,  1.07s/it, loss=4.01]epoch 3/100: 4it [00:04,  1.25it/s, loss=3.96]epoch 3/100: 4it [00:04,  1.25it/s, loss=4.01]epoch 3/100: 4it [00:04,  1.25it/s, loss=3.91]epoch 3/100: 4it [00:05,  1.25it/s, loss=4.13]epoch 3/100: 5it [00:04,  1.54it/s, loss=3.91]epoch 3/100: 5it [00:05,  1.53it/s, loss=4.13]epoch 3/100: 5it [00:05,  1.54it/s, loss=4.05]epoch 3/100: 5it [00:05,  1.53it/s, loss=4.1] epoch 3/100: 6it [00:05,  1.78it/s, loss=4.05]epoch 3/100: 6it [00:05,  1.78it/s, loss=4.1]epoch 3/100: 6it [00:05,  1.78it/s, loss=4.18]epoch 3/100: 6it [00:05,  1.78it/s, loss=4.23]epoch 3/100: 7it [00:05,  1.99it/s, loss=4.18]epoch 3/100: 7it [00:05,  1.98it/s, loss=4.23]epoch 3/100: 7it [00:06,  1.99it/s, loss=4.11]epoch 3/100: 7it [00:06,  1.98it/s, loss=3.98]epoch 3/100: 8it [00:06,  2.15it/s, loss=4.11]epoch 3/100: 8it [00:06,  2.14it/s, loss=3.98]epoch 3/100: 8it [00:06,  2.15it/s, loss=4.05]epoch 3/100: 8it [00:06,  2.14it/s, loss=4.13]epoch 3/100: 9it [00:06,  2.27it/s, loss=4.05]epoch 3/100: 9it [00:06,  2.27it/s, loss=4.13]epoch 3/100: 9it [00:06,  2.27it/s, loss=4.07]epoch 3/100: 9it [00:06,  2.27it/s, loss=3.8] epoch 3/100: 10it [00:06,  2.36it/s, loss=4.07]epoch 3/100: 10it [00:06,  2.36it/s, loss=3.8]epoch 3/100: 10it [00:07,  2.36it/s, loss=3.95]epoch 3/100: 10it [00:07,  2.36it/s, loss=3.89]epoch 3/100: 11it [00:07,  2.43it/s, loss=3.95]epoch 3/100: 11it [00:07,  2.43it/s, loss=3.89]epoch 3/100: 11it [00:07,  2.43it/s, loss=4.08]epoch 3/100: 11it [00:07,  2.43it/s, loss=3.99]epoch 3/100: 12it [00:07,  2.48it/s, loss=4.08]epoch 3/100: 12it [00:07,  2.48it/s, loss=3.99]epoch 3/100: 12it [00:08,  2.48it/s, loss=4.07]epoch 3/100: 12it [00:08,  2.48it/s, loss=4.27]epoch 3/100: 13it [00:08,  2.51it/s, loss=4.27]epoch 3/100: 13it [00:08,  2.52it/s, loss=4.07]epoch 3/100: 13it [00:08,  2.52it/s, loss=3.86]epoch 3/100: 13it [00:08,  2.51it/s, loss=4.19]epoch 3/100: 14it [00:08,  2.54it/s, loss=3.86]epoch 3/100: 14it [00:08,  2.54it/s, loss=4.19]epoch 3/100: 14it [00:08,  2.54it/s, loss=3.89]epoch 3/100: 14it [00:08,  2.54it/s, loss=3.9] epoch 3/100: 15it [00:08,  2.56it/s, loss=3.89]epoch 3/100: 15it [00:08,  2.56it/s, loss=3.9]epoch 3/100: 15it [00:09,  2.56it/s, loss=4.13]epoch 3/100: 15it [00:09,  2.56it/s, loss=3.93]epoch 3/100: 16it [00:09,  2.57it/s, loss=4.13]epoch 3/100: 16it [00:09,  2.57it/s, loss=3.93]epoch 3/100: 16it [00:09,  2.57it/s, loss=4.05]epoch 3/100: 16it [00:09,  2.57it/s, loss=3.94]epoch 3/100: 17it [00:09,  2.58it/s, loss=4.05]epoch 3/100: 17it [00:09,  2.58it/s, loss=3.94]epoch 3/100: 17it [00:09,  2.58it/s, loss=4.04]epoch 3/100: 17it [00:10,  2.58it/s, loss=3.91]epoch 3/100: 18it [00:09,  2.58it/s, loss=4.04]epoch 3/100: 18it [00:10,  2.58it/s, loss=3.91]epoch 3/100: 18it [00:10,  2.58it/s, loss=4.02]epoch 3/100: 18it [00:10,  2.58it/s, loss=4.2] epoch 3/100: 19it [00:10,  2.59it/s, loss=4.02]epoch 3/100: 19it [00:10,  2.59it/s, loss=4.2]epoch 3/100: 19it [00:10,  2.59it/s, loss=4.03]epoch 3/100: 19it [00:10,  2.59it/s, loss=3.89]epoch 3/100: 20it [00:10,  2.60it/s, loss=4.03]epoch 3/100: 20it [00:10,  2.60it/s, loss=3.89]epoch 3/100: 20it [00:11,  2.60it/s, loss=4.09]epoch 3/100: 20it [00:11,  2.60it/s, loss=4.03]epoch 3/100: 21it [00:11,  2.60it/s, loss=4.09]epoch 3/100: 21it [00:11,  2.60it/s, loss=4.03]epoch 3/100: 21it [00:11,  2.60it/s, loss=3.98]epoch 3/100: 21it [00:11,  2.60it/s, loss=3.71]epoch 3/100: 22it [00:11,  2.60it/s, loss=3.98]epoch 3/100: 22it [00:11,  2.60it/s, loss=3.71]epoch 3/100: 22it [00:11,  2.60it/s, loss=4.05]epoch 3/100: 22it [00:11,  2.60it/s, loss=4.03]epoch 3/100: 23it [00:11,  2.60it/s, loss=4.03]epoch 3/100: 23it [00:11,  2.60it/s, loss=4.05]epoch 3/100: 23it [00:12,  2.60it/s, loss=4.13]epoch 3/100: 23it [00:12,  2.60it/s, loss=4.05]epoch 3/100: 24it [00:12,  2.60it/s, loss=4.13]epoch 3/100: 24it [00:12,  2.60it/s, loss=4.05]epoch 3/100: 24it [00:12,  2.60it/s, loss=4.19]epoch 3/100: 24it [00:12,  2.60it/s, loss=4.01]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

val: 1it [00:27, 27.31s/it][A
val: 1it [00:27, 27.32s/it][A
val: 2it [00:27, 11.39s/it][A
val: 2it [00:27, 11.40s/it][A
val: 3it [00:27,  6.31s/it][A
val: 3it [00:27,  6.31s/it][A
val: 4it [00:28,  3.91s/it][A
val: 4it [00:28,  3.92s/it][A
val: 5it [00:28,  2.59s/it][A
val: 5it [00:28,  2.59s/it][A
val: 6it [00:28,  1.80s/it][A
val: 6it [00:28,  1.80s/it][A
val: 7it [00:28,  1.29s/it][A
val: 7it [00:28,  1.29s/it][A
val: 8it [00:29,  1.04it/s][A
val: 8it [00:29,  1.04it/s][A
val: 9it [00:29,  1.36it/s][A
val: 9it [00:29,  1.36it/s][A
val: 10it [00:29,  1.70it/s][A
val: 10it [00:29,  1.70it/s][A
val: 11it [00:29,  2.07it/s][A
val: 11it [00:29,  2.07it/s][A
val: 12it [00:30,  2.42it/s][A
val: 12it [00:30,  2.43it/s][A
val: 13it [00:30,  2.75it/s][A
val: 13it [00:30,  2.75it/s][A
val: 14it [00:30,  3.04it/s][A
val: 14it [00:30,  3.04it/s][A
val: 15it [00:30,  3.27it/s][A
val: 15it [00:30,  3.28it/s][A
val: 16it [00:31,  3.47it/s][A
val: 16it [00:31,  3.47it/s][A
val: 17it [00:31,  3.61it/s][A
val: 17it [00:31,  3.61it/s][A
val: 18it [00:31,  3.72it/s][A
val: 18it [00:31,  3.72it/s][A
val: 19it [00:31,  3.80it/s][A
val: 19it [00:31,  3.80it/s][A
val: 20it [00:32,  3.86it/s][A
val: 20it [00:32,  3.85it/s][A
val: 21it [00:32,  3.89it/s][A
val: 21it [00:32,  3.89it/s][A
val: 22it [00:32,  3.93it/s][A
val: 22it [00:32,  3.91it/s][A
val: 23it [00:32,  3.95it/s][A
val: 23it [00:32,  3.94it/s][A
val: 24it [00:33,  3.97it/s][A
val: 24it [00:33,  3.95it/s][A
                            [A
                            [Aepoch 3/100: 24it [00:45,  2.60it/s, loss=4.01]epoch 3/100: 24it [00:55,  2.60it/s, best_val=4.01, loss=4.19, val=4.01]epoch 3/100: 25it [00:55, 13.27s/it, loss=4.01]epoch 3/100: 25it [00:55, 13.27s/it, best_val=4.01, loss=4.19, val=4.01]epoch 3/100: 25it [00:56, 13.27s/it, loss=3.91]epoch 3/100: 25it [00:56, 13.27s/it, loss=3.93]                         epoch 3/100: 26it [00:56,  9.41s/it, loss=3.91]epoch 3/100: 26it [00:56,  9.41s/it, loss=3.93]epoch 3/100: 26it [00:56,  9.41s/it, loss=4.05]epoch 3/100: 26it [00:56,  9.41s/it, loss=3.92]epoch 3/100: 27it [00:56,  6.70s/it, loss=4.05]epoch 3/100: 27it [00:56,  6.70s/it, loss=3.92]epoch 3/100: 27it [00:56,  6.70s/it, loss=3.99]epoch 3/100: 27it [00:56,  6.70s/it, loss=4.04]epoch 3/100: 28it [00:56,  4.80s/it, loss=3.99]epoch 3/100: 28it [00:56,  4.80s/it, loss=4.04]epoch 3/100: 28it [00:57,  4.80s/it, loss=3.98]epoch 3/100: 28it [00:57,  4.80s/it, loss=4.08]epoch 3/100: 29it [00:57,  3.48s/it, loss=4.08]epoch 3/100: 29it [00:57,  3.48s/it, loss=3.98]epoch 3/100: 29it [00:57,  3.48s/it, loss=4.01]epoch 3/100: 29it [00:57,  3.48s/it, loss=4.09]epoch 3/100: 30it [00:57,  2.55s/it, loss=4.01]epoch 3/100: 30it [00:57,  2.55s/it, loss=4.09]epoch 3/100: 30it [00:57,  2.55s/it, loss=3.99]epoch 3/100: 30it [00:57,  2.55s/it, loss=4.06]epoch 3/100: 31it [00:57,  1.90s/it, loss=3.99]epoch 3/100: 31it [00:57,  1.90s/it, loss=4.06]epoch 3/100: 31it [00:58,  1.90s/it, loss=3.9] epoch 3/100: 31it [00:58,  1.90s/it, loss=4.03]epoch 3/100: 32it [00:58,  1.45s/it, loss=3.9]epoch 3/100: 32it [00:58,  1.45s/it, loss=4.03]epoch 3/100: 32it [00:58,  1.45s/it, loss=4.28]epoch 3/100: 32it [00:58,  1.45s/it, loss=4.03]epoch 3/100: 33it [00:58,  1.13s/it, loss=4.28]epoch 3/100: 33it [00:58,  1.13s/it, loss=4.03]epoch 3/100: 33it [00:59,  1.13s/it, loss=4.08]epoch 3/100: 33it [00:59,  1.13s/it, loss=4.03]epoch 3/100: 34it [00:59,  1.11it/s, loss=4.08]epoch 3/100: 34it [00:59,  1.11it/s, loss=4.03]epoch 3/100: 34it [00:59,  1.11it/s, loss=4.03]epoch 3/100: 34it [00:59,  1.11it/s, loss=4.1] epoch 3/100: 35it [00:59,  1.34it/s, loss=4.1]epoch 3/100: 35it [00:59,  1.34it/s, loss=4.03]epoch 3/100: 35it [00:59,  1.34it/s, loss=4.11]epoch 3/100: 35it [00:59,  1.34it/s, loss=4.18]epoch 3/100: 36it [00:59,  1.56it/s, loss=4.11]epoch 3/100: 36it [00:59,  1.56it/s, loss=4.18]epoch 3/100: 36it [01:00,  1.56it/s, loss=4.11]epoch 3/100: 36it [01:00,  1.56it/s, loss=3.88]epoch 3/100: 37it [01:00,  1.78it/s, loss=4.11]epoch 3/100: 37it [01:00,  1.78it/s, loss=3.88]epoch 3/100: 37it [01:00,  1.78it/s, loss=4.31]epoch 3/100: 37it [01:00,  1.78it/s, loss=4]   epoch 3/100: 38it [01:00,  1.96it/s, loss=4.31]epoch 3/100: 38it [01:00,  1.96it/s, loss=4]epoch 3/100: 38it [01:01,  1.96it/s, loss=3.79]epoch 3/100: 38it [01:01,  1.96it/s, loss=4.17]epoch 3/100: 39it [01:01,  2.09it/s, loss=3.79]epoch 3/100: 39it [01:01,  2.09it/s, loss=4.17]epoch 3/100: 39it [01:01,  2.09it/s, loss=3.92]epoch 3/100: 39it [01:01,  2.09it/s, loss=4.07]epoch 3/100: 40it [01:01,  2.21it/s, loss=3.92]epoch 3/100: 40it [01:01,  2.21it/s, loss=4.07]epoch 3/100: 40it [01:01,  2.21it/s, loss=4.01]epoch 3/100: 40it [01:01,  2.21it/s, loss=4.19]epoch 3/100: 41it [01:01,  2.32it/s, loss=4.19]epoch 3/100: 41it [01:01,  2.32it/s, loss=4.01]epoch 3/100: 41it [01:02,  2.32it/s, loss=3.95]epoch 3/100: 41it [01:02,  2.32it/s, loss=4.03]epoch 3/100: 42it [01:02,  2.39it/s, loss=3.95]epoch 3/100: 42it [01:02,  2.39it/s, loss=4.03]epoch 3/100: 42it [01:02,  2.39it/s, loss=4.13]epoch 3/100: 42it [01:02,  2.39it/s, loss=4.08]epoch 3/100: 43it [01:02,  2.45it/s, loss=4.08]epoch 3/100: 43it [01:02,  2.45it/s, loss=4.13]epoch 3/100: 43it [01:02,  2.45it/s, loss=3.95]epoch 3/100: 43it [01:02,  2.45it/s, loss=3.78]epoch 3/100: 44it [01:02,  2.49it/s, loss=3.78]epoch 3/100: 44it [01:02,  2.49it/s, loss=3.95]epoch 3/100: 44it [01:03,  2.49it/s, loss=4.15]epoch 3/100: 44it [01:03,  2.49it/s, loss=4.08]epoch 3/100: 45it [01:03,  2.53it/s, loss=4.15]epoch 3/100: 45it [01:03,  2.53it/s, loss=4.08]epoch 3/100: 45it [01:03,  2.53it/s, loss=3.76]epoch 3/100: 45it [01:03,  2.53it/s, loss=4.15]epoch 3/100: 46it [01:03,  2.55it/s, loss=3.76]epoch 3/100: 46it [01:03,  2.55it/s, loss=4.15]epoch 3/100: 46it [01:04,  2.55it/s, loss=4.06]epoch 3/100: 46it [01:04,  2.55it/s, loss=3.98]epoch 3/100: 47it [01:04,  2.57it/s, loss=4.06]epoch 3/100: 47it [01:04,  2.56it/s, loss=3.98]epoch 3/100: 47it [01:04,  2.57it/s, loss=3.84]epoch 3/100: 47it [01:04,  2.56it/s, loss=4.01]epoch 3/100: 48it [01:04,  2.58it/s, loss=3.84]epoch 3/100: 48it [01:04,  2.58it/s, loss=4.01]epoch 3/100: 48it [01:04,  2.58it/s, loss=3.92]epoch 3/100: 48it [01:04,  2.58it/s, loss=4.03]epoch 3/100: 49it [01:04,  2.58it/s, loss=4.03]epoch 3/100: 49it [01:04,  2.58it/s, loss=3.92]epoch 3/100: 49it [01:05,  2.58it/s, loss=4.16]epoch 3/100: 49it [01:05,  2.58it/s, loss=4.07]
val: 0it [00:00, ?it/s][A
val: 0it [00:00, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Too many dataloader workers: 4 (max is dataset.num_shards=2). Stopping 2 dataloader workers.

                       [Aepoch 3/100: 49it [01:15,  1.55s/it, loss=4.16]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/predictive_concept_decoders/predictive_concept_decoders.py", line 542, in <module>
[rank0]:     do_train_full()
[rank0]:   File "/workspace/predictive_concept_decoders/predictive_concept_decoders.py", line 481, in do_train_full
[rank0]:     for val_batch in tqdm(val_loader, desc="val", leave=False):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/tqdm/std.py", line 1181, in __iter__
[rank0]:     for obj in iterable:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py", line 732, in __next__
[rank0]:     data = self._next_data()
[rank0]:            ^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py", line 1506, in _next_data
[rank0]:     return self._process_data(data, worker_id)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py", line 1541, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/_utils.py", line 769, in reraise
[rank0]:     raise exception
[rank0]: FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py", line 33, in fetch
[rank0]:     data.append(next(self.dataset_iter))
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/predictive_concept_decoders/predictive_concept_decoders.py", line 200, in __iter__
[rank0]:     for item in ds:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 2529, in __iter__
[rank0]:     yield from self._iter_pytorch()
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 2444, in _iter_pytorch
[rank0]:     for key, example in ex_iterable:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 2060, in __iter__
[rank0]:     for key, pa_table in self._iter_arrow():
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 2083, in _iter_arrow
[rank0]:     for key, pa_table in self.ex_iterable._iter_arrow():
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 544, in _iter_arrow
[rank0]:     for key, pa_table in iterator:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 164, in _convert_to_arrow
[rank0]:     for key, example in iterator:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 1923, in __iter__
[rank0]:     for key_example in islice(self.ex_iterable, self.n - ex_iterable_num_taken):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 1801, in __iter__
[rank0]:     yield from islice(self.ex_iterable, ex_iterable_idx_start, None)
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 1719, in __iter__
[rank0]:     for x in self.ex_iterable:
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 518, in __iter__
[rank0]:     yield from self.ex_iterable
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py", line 362, in __iter__
[rank0]:     for key, pa_table in self.generate_tables_fn(**gen_kwags):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/parquet/parquet.py", line 163, in _generate_tables
[rank0]:     for file_idx, file in enumerate(itertools.chain.from_iterable(files)):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/utils/track.py", line 49, in __iter__
[rank0]:     for x in self.generator(*self.args):
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py", line 1399, in _iter_from_urlpaths
[rank0]:     raise FileNotFoundError(urlpath)
[rank0]: FileNotFoundError: hf://datasets/HuggingFaceFW/fineweb@9bb295ddab0e05d785b879661af7260fed5140fc/sample/10BT/000_00000.parquet

[rank0]:[W104 06:48:01.711144946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[E104 06:57:53.730227055 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7238, OpType=BROADCAST, NumelIn=32, NumelOut=32, Timeout(ms)=600000) ran for 600057 milliseconds before timing out.
[rank1]:[E104 06:57:53.730981029 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 7238 PG status: last enqueued work: 7238, last completed work: 7237
[rank1]:[E104 06:57:53.731009166 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E104 06:57:53.731143694 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank0]:[E104 06:57:54.129681072 ProcessGroupNCCL.cpp:1794] [PG ID 0 PG GUID 0(default_pg) Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E104 06:57:54.130121416 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from  rank 1 and we will try our best to dump the debug info. Last enqueued NCCL work: 7237, last completed NCCL work: 7237.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E104 06:57:54.130384157 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E104 06:57:54.336364045 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 7238, last completed NCCL work: 7237.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E104 06:57:54.336756886 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E104 06:58:01.711346431 ProcessGroupNCCL.cpp:1362] [PG ID 0 PG GUID 0(default_pg) Rank 0] Future for ProcessGroup abort timed out after 600000 ms
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Future for ProcessGroup abort timed out after 600000 ms
Exception raised from waitForFutureOrTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1365 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x729101132b80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe34731 (0x729101fea731 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x94e570 (0x729101b04570 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::abort() + 0x297 (0x72910200abd7 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::~ProcessGroupNCCL() + 0xa4 (0x72910200aed4 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: c10d::ProcessGroupNCCL::~ProcessGroupNCCL() + 0x9 (0x72910200b489 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
frame #6: c10d::ProcessGroup::release_resources() + 0x14a (0x729143440a9a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0x5f8a838 (0x72914341e838 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::Reducer::~Reducer() + 0x500 (0x7291434f3170 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xdb61c2 (0x729152ba01c2 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x3bc537 (0x7291521a6537 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)
frame #11: <unknown function> + 0xdc11c1 (0x729152bab1c1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)
frame #12: <unknown function> + 0x3cb16c (0x7291521b516c in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)
frame #13: <unknown function> + 0x3cb7be (0x7291521b57be in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_python.so)
frame #14: /usr/bin/python() [0x52dade]
frame #15: /usr/bin/python() [0x58ca9f]
frame #16: /usr/bin/python() [0x5208fb]
frame #17: _PyModule_ClearDict + 0x19c (0x5b82ac in /usr/bin/python)
frame #18: /usr/bin/python() [0x644895]
frame #19: Py_FinalizeEx + 0x14b (0x63356b in /usr/bin/python)
frame #20: Py_RunMain + 0x185 (0x63eae5 in /usr/bin/python)
frame #21: Py_BytesMain + 0x2d (0x60486d in /usr/bin/python)
frame #22: <unknown function> + 0x29d90 (0x729160ea0d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x729160ea0e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #24: _start + 0x25 (0x6046f5 in /usr/bin/python)


val: 1it [11:03, 663.24s/it][A
val: 2it [11:03, 273.25s/it][AW0104 06:58:54.215000 10397 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 10467 closing signal SIGTERM
E0104 06:58:54.333000 10397 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: -6) local_rank: 0 (pid: 10466) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
predictive_concept_decoders.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-04_06:58:54
  host      : 68d2614c9a61
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 10466)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 10466
======================================================
