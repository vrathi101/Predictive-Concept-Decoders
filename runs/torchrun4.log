W0104 15:29:26.079000 13134 torch/distributed/run.py:803] 
W0104 15:29:26.079000 13134 torch/distributed/run.py:803] *****************************************
W0104 15:29:26.079000 13134 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0104 15:29:26.079000 13134 torch/distributed/run.py:803] *****************************************
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
epoch 1/100: 0it [00:00, ?it/s]Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.
epoch 1/100: 0it [00:00, ?it/s]Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.
epoch 1/100: 0it [00:09, ?it/s, loss=7.18]epoch 1/100: 1it [00:09,  9.55s/it, loss=7.18]epoch 1/100: 0it [00:09, ?it/s, loss=7.1]epoch 1/100: 1it [00:09,  9.55s/it, loss=7.1]epoch 1/100: 1it [00:09,  9.55s/it, loss=7.04]epoch 1/100: 1it [00:09,  9.55s/it, loss=7.18]epoch 1/100: 2it [00:09,  4.16s/it, loss=7.04]epoch 1/100: 2it [00:09,  4.16s/it, loss=7.18]epoch 1/100: 2it [00:10,  4.16s/it, loss=6.58]epoch 1/100: 2it [00:10,  4.16s/it, loss=6.96]epoch 1/100: 3it [00:10,  2.44s/it, loss=6.58]epoch 1/100: 3it [00:10,  2.44s/it, loss=6.96]epoch 1/100: 3it [00:10,  2.44s/it, loss=6.8] epoch 1/100: 3it [00:10,  2.44s/it, loss=6.52]epoch 1/100: 4it [00:10,  1.63s/it, loss=6.8]epoch 1/100: 4it [00:10,  1.62s/it, loss=6.52]epoch 1/100: 4it [00:11,  1.63s/it, loss=6.21]epoch 1/100: 4it [00:11,  1.62s/it, loss=6.36]epoch 1/100: 5it [00:11,  1.18s/it, loss=6.21]epoch 1/100: 5it [00:11,  1.18s/it, loss=6.36]epoch 1/100: 5it [00:11,  1.18s/it, loss=6.15]epoch 1/100: 5it [00:11,  1.18s/it, loss=6.2] epoch 1/100: 6it [00:11,  1.10it/s, loss=6.15]epoch 1/100: 6it [00:11,  1.10it/s, loss=6.2]epoch 1/100: 6it [00:11,  1.10it/s, loss=5.97]epoch 1/100: 6it [00:11,  1.10it/s, loss=6.01]epoch 1/100: 7it [00:11,  1.36it/s, loss=5.97]epoch 1/100: 7it [00:11,  1.36it/s, loss=6.01]epoch 1/100: 7it [00:12,  1.36it/s, loss=5.92]epoch 1/100: 7it [00:12,  1.36it/s, loss=5.92]epoch 1/100: 8it [00:12,  1.61it/s, loss=5.92]epoch 1/100: 8it [00:12,  1.61it/s, loss=5.92]epoch 1/100: 8it [00:12,  1.61it/s, loss=6.02]epoch 1/100: 8it [00:12,  1.61it/s, loss=6.29]epoch 1/100: 9it [00:12,  1.83it/s, loss=6.02]epoch 1/100: 9it [00:12,  1.83it/s, loss=6.29]epoch 1/100: 9it [00:13,  1.83it/s, loss=6.08]epoch 1/100: 9it [00:12,  1.83it/s, loss=5.69]epoch 1/100: 10it [00:13,  2.01it/s, loss=6.08]epoch 1/100: 10it [00:12,  2.01it/s, loss=5.69]epoch 1/100: 10it [00:13,  2.01it/s, loss=6.34]epoch 1/100: 10it [00:13,  2.01it/s, loss=6.09]epoch 1/100: 11it [00:13,  2.17it/s, loss=6.34]epoch 1/100: 11it [00:13,  2.17it/s, loss=6.09]epoch 1/100: 11it [00:13,  2.17it/s, loss=6.11]epoch 1/100: 11it [00:13,  2.17it/s, loss=6.11]epoch 1/100: 12it [00:13,  2.28it/s, loss=6.11]epoch 1/100: 12it [00:13,  2.28it/s, loss=6.11]epoch 1/100: 12it [00:14,  2.28it/s, loss=6.03]epoch 1/100: 12it [00:14,  2.28it/s, loss=5.78]epoch 1/100: 13it [00:14,  2.37it/s, loss=6.03]epoch 1/100: 13it [00:14,  2.37it/s, loss=5.78]epoch 1/100: 13it [00:14,  2.37it/s, loss=5.97]epoch 1/100: 13it [00:14,  2.37it/s, loss=6.09]epoch 1/100: 14it [00:14,  2.44it/s, loss=5.97]epoch 1/100: 14it [00:14,  2.44it/s, loss=6.09]